{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyORo59JCCotHH89DeCSFQUA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import pandas as pd\n","import os\n","import random\n","import matplotlib.pyplot as plt\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"OKdr0AFPWE7x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["WAVELENGTHS = np.arange(380, 781, 1) # 파장 범위 및 간격 (nm), 예시로 5nm 간격\n","NUM_WAVELENGTHS = len(WAVELENGTHS)\n","DRIVE_PATH = \"C:/Users/PC/Desktop/Deep/Materials_380\"\n","DATA_DIR = DRIVE_PATH\n","# 재료별 n, k 데이터 로드\n","\n","\n","material_files = [\n","    #('HfO2', 'HfO2_380.csv'), # 사용자 코드에서 가져온 변수명 기반 추정\n","    ('LiF',  'LiF_380.csv'),\n","    ('SiO2', 'SiO2_380.csv'),\n","    ('WO3',  'WO3_380.csv'),\n","    #('ZnS',  'ZnS_380.csv'),\n","    #('Al2O3',  'Al2O3_380.csv'),\n","    #('MgF2',  'MgF2_380.csv'),\n","    #('TiO2',  'TiO2_380.csv'),\n","    ('Ag' , 'Ag_380.csv')\n","]\n","material_data = {}\n","material_names = [info[0] for info in material_files]\n","print(material_names)\n","\n","material_names = sorted(set(info[0] for info in material_files))  # 순서 고정\n","material_to_index = {name: i for i, name in enumerate(material_names)}\n","index_to_material = {i: name for name, i in material_to_index.items()}\n","print(index_to_material)\n","\n","\n","# --- 절연체 파일 처리 ---\n","for material_name, filename in material_files:\n","    file_path = os.path.join(DATA_DIR, filename)\n","    print(f\"처리 중: {filename} (materials: {material_name})\")\n","    data = pd.read_csv(file_path, header=None)\n","\n","    num_samples_in_file = data.shape[0]\n","    if num_samples_in_file == 0:\n","        print(f\"  경고: 파일이 비어있습니다.\")\n","        continue\n","    print(f\"  로드된 샘플 수: {num_samples_in_file}\")\n","\n","    # 파장, n, k 분리\n","    wavelengths = data.iloc[:,\n","        0].values  # 첫 번째 열: 파장\n","    n = data.iloc[:, 1].values            # 두 번째 열: n\n","    k = data.iloc[:, 2].values            # 세 번째 열: k\n","\n","    # 복소 굴절률 생성\n","    n_complex = n + 1j * k\n","\n","    # insulator_data에 저장\n","    material_data[material_name] = (wavelengths, n_complex)\n","# --- 공기 굴절률 처리 ---\n","material_data['Air'] = np.ones(NUM_WAVELENGTHS, dtype=np.complex128)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q6wcpY_qWA9B","executionInfo":{"status":"ok","timestamp":1752135964888,"user_tz":-540,"elapsed":19,"user":{"displayName":"이스타의행운","userId":"08935579543132628543"}},"outputId":"4756a9fc-9840-46da-dc3c-61e1a9ece747"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['LiF', 'SiO2', 'WO3', 'Ag']\n","{0: 'Ag', 1: 'LiF', 2: 'SiO2', 3: 'WO3'}\n","처리 중: LiF_380.csv (materials: LiF)\n","  로드된 샘플 수: 401\n","처리 중: SiO2_380.csv (materials: SiO2)\n","  로드된 샘플 수: 401\n","처리 중: WO3_380.csv (materials: WO3)\n","  로드된 샘플 수: 401\n","처리 중: Ag_380.csv (materials: Ag)\n","  로드된 샘플 수: 401\n"]}]},{"cell_type":"code","source":["# ────────────────────────────────────────────────────────────────────────────\n","# (A) 환경 및 TMM 파라미터 준비\n","# ────────────────────────────────────────────────────────────────────────────\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# (1) 파장 정의 (380~780 nm, 5 nm 간격 → 81개)\n","WAVELENGTHS = np.arange(380, 781, 1)            # [380, 385, ..., 780]\n","NUM_WAVELENGTHS = len(WAVELENGTHS)             # 81\n","λ_tensor_global = torch.tensor(WAVELENGTHS, dtype=torch.float64).to(device) * 1e-9  # [m 단위]\n","\n","material_sequence = ['Ag', 'WO3', 'Ag']\n","\n","#    각 층(material_sequence)의 복소 n(λ) 배열을 numpy로부터 추출\n","n_list_np_arrays = [material_data[mat][1] for mat in material_sequence]  # 길이 3 리스트\n","\n","\n","#    numpy → torch로 변환 & device로 이동 (dtype=torch.cdouble)\n","n_list_torch = torch.stack([\n","    torch.from_numpy(arr).to(torch.complex64) for arr in n_list_np_arrays\n","], dim=0).to(device)\n","\n","print(n_list_torch.shape)\n","\n","#    입사·출사 매질 굴절률 (예: 입사=SiO2, 출사=Air)\n","n_i_np = material_data['SiO2'][1]   # numpy complex\n","n_s_np = material_data['Air']       # numpy complex\n","\n","n_i_torch = torch.from_numpy(n_i_np).to(torch.complex64).to(device)\n","n_s_torch = torch.from_numpy(n_s_np).to(torch.complex64).to(device)\n","\n","# (4) 두께 그리드 (nm 단위)\n","d1_list_nm = np.arange(5, 41, 5)       # [5, 10, 15, 20, 25, 30]\n","d2_list_nm = np.arange(150, 801, 5)  # [150, 160, …, 1000]\n","d3_list_nm = np.arange(5, 41, 5)        # [5, 10, 15, 20, 25, 30]\n","print(len(d1_list_nm)*len(d2_list_nm)*len(d3_list_nm))\n","# (5) 샘플 저장용 리스트\n","all_d_tilde = []    # [(tilde1, tilde2, tilde3), …]\n","all_T_target = []   # [array([T(λ), …, T(λ)]), …]"],"metadata":{"id":"qKoHAdIqxvHg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1752135966732,"user_tz":-540,"elapsed":79,"user":{"displayName":"이스타의행운","userId":"08935579543132628543"}},"outputId":"73ca5350-9201-4254-e44c-81892057bb0d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","torch.Size([3, 401])\n","8384\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","import os\n","\n","# ────────────────────────────────────────────────────────────────────────────\n","# (A) 환경 및 TMM 파라미터 준비\n","# ────────────────────────────────────────────────────────────────────────────\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# (1) 파장 정의 (380~780 nm, 1 nm 간격 → 401개)\n","WAVELENGTHS = np.arange(380, 781, 1)            # [380, 381, ..., 780] (nm)\n","NUM_WAVELENGTHS = len(WAVELENGTHS)              # 401\n","# 반드시 “nm → m 단위 torch.Tensor” 형태로 넘겨야 함\n","λ_tensor_global = (\n","    torch.tensor(WAVELENGTHS, dtype=torch.float64)\n","    .to(device)\n","    * 1e-9\n",")  # [m 단위], shape: (401,)\n","\n","# (2) material_data에서 복소 굴절률 로드\n","#    material_sequence: 3층 구조 예시\n","material_sequence = ['Ag', 'WO3', 'Ag']\n","n_list_np_arrays = [material_data[mat][1] for mat in material_sequence]  # [ (401,), (401,), (401,) ]\n","\n","#    numpy → torch로 변환 & device로 이동 (dtype=torch.complex128)\n","n_list_torch = torch.stack([\n","    torch.from_numpy(arr).to(torch.complex128) for arr in n_list_np_arrays\n","], dim=0).to(device)  # shape: (3, 401), dtype: complex128\n","\n","#    입사·출사 매질 굴절률 (예: 입사=SiO2, 출사=Air)\n","n_i_np = material_data['SiO2'][1]   # numpy complex128 shape: (401,)\n","n_s_np = material_data['Air']       # numpy complex128 shape: (401,)\n","\n","n_i_torch = torch.from_numpy(n_i_np).to(torch.complex128).to(device)  # shape: (401,)\n","n_s_torch = torch.from_numpy(n_s_np).to(torch.complex128).to(device)  # shape: (401,)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k0T0xZnUaIpf","executionInfo":{"status":"ok","timestamp":1752135967915,"user_tz":-540,"elapsed":10,"user":{"displayName":"이스타의행운","userId":"08935579543132628543"}},"outputId":"eae969a1-7004-49da-99a3-02d61236bb93"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}]},{"cell_type":"code","source":["# ────────────────────────────────────────────────────────────────────────────\n","# (A-1) TMMNetwork 정의 (클래스 이름, dtype 통일, calculate() 추가)\n","# ────────────────────────────────────────────────────────────────────────────\n","class TMMNetwork(nn.Module):\n","    def __init__(self, n_list_input, n_i_input, n_s_input, wavelengths_m_tensor):\n","        super().__init__()\n","        dtype_complex = torch.complex128\n","        dtype_float = torch.float64\n","\n","        # 기준길이 L0 (이제 tilde는 안 쓰므로 없어도 무방하지만 유지 가능)\n","        n_list_real = n_list_input.real\n","        n_list_imag = -torch.abs(n_list_input.imag)\n","        self.register_buffer('n_list', torch.complex(n_list_real, n_list_imag))\n","        self.register_buffer('n_i', n_i_input.to(dtype_complex))\n","        self.register_buffer('n_s', n_s_input.to(dtype_complex))\n","\n","        # 파수 k0\n","        k0 = 2 * torch.pi / torch.clamp(wavelengths_m_tensor, min=1e-20)\n","        self.register_buffer('k0', k0)  # 이제 k0_tilde 아님\n","\n","        self.num_layers = self.n_list.shape[0]\n","        self.num_wavelengths = wavelengths_m_tensor.shape[0]\n","\n","    def forward(self, thicknesses_nm):\n","        device = thicknesses_nm.device\n","        dtype_complex = torch.complex128\n","        imag_unit = torch.tensor(1j, dtype=dtype_complex, device=device)\n","\n","        if thicknesses_nm.ndim == 1:\n","            thicknesses_nm = thicknesses_nm.unsqueeze(0)  # (1, 3)\n","        B = thicknesses_nm.shape[0]  # batch size\n","\n","        # Convert to meters\n","        thicknesses_m = thicknesses_nm * 1e-9  # (B, 3)\n","\n","        # (B, 3, 401): broadcasting layer × wavelength\n","        delta = thicknesses_m[:, :, None] * self.k0[None, None, :] * self.n_list[None, :, :]\n","\n","        cosδ = torch.cos(delta)\n","        sinδ = torch.sin(delta)\n","\n","        # 초기 M_total: (B, 401, 2, 2)\n","        M_total = torch.eye(2, dtype=dtype_complex, device=device).repeat(B, self.num_wavelengths, 1, 1)\n","\n","        for j in range(self.num_layers):\n","            Yj = self.n_list[j] * 2.654e-3  # (401,)\n","\n","            sin_j = sinδ[:, j, :]\n","            cos_j = cosδ[:, j, :]\n","\n","            m11 = cos_j\n","            m12 = imag_unit * sin_j / Yj[None, :]\n","            m21 = imag_unit * Yj[None, :] * sin_j\n","            m22 = cos_j\n","\n","            Mj = torch.stack([\n","                torch.stack([m11, m12], dim=-1),\n","                torch.stack([m21, m22], dim=-1)\n","            ], dim=-2)  # shape: (B, 401, 2, 2)\n","\n","            M_total = torch.matmul(M_total, Mj)\n","\n","        Y_in = self.n_i * 2.654e-3\n","        Y_out = self.n_s * 2.654e-3\n","\n","        B_ = M_total[:, :, 0, 0] + M_total[:, :, 0, 1] * Y_out\n","        C_ = M_total[:, :, 1, 0] + M_total[:, :, 1, 1] * Y_out\n","        denom = Y_in * B_ + C_\n","        t1 = (2 * Y_in) / denom  # (B, 401)\n","\n","        T = (torch.real(self.n_s) / torch.real(self.n_i)) * torch.abs(t1) ** 2\n","        return T.to(torch.float64)  # (B, 401)\n","# ────────────────────────────────────────────────────────────────────────────\n","# (A-2) 단일 시험용 스펙트럼 계산 (디버깅 / 시각화)\n","# ────────────────────────────────────────────────────────────────────────────\n","model = TMMNetwork(n_list_torch, n_i_torch, n_s_torch, λ_tensor_global).to(device)"],"metadata":{"id":"V33NauzAxlVI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import random\n","import numpy as np\n","import torch\n","\n","# 시드 고정 (재현성 확보)\n","\n","def seed_everything(seed=42):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)  # if using multi-GPU\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","seed_everything(42)  # 시드 고정 실행"],"metadata":{"id":"5p_PSFaJx_Y3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title\n","from torch.utils.data import random_split\n","all_d = []\n","all_T_target = []\n","\n","print(\"Generating (normalized_d → T_target) pairs ...\")\n","for d1 in d1_list_nm:\n","    for d2 in d2_list_nm:\n","        for d3 in d3_list_nm:\n","            # (a) [d1, d2, d3]\n","            d_nm_vec = torch.tensor([[d1, d2, d3]], dtype=torch.float64)  # shape: (1, 3)\n","            all_d.append(d_nm_vec.cpu().numpy())\n","            # (b) TMM forward\n","            T_spec = model(d_nm_vec.to(device))  # forward(nm 단위 두께)\n","            all_T_target.append(T_spec.detach().cpu().numpy())\n","# NumPy 배열로 변환\n","all_d = np.array(all_d, dtype=np.float64)   # shape: (3096, 3)\n","#ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ두께표준화ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n","THICKNESS_MIN = torch.tensor(np.min(all_d, axis=0), dtype=torch.float64, device=device)\n","THICKNESS_MAX = torch.tensor(np.max(all_d, axis=0), dtype=torch.float64, device=device)\n","def standardize_thickness(d_nm_array):\n","    d_nm_array = torch.tensor(d_nm_array, dtype=torch.float64, device=THICKNESS_MIN.device) if not isinstance(d_nm_array, torch.Tensor) else d_nm_array.to(dtype=torch.float64, device=THICKNESS_MIN.device)\n","    return (d_nm_array - THICKNESS_MIN) / (THICKNESS_MAX - THICKNESS_MIN)\n","\n","def destandardize_thickness(d_norm_array):\n","    d_norm_array = d_norm_array.clone().to(dtype=torch.float64, device=THICKNESS_MIN.device)\n","    return d_norm_array * (THICKNESS_MAX - THICKNESS_MIN) + THICKNESS_MIN\n","\n","all_d_norm = standardize_thickness(all_d)  # shape: (3096, 3)\n","d_nm_check = destandardize_thickness(all_d_norm)\n","\n","\n","#ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ스펙트럼표준화ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n","all_T_target = np.array(all_T_target, dtype=np.float64) # shape: (3096, 401)\n","print(\"Total samples:\", all_d.shape[0])  # 3096\n","\n","SPECTRUM_MIN = torch.tensor(np.min(all_T_target, axis=0), dtype=torch.float64).to(device)\n","SPECTRUM_MAX = torch.tensor(np.max(all_T_target, axis=0), dtype=torch.float64).to(device)\n","def standardize_spectrum(T_array):\n","    T_tensor = torch.tensor(T_array, dtype=torch.float64).to(device)\n","    return (T_tensor - SPECTRUM_MIN) / (SPECTRUM_MAX - SPECTRUM_MIN)\n","\n","def destandardize_spectrum(T_std_array):\n","    return T_std_array * (SPECTRUM_MAX - SPECTRUM_MIN) + SPECTRUM_MIN\n","\n","# destandardize_spectrum의 결과(GPU 텐서)를 .cpu().numpy()로 변환하여 비교합니다.\n","all_T_target_std = standardize_spectrum(all_T_target)\n","restored_T = destandardize_spectrum(all_T_target_std)\n","assert np.allclose(restored_T.cpu().numpy(), all_T_target, atol=1e-5)\n","\n","# ────────────────────────────────────────────────────────────────────────────\n","# (B) Dataset / DataLoader 구축\n","# ────────────────────────────────────────────────────────────────────────────\n","class BiTMMNormalizedDataset(Dataset):\n","    def __init__(self, d_norm_array, T_array):\n","        self.d_norm = d_norm_array.clone().to(dtype=torch.float64)\n","        self.T_spec = T_array.clone().detach().to(dtype=torch.float64) if isinstance(T_array, torch.Tensor) else torch.tensor(T_array, dtype=torch.float64)\n","\n","    def __len__(self):\n","        return self.d_norm.shape[0]\n","\n","    def __getitem__(self, idx):\n","        return {\n","            'd_norm': self.d_norm[idx],       # shape: (3,)1\n","            'T_target': self.T_spec[idx]      # shape: (401,)\n","        }\n","\n","batch_size = 32\n","\n","\n","# 시드 고정\n","seed = 42\n","g = torch.Generator().manual_seed(seed)\n","\n","# Dataset 생성\n","dataset = BiTMMNormalizedDataset(all_d_norm, all_T_target_std)\n","\n","# 전체 길이 및 split 비율\n","total_size = len(dataset)\n","train_size = int(0.9 * total_size)\n","val_size   = int(0.05 * total_size)\n","test_size  = total_size - train_size - val_size  # 나머지\n","\n","print(f\"Total: {total_size}, Train: {train_size}, Val: {val_size}, Test: {test_size}\")\n","\n","# Dataset 분할\n","train_dataset, val_dataset, test_dataset = random_split(\n","    dataset, [train_size, val_size, test_size], generator=g  # g로 통일\n",")\n","\n","# DataLoader 정의\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  num_workers=0, generator=g)\n","val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, num_workers=0)\n","test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, num_workers=0)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"suR-NIYM9jjV","executionInfo":{"status":"ok","timestamp":1752135990878,"user_tz":-540,"elapsed":18657,"user":{"displayName":"이스타의행운","userId":"08935579543132628543"}},"outputId":"0f357b1f-c46f-42ee-8e1d-9df828cd5c69"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Generating (normalized_d → T_target) pairs ...\n","Total samples: 8384\n","Total: 8384, Train: 7545, Val: 419, Test: 420\n"]}]},{"cell_type":"code","source":["# @title\n","from torch.utils.data import random_split\n","all_d = []\n","all_T_target = []\n","\n","print(\"Generating (normalized_d → T_target) pairs ...\")\n","for d1 in d1_list_nm:\n","    for d2 in d2_list_nm:\n","        for d3 in d3_list_nm:\n","            # (a) [d1, d2, d3]\n","            d_nm_vec = torch.tensor([[d1, d2, d3]], dtype=torch.float64)  # shape: (1, 3)\n","            all_d.append(d_nm_vec.cpu().numpy())\n","            # (b) TMM forward\n","            T_spec = model(d_nm_vec.to(device))  # forward(nm 단위 두께)\n","            all_T_target.append(T_spec.detach().cpu().numpy())\n","# NumPy 배열로 변환\n","all_d = np.array(all_d, dtype=np.float64)   # shape: (3096, 3)\n","#ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ두께표준화ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n","THICKNESS_MEAN = torch.tensor(np.mean(all_d, axis=0), dtype=torch.float64, device=device)\n","THICKNESS_STD  = torch.tensor(np.std(all_d, axis=0),  dtype=torch.float64, device=device)\n","def standardize_thickness(d_nm_array):\n","    if not isinstance(d_nm_array, torch.Tensor):\n","        d_nm_array = torch.tensor(d_nm_array, dtype=torch.float64, device=THICKNESS_MEAN.device)\n","    else:\n","        d_nm_array = d_nm_array.to(dtype=torch.float64, device=THICKNESS_MEAN.device)\n","    return (d_nm_array - THICKNESS_MEAN) / THICKNESS_STD\n","\n","def destandardize_thickness(d_norm_array):\n","    d_norm_array = d_norm_array.clone().to(dtype=torch.float64, device=THICKNESS_MEAN.device)\n","    return d_norm_array * THICKNESS_STD + THICKNESS_MEAN\n","\n","all_d_norm = standardize_thickness(all_d)  # shape: (3096, 3)\n","d_nm_check = destandardize_thickness(all_d_norm)\n","\n","\n","#ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ스펙트럼표준화ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n","all_T_target = np.array(all_T_target, dtype=np.float64) # shape: (3096, 401)\n","print(\"Total samples:\", all_d.shape[0])  # 3096\n","\n","SPECTRUM_MEAN = torch.tensor(np.mean(all_T_target, axis=0), dtype=torch.float64).to(device)\n","SPECTRUM_STD = torch.tensor(np.std(all_T_target, axis=0), dtype=torch.float64).to(device)\n","def standardize_spectrum(T_array):\n","    # 1. 입력받은 NumPy 배열(T_array)을 PyTorch 텐서로 변환하고 GPU로 보냅니다.\n","    T_tensor = torch.tensor(T_array, dtype=torch.float64).to(device)\n","\n","    # 2. 이제 GPU 텐서끼리의 연산을 수행합니다.\n","    return (T_tensor - SPECTRUM_MEAN) / SPECTRUM_STD\n","def destandardize_spectrum(T_std_array):\n","    return T_std_array * SPECTRUM_STD + SPECTRUM_MEAN\n","\n","# destandardize_spectrum의 결과(GPU 텐서)를 .cpu().numpy()로 변환하여 비교합니다.\n","all_T_target_std = standardize_spectrum(all_T_target)\n","restored_T = destandardize_spectrum(all_T_target_std)\n","assert np.allclose(restored_T.cpu().numpy(), all_T_target, atol=1e-5)\n","\n","# ────────────────────────────────────────────────────────────────────────────\n","# (B) Dataset / DataLoader 구축\n","# ────────────────────────────────────────────────────────────────────────────\n","class BiTMMNormalizedDataset(Dataset):\n","    def __init__(self, d_norm_array, T_array):\n","        self.d_norm = d_norm_array.clone().to(dtype=torch.float64)\n","        self.T_spec = T_array.clone().detach().to(dtype=torch.float64) if isinstance(T_array, torch.Tensor) else torch.tensor(T_array, dtype=torch.float64)\n","\n","    def __len__(self):\n","        return self.d_norm.shape[0]\n","\n","    def __getitem__(self, idx):\n","        return {\n","            'd_norm': self.d_norm[idx],       # shape: (3,)1\n","            'T_target': self.T_spec[idx]      # shape: (401,)\n","        }\n","\n","\n","# 시드 고정\n","seed = 42\n","g = torch.Generator().manual_seed(seed)\n","\n","# Dataset 생성\n","dataset = BiTMMNormalizedDataset(all_d_norm, all_T_target_std)\n","\n","# 전체 길이 및 split 비율\n","total_size = len(dataset)\n","train_size = int(0.9 * total_size)\n","val_size   = int(0.05 * total_size)\n","test_size  = total_size - train_size - val_size  # 나머지\n","\n","print(f\"Total: {total_size}, Train: {train_size}, Val: {val_size}, Test: {test_size}\")\n","\n","# Dataset 분할\n","train_dataset, val_dataset, test_dataset = random_split(\n","    dataset, [train_size, val_size, test_size], generator=g  # g로 통일\n",")\n","\n","# DataLoader 정의\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  num_workers=0, generator=g)\n","val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, num_workers=0)\n","test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, num_workers=0)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wFDP0rmDaMYO","executionInfo":{"status":"ok","timestamp":1750228969127,"user_tz":-540,"elapsed":16641,"user":{"displayName":"이스타의행운","userId":"08935579543132628543"}},"outputId":"7ee263a4-f2ef-4d76-ab86-55948b27c347","cellView":"form"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Generating (normalized_d → T_target) pairs ...\n","Total samples: 7399\n","Total: 7399, Train: 6659, Val: 369, Test: 371\n"]}]},{"cell_type":"markdown","source":["# **Forward Training **"],"metadata":{"id":"GF1IxuMUxqpY"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import os\n","\n","# ──────────────────────────────────────────────────────\n","# 모델 정의\n","# ──────────────────────────────────────────────────────\n","\n","batch_size = 128\n","\n","N1 = 1190\n","N2 = 824\n","N3 = 920\n","\n","class ForwardNet(nn.Module):\n","    def __init__(self, input_dim=3, output_dim=401):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","    nn.Linear(3, N1),\n","    nn.ReLU(),\n","    nn.Linear(N1, N2),\n","    nn.ReLU(),\n","    nn.Linear(N2, N3),\n","    nn.ReLU(),\n","    nn.Linear(N3, 401)\n",")\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","# 모델 및 옵티마이저 초기화\n","forward_net = ForwardNet(input_dim=3, output_dim=401).to(device).to(torch.float64)\n","optimizer = torch.optim.Adam(forward_net.parameters(), lr=1e-3)\n","\n","# ──────────────────────────────────────────────────────\n","# 학습 파라미터 설정\n","# ──────────────────────────────────────────────────────\n","num_epochs =600\n","output_dir = \"C:/Users/PC/Desktop/Deep/\"\n","os.makedirs(output_dir, exist_ok=True)\n","\n","train_loss_history = []\n","val_loss_history = []\n","# ──────────────────────────────────────────────────────\n","# 학습 루프\n","# ──────────────────────────────────────────────────────\n","for epoch in range(num_epochs):\n","    forward_net.train()\n","    total_loss = 0.0\n","\n","    for batch in train_loader:\n","        d_norm = batch['d_norm'].to(device)        # [B, 3]\n","        T_target = batch['T_target'].to(device)    # [B, 401]\n","\n","        optimizer.zero_grad()\n","        T_pred = forward_net(d_norm)\n","        loss = F.mse_loss(T_pred, T_target)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    avg_train_loss = total_loss / len(train_loader)\n","    train_loss_history.append(avg_train_loss)\n","    print(f\"[Train] Epoch {epoch+1}, Avg Loss = {avg_train_loss:.6f}\")\n","\n","    # ───────────────────────────────────────────\n","    # Validation\n","    # ───────────────────────────────────────────\n","    forward_net.eval()\n","    val_loss_total = 0.0\n","    with torch.no_grad():\n","        for batch in val_loader:\n","            d_norm = batch['d_norm'].to(device)\n","            T_target = batch['T_target'].to(device)\n","\n","            T_pred = forward_net(d_norm)\n","            loss = F.mse_loss(T_pred, T_target)\n","            val_loss_total += loss.item()\n","\n","    avg_val_loss = val_loss_total / len(val_loader)\n","    val_loss_history.append(avg_val_loss)\n","    print(f\"[Val]   Epoch [{epoch+1}/{num_epochs}] done. Avg Loss: {avg_val_loss:.6f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lsU_1U9mxoxo","outputId":"681e04f2-4704-4f9a-c39d-7aa7d7e96716","executionInfo":{"status":"ok","timestamp":1752138363370,"user_tz":-540,"elapsed":851187,"user":{"displayName":"이스타의행운","userId":"08935579543132628543"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[Train] Epoch 1, Avg Loss = 0.025017\n","[Val]   Epoch [1/600] done. Avg Loss: 0.022380\n","[Train] Epoch 2, Avg Loss = 0.022529\n","[Val]   Epoch [2/600] done. Avg Loss: 0.021840\n","[Train] Epoch 3, Avg Loss = 0.021614\n","[Val]   Epoch [3/600] done. Avg Loss: 0.020993\n","[Train] Epoch 4, Avg Loss = 0.020435\n","[Val]   Epoch [4/600] done. Avg Loss: 0.018758\n","[Train] Epoch 5, Avg Loss = 0.018667\n","[Val]   Epoch [5/600] done. Avg Loss: 0.017885\n","[Train] Epoch 6, Avg Loss = 0.016694\n","[Val]   Epoch [6/600] done. Avg Loss: 0.015015\n","[Train] Epoch 7, Avg Loss = 0.014739\n","[Val]   Epoch [7/600] done. Avg Loss: 0.013633\n","[Train] Epoch 8, Avg Loss = 0.013267\n","[Val]   Epoch [8/600] done. Avg Loss: 0.012304\n","[Train] Epoch 9, Avg Loss = 0.012018\n","[Val]   Epoch [9/600] done. Avg Loss: 0.011191\n","[Train] Epoch 10, Avg Loss = 0.010457\n","[Val]   Epoch [10/600] done. Avg Loss: 0.010122\n","[Train] Epoch 11, Avg Loss = 0.009094\n","[Val]   Epoch [11/600] done. Avg Loss: 0.007966\n","[Train] Epoch 12, Avg Loss = 0.007542\n","[Val]   Epoch [12/600] done. Avg Loss: 0.007138\n","[Train] Epoch 13, Avg Loss = 0.006465\n","[Val]   Epoch [13/600] done. Avg Loss: 0.005974\n","[Train] Epoch 14, Avg Loss = 0.005875\n","[Val]   Epoch [14/600] done. Avg Loss: 0.005696\n","[Train] Epoch 15, Avg Loss = 0.005477\n","[Val]   Epoch [15/600] done. Avg Loss: 0.005412\n","[Train] Epoch 16, Avg Loss = 0.004909\n","[Val]   Epoch [16/600] done. Avg Loss: 0.004715\n","[Train] Epoch 17, Avg Loss = 0.004544\n","[Val]   Epoch [17/600] done. Avg Loss: 0.004926\n","[Train] Epoch 18, Avg Loss = 0.004406\n","[Val]   Epoch [18/600] done. Avg Loss: 0.004234\n","[Train] Epoch 19, Avg Loss = 0.004488\n","[Val]   Epoch [19/600] done. Avg Loss: 0.004223\n","[Train] Epoch 20, Avg Loss = 0.003917\n","[Val]   Epoch [20/600] done. Avg Loss: 0.003877\n","[Train] Epoch 21, Avg Loss = 0.003769\n","[Val]   Epoch [21/600] done. Avg Loss: 0.004723\n","[Train] Epoch 22, Avg Loss = 0.003848\n","[Val]   Epoch [22/600] done. Avg Loss: 0.003767\n","[Train] Epoch 23, Avg Loss = 0.003568\n","[Val]   Epoch [23/600] done. Avg Loss: 0.003639\n","[Train] Epoch 24, Avg Loss = 0.003376\n","[Val]   Epoch [24/600] done. Avg Loss: 0.003341\n","[Train] Epoch 25, Avg Loss = 0.003436\n","[Val]   Epoch [25/600] done. Avg Loss: 0.004620\n","[Train] Epoch 26, Avg Loss = 0.003494\n","[Val]   Epoch [26/600] done. Avg Loss: 0.003320\n","[Train] Epoch 27, Avg Loss = 0.003049\n","[Val]   Epoch [27/600] done. Avg Loss: 0.002942\n","[Train] Epoch 28, Avg Loss = 0.002955\n","[Val]   Epoch [28/600] done. Avg Loss: 0.002924\n","[Train] Epoch 29, Avg Loss = 0.002947\n","[Val]   Epoch [29/600] done. Avg Loss: 0.003161\n","[Train] Epoch 30, Avg Loss = 0.002845\n","[Val]   Epoch [30/600] done. Avg Loss: 0.003136\n","[Train] Epoch 31, Avg Loss = 0.002916\n","[Val]   Epoch [31/600] done. Avg Loss: 0.002803\n","[Train] Epoch 32, Avg Loss = 0.002747\n","[Val]   Epoch [32/600] done. Avg Loss: 0.002913\n","[Train] Epoch 33, Avg Loss = 0.002786\n","[Val]   Epoch [33/600] done. Avg Loss: 0.002622\n","[Train] Epoch 34, Avg Loss = 0.002668\n","[Val]   Epoch [34/600] done. Avg Loss: 0.002530\n","[Train] Epoch 35, Avg Loss = 0.002488\n","[Val]   Epoch [35/600] done. Avg Loss: 0.002545\n","[Train] Epoch 36, Avg Loss = 0.002460\n","[Val]   Epoch [36/600] done. Avg Loss: 0.002451\n","[Train] Epoch 37, Avg Loss = 0.002506\n","[Val]   Epoch [37/600] done. Avg Loss: 0.002343\n","[Train] Epoch 38, Avg Loss = 0.002631\n","[Val]   Epoch [38/600] done. Avg Loss: 0.002887\n","[Train] Epoch 39, Avg Loss = 0.002655\n","[Val]   Epoch [39/600] done. Avg Loss: 0.002539\n","[Train] Epoch 40, Avg Loss = 0.002329\n","[Val]   Epoch [40/600] done. Avg Loss: 0.002396\n","[Train] Epoch 41, Avg Loss = 0.002448\n","[Val]   Epoch [41/600] done. Avg Loss: 0.002447\n","[Train] Epoch 42, Avg Loss = 0.002360\n","[Val]   Epoch [42/600] done. Avg Loss: 0.002853\n","[Train] Epoch 43, Avg Loss = 0.002320\n","[Val]   Epoch [43/600] done. Avg Loss: 0.002189\n","[Train] Epoch 44, Avg Loss = 0.002196\n","[Val]   Epoch [44/600] done. Avg Loss: 0.002229\n","[Train] Epoch 45, Avg Loss = 0.002221\n","[Val]   Epoch [45/600] done. Avg Loss: 0.002322\n","[Train] Epoch 46, Avg Loss = 0.002195\n","[Val]   Epoch [46/600] done. Avg Loss: 0.002149\n","[Train] Epoch 47, Avg Loss = 0.002351\n","[Val]   Epoch [47/600] done. Avg Loss: 0.002975\n","[Train] Epoch 48, Avg Loss = 0.002225\n","[Val]   Epoch [48/600] done. Avg Loss: 0.002873\n","[Train] Epoch 49, Avg Loss = 0.002186\n","[Val]   Epoch [49/600] done. Avg Loss: 0.002098\n","[Train] Epoch 50, Avg Loss = 0.002084\n","[Val]   Epoch [50/600] done. Avg Loss: 0.002256\n","[Train] Epoch 51, Avg Loss = 0.002154\n","[Val]   Epoch [51/600] done. Avg Loss: 0.002319\n","[Train] Epoch 52, Avg Loss = 0.002108\n","[Val]   Epoch [52/600] done. Avg Loss: 0.002103\n","[Train] Epoch 53, Avg Loss = 0.001944\n","[Val]   Epoch [53/600] done. Avg Loss: 0.002146\n","[Train] Epoch 54, Avg Loss = 0.002051\n","[Val]   Epoch [54/600] done. Avg Loss: 0.001996\n","[Train] Epoch 55, Avg Loss = 0.001929\n","[Val]   Epoch [55/600] done. Avg Loss: 0.002320\n","[Train] Epoch 56, Avg Loss = 0.002140\n","[Val]   Epoch [56/600] done. Avg Loss: 0.002501\n","[Train] Epoch 57, Avg Loss = 0.001985\n","[Val]   Epoch [57/600] done. Avg Loss: 0.002035\n","[Train] Epoch 58, Avg Loss = 0.001849\n","[Val]   Epoch [58/600] done. Avg Loss: 0.002102\n","[Train] Epoch 59, Avg Loss = 0.001935\n","[Val]   Epoch [59/600] done. Avg Loss: 0.002030\n","[Train] Epoch 60, Avg Loss = 0.001900\n","[Val]   Epoch [60/600] done. Avg Loss: 0.001740\n","[Train] Epoch 61, Avg Loss = 0.001801\n","[Val]   Epoch [61/600] done. Avg Loss: 0.001823\n","[Train] Epoch 62, Avg Loss = 0.001984\n","[Val]   Epoch [62/600] done. Avg Loss: 0.001798\n","[Train] Epoch 63, Avg Loss = 0.001775\n","[Val]   Epoch [63/600] done. Avg Loss: 0.001878\n","[Train] Epoch 64, Avg Loss = 0.001926\n","[Val]   Epoch [64/600] done. Avg Loss: 0.001747\n","[Train] Epoch 65, Avg Loss = 0.001772\n","[Val]   Epoch [65/600] done. Avg Loss: 0.001864\n","[Train] Epoch 66, Avg Loss = 0.001905\n","[Val]   Epoch [66/600] done. Avg Loss: 0.001794\n","[Train] Epoch 67, Avg Loss = 0.001862\n","[Val]   Epoch [67/600] done. Avg Loss: 0.001776\n","[Train] Epoch 68, Avg Loss = 0.001838\n","[Val]   Epoch [68/600] done. Avg Loss: 0.001894\n","[Train] Epoch 69, Avg Loss = 0.001656\n","[Val]   Epoch [69/600] done. Avg Loss: 0.001835\n","[Train] Epoch 70, Avg Loss = 0.001677\n","[Val]   Epoch [70/600] done. Avg Loss: 0.001751\n","[Train] Epoch 71, Avg Loss = 0.001793\n","[Val]   Epoch [71/600] done. Avg Loss: 0.001772\n","[Train] Epoch 72, Avg Loss = 0.001694\n","[Val]   Epoch [72/600] done. Avg Loss: 0.002838\n","[Train] Epoch 73, Avg Loss = 0.001792\n","[Val]   Epoch [73/600] done. Avg Loss: 0.001619\n","[Train] Epoch 74, Avg Loss = 0.001656\n","[Val]   Epoch [74/600] done. Avg Loss: 0.001905\n","[Train] Epoch 75, Avg Loss = 0.001789\n","[Val]   Epoch [75/600] done. Avg Loss: 0.001703\n","[Train] Epoch 76, Avg Loss = 0.001621\n","[Val]   Epoch [76/600] done. Avg Loss: 0.001626\n","[Train] Epoch 77, Avg Loss = 0.001754\n","[Val]   Epoch [77/600] done. Avg Loss: 0.001603\n","[Train] Epoch 78, Avg Loss = 0.001567\n","[Val]   Epoch [78/600] done. Avg Loss: 0.001444\n","[Train] Epoch 79, Avg Loss = 0.001636\n","[Val]   Epoch [79/600] done. Avg Loss: 0.001887\n","[Train] Epoch 80, Avg Loss = 0.001813\n","[Val]   Epoch [80/600] done. Avg Loss: 0.001824\n","[Train] Epoch 81, Avg Loss = 0.001495\n","[Val]   Epoch [81/600] done. Avg Loss: 0.001450\n","[Train] Epoch 82, Avg Loss = 0.001575\n","[Val]   Epoch [82/600] done. Avg Loss: 0.001507\n","[Train] Epoch 83, Avg Loss = 0.001665\n","[Val]   Epoch [83/600] done. Avg Loss: 0.001731\n","[Train] Epoch 84, Avg Loss = 0.001519\n","[Val]   Epoch [84/600] done. Avg Loss: 0.001471\n","[Train] Epoch 85, Avg Loss = 0.001588\n","[Val]   Epoch [85/600] done. Avg Loss: 0.002438\n","[Train] Epoch 86, Avg Loss = 0.001636\n","[Val]   Epoch [86/600] done. Avg Loss: 0.001693\n","[Train] Epoch 87, Avg Loss = 0.001680\n","[Val]   Epoch [87/600] done. Avg Loss: 0.001536\n","[Train] Epoch 88, Avg Loss = 0.001522\n","[Val]   Epoch [88/600] done. Avg Loss: 0.001545\n","[Train] Epoch 89, Avg Loss = 0.001416\n","[Val]   Epoch [89/600] done. Avg Loss: 0.001782\n","[Train] Epoch 90, Avg Loss = 0.001569\n","[Val]   Epoch [90/600] done. Avg Loss: 0.001573\n","[Train] Epoch 91, Avg Loss = 0.001574\n","[Val]   Epoch [91/600] done. Avg Loss: 0.001513\n","[Train] Epoch 92, Avg Loss = 0.001471\n","[Val]   Epoch [92/600] done. Avg Loss: 0.001628\n","[Train] Epoch 93, Avg Loss = 0.001633\n","[Val]   Epoch [93/600] done. Avg Loss: 0.001417\n","[Train] Epoch 94, Avg Loss = 0.001456\n","[Val]   Epoch [94/600] done. Avg Loss: 0.001399\n","[Train] Epoch 95, Avg Loss = 0.001416\n","[Val]   Epoch [95/600] done. Avg Loss: 0.001441\n","[Train] Epoch 96, Avg Loss = 0.001416\n","[Val]   Epoch [96/600] done. Avg Loss: 0.001415\n","[Train] Epoch 97, Avg Loss = 0.001758\n","[Val]   Epoch [97/600] done. Avg Loss: 0.001715\n","[Train] Epoch 98, Avg Loss = 0.001437\n","[Val]   Epoch [98/600] done. Avg Loss: 0.001324\n","[Train] Epoch 99, Avg Loss = 0.001528\n","[Val]   Epoch [99/600] done. Avg Loss: 0.001519\n","[Train] Epoch 100, Avg Loss = 0.001524\n","[Val]   Epoch [100/600] done. Avg Loss: 0.001441\n","[Train] Epoch 101, Avg Loss = 0.001373\n","[Val]   Epoch [101/600] done. Avg Loss: 0.001607\n","[Train] Epoch 102, Avg Loss = 0.001429\n","[Val]   Epoch [102/600] done. Avg Loss: 0.001477\n","[Train] Epoch 103, Avg Loss = 0.001501\n","[Val]   Epoch [103/600] done. Avg Loss: 0.001358\n","[Train] Epoch 104, Avg Loss = 0.001334\n","[Val]   Epoch [104/600] done. Avg Loss: 0.001787\n","[Train] Epoch 105, Avg Loss = 0.001456\n","[Val]   Epoch [105/600] done. Avg Loss: 0.001744\n","[Train] Epoch 106, Avg Loss = 0.001311\n","[Val]   Epoch [106/600] done. Avg Loss: 0.001392\n","[Train] Epoch 107, Avg Loss = 0.001501\n","[Val]   Epoch [107/600] done. Avg Loss: 0.001735\n","[Train] Epoch 108, Avg Loss = 0.001526\n","[Val]   Epoch [108/600] done. Avg Loss: 0.001412\n","[Train] Epoch 109, Avg Loss = 0.001308\n","[Val]   Epoch [109/600] done. Avg Loss: 0.001498\n","[Train] Epoch 110, Avg Loss = 0.001408\n","[Val]   Epoch [110/600] done. Avg Loss: 0.001348\n","[Train] Epoch 111, Avg Loss = 0.001422\n","[Val]   Epoch [111/600] done. Avg Loss: 0.001485\n","[Train] Epoch 112, Avg Loss = 0.001438\n","[Val]   Epoch [112/600] done. Avg Loss: 0.001462\n","[Train] Epoch 113, Avg Loss = 0.001400\n","[Val]   Epoch [113/600] done. Avg Loss: 0.001497\n","[Train] Epoch 114, Avg Loss = 0.001591\n","[Val]   Epoch [114/600] done. Avg Loss: 0.001347\n","[Train] Epoch 115, Avg Loss = 0.001339\n","[Val]   Epoch [115/600] done. Avg Loss: 0.001461\n","[Train] Epoch 116, Avg Loss = 0.001304\n","[Val]   Epoch [116/600] done. Avg Loss: 0.001407\n","[Train] Epoch 117, Avg Loss = 0.001257\n","[Val]   Epoch [117/600] done. Avg Loss: 0.001421\n","[Train] Epoch 118, Avg Loss = 0.001379\n","[Val]   Epoch [118/600] done. Avg Loss: 0.001472\n","[Train] Epoch 119, Avg Loss = 0.001413\n","[Val]   Epoch [119/600] done. Avg Loss: 0.001369\n","[Train] Epoch 120, Avg Loss = 0.001247\n","[Val]   Epoch [120/600] done. Avg Loss: 0.001293\n","[Train] Epoch 121, Avg Loss = 0.001368\n","[Val]   Epoch [121/600] done. Avg Loss: 0.001919\n","[Train] Epoch 122, Avg Loss = 0.001346\n","[Val]   Epoch [122/600] done. Avg Loss: 0.001681\n","[Train] Epoch 123, Avg Loss = 0.001323\n","[Val]   Epoch [123/600] done. Avg Loss: 0.001280\n","[Train] Epoch 124, Avg Loss = 0.001258\n","[Val]   Epoch [124/600] done. Avg Loss: 0.001260\n","[Train] Epoch 125, Avg Loss = 0.001267\n","[Val]   Epoch [125/600] done. Avg Loss: 0.001659\n","[Train] Epoch 126, Avg Loss = 0.001375\n","[Val]   Epoch [126/600] done. Avg Loss: 0.001442\n","[Train] Epoch 127, Avg Loss = 0.001364\n","[Val]   Epoch [127/600] done. Avg Loss: 0.001439\n","[Train] Epoch 128, Avg Loss = 0.001433\n","[Val]   Epoch [128/600] done. Avg Loss: 0.002395\n","[Train] Epoch 129, Avg Loss = 0.001421\n","[Val]   Epoch [129/600] done. Avg Loss: 0.001612\n","[Train] Epoch 130, Avg Loss = 0.001161\n","[Val]   Epoch [130/600] done. Avg Loss: 0.001275\n","[Train] Epoch 131, Avg Loss = 0.001285\n","[Val]   Epoch [131/600] done. Avg Loss: 0.001306\n","[Train] Epoch 132, Avg Loss = 0.001206\n","[Val]   Epoch [132/600] done. Avg Loss: 0.001284\n","[Train] Epoch 133, Avg Loss = 0.001308\n","[Val]   Epoch [133/600] done. Avg Loss: 0.001440\n","[Train] Epoch 134, Avg Loss = 0.001229\n","[Val]   Epoch [134/600] done. Avg Loss: 0.001231\n","[Train] Epoch 135, Avg Loss = 0.001285\n","[Val]   Epoch [135/600] done. Avg Loss: 0.001389\n","[Train] Epoch 136, Avg Loss = 0.001259\n","[Val]   Epoch [136/600] done. Avg Loss: 0.001440\n","[Train] Epoch 137, Avg Loss = 0.001309\n","[Val]   Epoch [137/600] done. Avg Loss: 0.001195\n","[Train] Epoch 138, Avg Loss = 0.001170\n","[Val]   Epoch [138/600] done. Avg Loss: 0.001484\n","[Train] Epoch 139, Avg Loss = 0.001189\n","[Val]   Epoch [139/600] done. Avg Loss: 0.001220\n","[Train] Epoch 140, Avg Loss = 0.001243\n","[Val]   Epoch [140/600] done. Avg Loss: 0.001400\n","[Train] Epoch 141, Avg Loss = 0.001368\n","[Val]   Epoch [141/600] done. Avg Loss: 0.001369\n","[Train] Epoch 142, Avg Loss = 0.001249\n","[Val]   Epoch [142/600] done. Avg Loss: 0.001168\n","[Train] Epoch 143, Avg Loss = 0.001103\n","[Val]   Epoch [143/600] done. Avg Loss: 0.001643\n","[Train] Epoch 144, Avg Loss = 0.001173\n","[Val]   Epoch [144/600] done. Avg Loss: 0.001138\n","[Train] Epoch 145, Avg Loss = 0.001226\n","[Val]   Epoch [145/600] done. Avg Loss: 0.001303\n","[Train] Epoch 146, Avg Loss = 0.001240\n","[Val]   Epoch [146/600] done. Avg Loss: 0.001342\n","[Train] Epoch 147, Avg Loss = 0.001257\n","[Val]   Epoch [147/600] done. Avg Loss: 0.001204\n","[Train] Epoch 148, Avg Loss = 0.001143\n","[Val]   Epoch [148/600] done. Avg Loss: 0.001112\n","[Train] Epoch 149, Avg Loss = 0.001178\n","[Val]   Epoch [149/600] done. Avg Loss: 0.001156\n","[Train] Epoch 150, Avg Loss = 0.001236\n","[Val]   Epoch [150/600] done. Avg Loss: 0.001524\n","[Train] Epoch 151, Avg Loss = 0.001323\n","[Val]   Epoch [151/600] done. Avg Loss: 0.001172\n","[Train] Epoch 152, Avg Loss = 0.001246\n","[Val]   Epoch [152/600] done. Avg Loss: 0.001215\n","[Train] Epoch 153, Avg Loss = 0.001085\n","[Val]   Epoch [153/600] done. Avg Loss: 0.001152\n","[Train] Epoch 154, Avg Loss = 0.001285\n","[Val]   Epoch [154/600] done. Avg Loss: 0.001277\n","[Train] Epoch 155, Avg Loss = 0.001222\n","[Val]   Epoch [155/600] done. Avg Loss: 0.001085\n","[Train] Epoch 156, Avg Loss = 0.001105\n","[Val]   Epoch [156/600] done. Avg Loss: 0.001139\n","[Train] Epoch 157, Avg Loss = 0.001120\n","[Val]   Epoch [157/600] done. Avg Loss: 0.001193\n","[Train] Epoch 158, Avg Loss = 0.001104\n","[Val]   Epoch [158/600] done. Avg Loss: 0.001127\n","[Train] Epoch 159, Avg Loss = 0.001603\n","[Val]   Epoch [159/600] done. Avg Loss: 0.001244\n","[Train] Epoch 160, Avg Loss = 0.001106\n","[Val]   Epoch [160/600] done. Avg Loss: 0.001394\n","[Train] Epoch 161, Avg Loss = 0.001044\n","[Val]   Epoch [161/600] done. Avg Loss: 0.001085\n","[Train] Epoch 162, Avg Loss = 0.001102\n","[Val]   Epoch [162/600] done. Avg Loss: 0.001170\n","[Train] Epoch 163, Avg Loss = 0.001170\n","[Val]   Epoch [163/600] done. Avg Loss: 0.001196\n","[Train] Epoch 164, Avg Loss = 0.001137\n","[Val]   Epoch [164/600] done. Avg Loss: 0.001403\n","[Train] Epoch 165, Avg Loss = 0.001312\n","[Val]   Epoch [165/600] done. Avg Loss: 0.001404\n","[Train] Epoch 166, Avg Loss = 0.001101\n","[Val]   Epoch [166/600] done. Avg Loss: 0.001112\n","[Train] Epoch 167, Avg Loss = 0.001070\n","[Val]   Epoch [167/600] done. Avg Loss: 0.001288\n","[Train] Epoch 168, Avg Loss = 0.001148\n","[Val]   Epoch [168/600] done. Avg Loss: 0.001123\n","[Train] Epoch 169, Avg Loss = 0.001140\n","[Val]   Epoch [169/600] done. Avg Loss: 0.001133\n","[Train] Epoch 170, Avg Loss = 0.001063\n","[Val]   Epoch [170/600] done. Avg Loss: 0.001221\n","[Train] Epoch 171, Avg Loss = 0.001369\n","[Val]   Epoch [171/600] done. Avg Loss: 0.001960\n","[Train] Epoch 172, Avg Loss = 0.001145\n","[Val]   Epoch [172/600] done. Avg Loss: 0.001228\n","[Train] Epoch 173, Avg Loss = 0.001045\n","[Val]   Epoch [173/600] done. Avg Loss: 0.001370\n","[Train] Epoch 174, Avg Loss = 0.001169\n","[Val]   Epoch [174/600] done. Avg Loss: 0.001169\n","[Train] Epoch 175, Avg Loss = 0.001120\n","[Val]   Epoch [175/600] done. Avg Loss: 0.001107\n","[Train] Epoch 176, Avg Loss = 0.001094\n","[Val]   Epoch [176/600] done. Avg Loss: 0.001416\n","[Train] Epoch 177, Avg Loss = 0.001064\n","[Val]   Epoch [177/600] done. Avg Loss: 0.001115\n","[Train] Epoch 178, Avg Loss = 0.001093\n","[Val]   Epoch [178/600] done. Avg Loss: 0.001287\n","[Train] Epoch 179, Avg Loss = 0.001093\n","[Val]   Epoch [179/600] done. Avg Loss: 0.001420\n","[Train] Epoch 180, Avg Loss = 0.001114\n","[Val]   Epoch [180/600] done. Avg Loss: 0.001132\n","[Train] Epoch 181, Avg Loss = 0.001089\n","[Val]   Epoch [181/600] done. Avg Loss: 0.001211\n","[Train] Epoch 182, Avg Loss = 0.001175\n","[Val]   Epoch [182/600] done. Avg Loss: 0.001078\n","[Train] Epoch 183, Avg Loss = 0.001034\n","[Val]   Epoch [183/600] done. Avg Loss: 0.001005\n","[Train] Epoch 184, Avg Loss = 0.001052\n","[Val]   Epoch [184/600] done. Avg Loss: 0.001137\n","[Train] Epoch 185, Avg Loss = 0.001193\n","[Val]   Epoch [185/600] done. Avg Loss: 0.001213\n","[Train] Epoch 186, Avg Loss = 0.001146\n","[Val]   Epoch [186/600] done. Avg Loss: 0.001072\n","[Train] Epoch 187, Avg Loss = 0.001046\n","[Val]   Epoch [187/600] done. Avg Loss: 0.001117\n","[Train] Epoch 188, Avg Loss = 0.001226\n","[Val]   Epoch [188/600] done. Avg Loss: 0.001022\n","[Train] Epoch 189, Avg Loss = 0.000983\n","[Val]   Epoch [189/600] done. Avg Loss: 0.001065\n","[Train] Epoch 190, Avg Loss = 0.000995\n","[Val]   Epoch [190/600] done. Avg Loss: 0.001134\n","[Train] Epoch 191, Avg Loss = 0.001097\n","[Val]   Epoch [191/600] done. Avg Loss: 0.001210\n","[Train] Epoch 192, Avg Loss = 0.000982\n","[Val]   Epoch [192/600] done. Avg Loss: 0.001468\n","[Train] Epoch 193, Avg Loss = 0.001019\n","[Val]   Epoch [193/600] done. Avg Loss: 0.000995\n","[Train] Epoch 194, Avg Loss = 0.001263\n","[Val]   Epoch [194/600] done. Avg Loss: 0.001275\n","[Train] Epoch 195, Avg Loss = 0.000973\n","[Val]   Epoch [195/600] done. Avg Loss: 0.001039\n","[Train] Epoch 196, Avg Loss = 0.001032\n","[Val]   Epoch [196/600] done. Avg Loss: 0.001397\n","[Train] Epoch 197, Avg Loss = 0.001049\n","[Val]   Epoch [197/600] done. Avg Loss: 0.000973\n","[Train] Epoch 198, Avg Loss = 0.001092\n","[Val]   Epoch [198/600] done. Avg Loss: 0.001179\n","[Train] Epoch 199, Avg Loss = 0.001158\n","[Val]   Epoch [199/600] done. Avg Loss: 0.001165\n","[Train] Epoch 200, Avg Loss = 0.000955\n","[Val]   Epoch [200/600] done. Avg Loss: 0.000979\n","[Train] Epoch 201, Avg Loss = 0.001042\n","[Val]   Epoch [201/600] done. Avg Loss: 0.001340\n","[Train] Epoch 202, Avg Loss = 0.001011\n","[Val]   Epoch [202/600] done. Avg Loss: 0.001284\n","[Train] Epoch 203, Avg Loss = 0.001082\n","[Val]   Epoch [203/600] done. Avg Loss: 0.001098\n","[Train] Epoch 204, Avg Loss = 0.001065\n","[Val]   Epoch [204/600] done. Avg Loss: 0.001066\n","[Train] Epoch 205, Avg Loss = 0.001001\n","[Val]   Epoch [205/600] done. Avg Loss: 0.000951\n","[Train] Epoch 206, Avg Loss = 0.001020\n","[Val]   Epoch [206/600] done. Avg Loss: 0.001144\n","[Train] Epoch 207, Avg Loss = 0.000992\n","[Val]   Epoch [207/600] done. Avg Loss: 0.001043\n","[Train] Epoch 208, Avg Loss = 0.000979\n","[Val]   Epoch [208/600] done. Avg Loss: 0.001021\n","[Train] Epoch 209, Avg Loss = 0.001145\n","[Val]   Epoch [209/600] done. Avg Loss: 0.001105\n","[Train] Epoch 210, Avg Loss = 0.000985\n","[Val]   Epoch [210/600] done. Avg Loss: 0.001128\n","[Train] Epoch 211, Avg Loss = 0.000974\n","[Val]   Epoch [211/600] done. Avg Loss: 0.001168\n","[Train] Epoch 212, Avg Loss = 0.001049\n","[Val]   Epoch [212/600] done. Avg Loss: 0.000970\n","[Train] Epoch 213, Avg Loss = 0.001027\n","[Val]   Epoch [213/600] done. Avg Loss: 0.001098\n","[Train] Epoch 214, Avg Loss = 0.000989\n","[Val]   Epoch [214/600] done. Avg Loss: 0.001185\n","[Train] Epoch 215, Avg Loss = 0.001007\n","[Val]   Epoch [215/600] done. Avg Loss: 0.001024\n","[Train] Epoch 216, Avg Loss = 0.001095\n","[Val]   Epoch [216/600] done. Avg Loss: 0.001003\n","[Train] Epoch 217, Avg Loss = 0.000903\n","[Val]   Epoch [217/600] done. Avg Loss: 0.001105\n","[Train] Epoch 218, Avg Loss = 0.001163\n","[Val]   Epoch [218/600] done. Avg Loss: 0.000960\n","[Train] Epoch 219, Avg Loss = 0.000922\n","[Val]   Epoch [219/600] done. Avg Loss: 0.001337\n","[Train] Epoch 220, Avg Loss = 0.000950\n","[Val]   Epoch [220/600] done. Avg Loss: 0.000950\n","[Train] Epoch 221, Avg Loss = 0.000952\n","[Val]   Epoch [221/600] done. Avg Loss: 0.001061\n","[Train] Epoch 222, Avg Loss = 0.001057\n","[Val]   Epoch [222/600] done. Avg Loss: 0.001571\n","[Train] Epoch 223, Avg Loss = 0.000987\n","[Val]   Epoch [223/600] done. Avg Loss: 0.001095\n","[Train] Epoch 224, Avg Loss = 0.001126\n","[Val]   Epoch [224/600] done. Avg Loss: 0.001070\n","[Train] Epoch 225, Avg Loss = 0.000957\n","[Val]   Epoch [225/600] done. Avg Loss: 0.001217\n","[Train] Epoch 226, Avg Loss = 0.000974\n","[Val]   Epoch [226/600] done. Avg Loss: 0.001057\n","[Train] Epoch 227, Avg Loss = 0.000925\n","[Val]   Epoch [227/600] done. Avg Loss: 0.001028\n","[Train] Epoch 228, Avg Loss = 0.000907\n","[Val]   Epoch [228/600] done. Avg Loss: 0.001164\n","[Train] Epoch 229, Avg Loss = 0.001090\n","[Val]   Epoch [229/600] done. Avg Loss: 0.000974\n","[Train] Epoch 230, Avg Loss = 0.000972\n","[Val]   Epoch [230/600] done. Avg Loss: 0.000973\n","[Train] Epoch 231, Avg Loss = 0.001091\n","[Val]   Epoch [231/600] done. Avg Loss: 0.000881\n","[Train] Epoch 232, Avg Loss = 0.000943\n","[Val]   Epoch [232/600] done. Avg Loss: 0.001356\n","[Train] Epoch 233, Avg Loss = 0.000908\n","[Val]   Epoch [233/600] done. Avg Loss: 0.001027\n","[Train] Epoch 234, Avg Loss = 0.000967\n","[Val]   Epoch [234/600] done. Avg Loss: 0.000904\n","[Train] Epoch 235, Avg Loss = 0.001022\n","[Val]   Epoch [235/600] done. Avg Loss: 0.000967\n","[Train] Epoch 236, Avg Loss = 0.001027\n","[Val]   Epoch [236/600] done. Avg Loss: 0.001281\n","[Train] Epoch 237, Avg Loss = 0.001157\n","[Val]   Epoch [237/600] done. Avg Loss: 0.000916\n","[Train] Epoch 238, Avg Loss = 0.000856\n","[Val]   Epoch [238/600] done. Avg Loss: 0.000973\n","[Train] Epoch 239, Avg Loss = 0.000890\n","[Val]   Epoch [239/600] done. Avg Loss: 0.000920\n","[Train] Epoch 240, Avg Loss = 0.000911\n","[Val]   Epoch [240/600] done. Avg Loss: 0.000943\n","[Train] Epoch 241, Avg Loss = 0.000995\n","[Val]   Epoch [241/600] done. Avg Loss: 0.001054\n","[Train] Epoch 242, Avg Loss = 0.000876\n","[Val]   Epoch [242/600] done. Avg Loss: 0.000882\n","[Train] Epoch 243, Avg Loss = 0.000931\n","[Val]   Epoch [243/600] done. Avg Loss: 0.001034\n","[Train] Epoch 244, Avg Loss = 0.001023\n","[Val]   Epoch [244/600] done. Avg Loss: 0.001002\n","[Train] Epoch 245, Avg Loss = 0.000917\n","[Val]   Epoch [245/600] done. Avg Loss: 0.001067\n","[Train] Epoch 246, Avg Loss = 0.000987\n","[Val]   Epoch [246/600] done. Avg Loss: 0.000976\n","[Train] Epoch 247, Avg Loss = 0.000881\n","[Val]   Epoch [247/600] done. Avg Loss: 0.000873\n","[Train] Epoch 248, Avg Loss = 0.000883\n","[Val]   Epoch [248/600] done. Avg Loss: 0.000992\n","[Train] Epoch 249, Avg Loss = 0.000969\n","[Val]   Epoch [249/600] done. Avg Loss: 0.001047\n","[Train] Epoch 250, Avg Loss = 0.000953\n","[Val]   Epoch [250/600] done. Avg Loss: 0.000990\n","[Train] Epoch 251, Avg Loss = 0.000884\n","[Val]   Epoch [251/600] done. Avg Loss: 0.001199\n","[Train] Epoch 252, Avg Loss = 0.000958\n","[Val]   Epoch [252/600] done. Avg Loss: 0.001104\n","[Train] Epoch 253, Avg Loss = 0.000966\n","[Val]   Epoch [253/600] done. Avg Loss: 0.001028\n","[Train] Epoch 254, Avg Loss = 0.000897\n","[Val]   Epoch [254/600] done. Avg Loss: 0.001023\n","[Train] Epoch 255, Avg Loss = 0.001022\n","[Val]   Epoch [255/600] done. Avg Loss: 0.001324\n","[Train] Epoch 256, Avg Loss = 0.000913\n","[Val]   Epoch [256/600] done. Avg Loss: 0.000858\n","[Train] Epoch 257, Avg Loss = 0.000915\n","[Val]   Epoch [257/600] done. Avg Loss: 0.001091\n","[Train] Epoch 258, Avg Loss = 0.000892\n","[Val]   Epoch [258/600] done. Avg Loss: 0.000896\n","[Train] Epoch 259, Avg Loss = 0.000818\n","[Val]   Epoch [259/600] done. Avg Loss: 0.001068\n","[Train] Epoch 260, Avg Loss = 0.000938\n","[Val]   Epoch [260/600] done. Avg Loss: 0.001056\n","[Train] Epoch 261, Avg Loss = 0.000984\n","[Val]   Epoch [261/600] done. Avg Loss: 0.000968\n","[Train] Epoch 262, Avg Loss = 0.001011\n","[Val]   Epoch [262/600] done. Avg Loss: 0.001127\n","[Train] Epoch 263, Avg Loss = 0.000909\n","[Val]   Epoch [263/600] done. Avg Loss: 0.000983\n","[Train] Epoch 264, Avg Loss = 0.000893\n","[Val]   Epoch [264/600] done. Avg Loss: 0.000951\n","[Train] Epoch 265, Avg Loss = 0.000953\n","[Val]   Epoch [265/600] done. Avg Loss: 0.000866\n","[Train] Epoch 266, Avg Loss = 0.000976\n","[Val]   Epoch [266/600] done. Avg Loss: 0.001032\n","[Train] Epoch 267, Avg Loss = 0.000897\n","[Val]   Epoch [267/600] done. Avg Loss: 0.000870\n","[Train] Epoch 268, Avg Loss = 0.000795\n","[Val]   Epoch [268/600] done. Avg Loss: 0.000842\n","[Train] Epoch 269, Avg Loss = 0.000921\n","[Val]   Epoch [269/600] done. Avg Loss: 0.000960\n","[Train] Epoch 270, Avg Loss = 0.001041\n","[Val]   Epoch [270/600] done. Avg Loss: 0.001102\n","[Train] Epoch 271, Avg Loss = 0.000870\n","[Val]   Epoch [271/600] done. Avg Loss: 0.000948\n","[Train] Epoch 272, Avg Loss = 0.001014\n","[Val]   Epoch [272/600] done. Avg Loss: 0.000962\n","[Train] Epoch 273, Avg Loss = 0.000843\n","[Val]   Epoch [273/600] done. Avg Loss: 0.001065\n","[Train] Epoch 274, Avg Loss = 0.000825\n","[Val]   Epoch [274/600] done. Avg Loss: 0.000811\n","[Train] Epoch 275, Avg Loss = 0.000900\n","[Val]   Epoch [275/600] done. Avg Loss: 0.000947\n","[Train] Epoch 276, Avg Loss = 0.000957\n","[Val]   Epoch [276/600] done. Avg Loss: 0.000884\n","[Train] Epoch 277, Avg Loss = 0.000864\n","[Val]   Epoch [277/600] done. Avg Loss: 0.001112\n","[Train] Epoch 278, Avg Loss = 0.000954\n","[Val]   Epoch [278/600] done. Avg Loss: 0.000981\n","[Train] Epoch 279, Avg Loss = 0.000880\n","[Val]   Epoch [279/600] done. Avg Loss: 0.000924\n","[Train] Epoch 280, Avg Loss = 0.000897\n","[Val]   Epoch [280/600] done. Avg Loss: 0.001284\n","[Train] Epoch 281, Avg Loss = 0.000952\n","[Val]   Epoch [281/600] done. Avg Loss: 0.001209\n","[Train] Epoch 282, Avg Loss = 0.000900\n","[Val]   Epoch [282/600] done. Avg Loss: 0.001016\n","[Train] Epoch 283, Avg Loss = 0.000943\n","[Val]   Epoch [283/600] done. Avg Loss: 0.001117\n","[Train] Epoch 284, Avg Loss = 0.000911\n","[Val]   Epoch [284/600] done. Avg Loss: 0.000837\n","[Train] Epoch 285, Avg Loss = 0.000804\n","[Val]   Epoch [285/600] done. Avg Loss: 0.000818\n","[Train] Epoch 286, Avg Loss = 0.000862\n","[Val]   Epoch [286/600] done. Avg Loss: 0.001147\n","[Train] Epoch 287, Avg Loss = 0.000953\n","[Val]   Epoch [287/600] done. Avg Loss: 0.001001\n","[Train] Epoch 288, Avg Loss = 0.000878\n","[Val]   Epoch [288/600] done. Avg Loss: 0.000893\n","[Train] Epoch 289, Avg Loss = 0.000855\n","[Val]   Epoch [289/600] done. Avg Loss: 0.001107\n","[Train] Epoch 290, Avg Loss = 0.000985\n","[Val]   Epoch [290/600] done. Avg Loss: 0.001087\n","[Train] Epoch 291, Avg Loss = 0.000813\n","[Val]   Epoch [291/600] done. Avg Loss: 0.000927\n","[Train] Epoch 292, Avg Loss = 0.000830\n","[Val]   Epoch [292/600] done. Avg Loss: 0.000862\n","[Train] Epoch 293, Avg Loss = 0.000939\n","[Val]   Epoch [293/600] done. Avg Loss: 0.001003\n","[Train] Epoch 294, Avg Loss = 0.000900\n","[Val]   Epoch [294/600] done. Avg Loss: 0.000958\n","[Train] Epoch 295, Avg Loss = 0.000856\n","[Val]   Epoch [295/600] done. Avg Loss: 0.000938\n","[Train] Epoch 296, Avg Loss = 0.000905\n","[Val]   Epoch [296/600] done. Avg Loss: 0.000982\n","[Train] Epoch 297, Avg Loss = 0.000801\n","[Val]   Epoch [297/600] done. Avg Loss: 0.000889\n","[Train] Epoch 298, Avg Loss = 0.000837\n","[Val]   Epoch [298/600] done. Avg Loss: 0.000822\n","[Train] Epoch 299, Avg Loss = 0.000838\n","[Val]   Epoch [299/600] done. Avg Loss: 0.001180\n","[Train] Epoch 300, Avg Loss = 0.000874\n","[Val]   Epoch [300/600] done. Avg Loss: 0.001237\n","[Train] Epoch 301, Avg Loss = 0.000942\n","[Val]   Epoch [301/600] done. Avg Loss: 0.000926\n","[Train] Epoch 302, Avg Loss = 0.000850\n","[Val]   Epoch [302/600] done. Avg Loss: 0.000891\n","[Train] Epoch 303, Avg Loss = 0.000814\n","[Val]   Epoch [303/600] done. Avg Loss: 0.001085\n","[Train] Epoch 304, Avg Loss = 0.000899\n","[Val]   Epoch [304/600] done. Avg Loss: 0.001021\n","[Train] Epoch 305, Avg Loss = 0.001194\n","[Val]   Epoch [305/600] done. Avg Loss: 0.001013\n","[Train] Epoch 306, Avg Loss = 0.000886\n","[Val]   Epoch [306/600] done. Avg Loss: 0.001056\n","[Train] Epoch 307, Avg Loss = 0.000806\n","[Val]   Epoch [307/600] done. Avg Loss: 0.001154\n","[Train] Epoch 308, Avg Loss = 0.000833\n","[Val]   Epoch [308/600] done. Avg Loss: 0.000916\n","[Train] Epoch 309, Avg Loss = 0.000831\n","[Val]   Epoch [309/600] done. Avg Loss: 0.000860\n","[Train] Epoch 310, Avg Loss = 0.000826\n","[Val]   Epoch [310/600] done. Avg Loss: 0.000850\n","[Train] Epoch 311, Avg Loss = 0.000806\n","[Val]   Epoch [311/600] done. Avg Loss: 0.000868\n","[Train] Epoch 312, Avg Loss = 0.000800\n","[Val]   Epoch [312/600] done. Avg Loss: 0.000993\n","[Train] Epoch 313, Avg Loss = 0.000885\n","[Val]   Epoch [313/600] done. Avg Loss: 0.000858\n","[Train] Epoch 314, Avg Loss = 0.001080\n","[Val]   Epoch [314/600] done. Avg Loss: 0.000983\n","[Train] Epoch 315, Avg Loss = 0.000947\n","[Val]   Epoch [315/600] done. Avg Loss: 0.001542\n","[Train] Epoch 316, Avg Loss = 0.000825\n","[Val]   Epoch [316/600] done. Avg Loss: 0.000839\n","[Train] Epoch 317, Avg Loss = 0.000775\n","[Val]   Epoch [317/600] done. Avg Loss: 0.000857\n","[Train] Epoch 318, Avg Loss = 0.000881\n","[Val]   Epoch [318/600] done. Avg Loss: 0.000947\n","[Train] Epoch 319, Avg Loss = 0.000801\n","[Val]   Epoch [319/600] done. Avg Loss: 0.000850\n","[Train] Epoch 320, Avg Loss = 0.000774\n","[Val]   Epoch [320/600] done. Avg Loss: 0.000760\n","[Train] Epoch 321, Avg Loss = 0.000888\n","[Val]   Epoch [321/600] done. Avg Loss: 0.001249\n","[Train] Epoch 322, Avg Loss = 0.000912\n","[Val]   Epoch [322/600] done. Avg Loss: 0.001131\n","[Train] Epoch 323, Avg Loss = 0.001043\n","[Val]   Epoch [323/600] done. Avg Loss: 0.001035\n","[Train] Epoch 324, Avg Loss = 0.000763\n","[Val]   Epoch [324/600] done. Avg Loss: 0.000832\n","[Train] Epoch 325, Avg Loss = 0.000740\n","[Val]   Epoch [325/600] done. Avg Loss: 0.000848\n","[Train] Epoch 326, Avg Loss = 0.000814\n","[Val]   Epoch [326/600] done. Avg Loss: 0.000820\n","[Train] Epoch 327, Avg Loss = 0.000844\n","[Val]   Epoch [327/600] done. Avg Loss: 0.000872\n","[Train] Epoch 328, Avg Loss = 0.000909\n","[Val]   Epoch [328/600] done. Avg Loss: 0.000917\n","[Train] Epoch 329, Avg Loss = 0.000813\n","[Val]   Epoch [329/600] done. Avg Loss: 0.000868\n","[Train] Epoch 330, Avg Loss = 0.000821\n","[Val]   Epoch [330/600] done. Avg Loss: 0.000994\n","[Train] Epoch 331, Avg Loss = 0.000773\n","[Val]   Epoch [331/600] done. Avg Loss: 0.000841\n","[Train] Epoch 332, Avg Loss = 0.000800\n","[Val]   Epoch [332/600] done. Avg Loss: 0.000995\n","[Train] Epoch 333, Avg Loss = 0.000895\n","[Val]   Epoch [333/600] done. Avg Loss: 0.000961\n","[Train] Epoch 334, Avg Loss = 0.000788\n","[Val]   Epoch [334/600] done. Avg Loss: 0.000837\n","[Train] Epoch 335, Avg Loss = 0.000953\n","[Val]   Epoch [335/600] done. Avg Loss: 0.001775\n","[Train] Epoch 336, Avg Loss = 0.000981\n","[Val]   Epoch [336/600] done. Avg Loss: 0.000898\n","[Train] Epoch 337, Avg Loss = 0.000827\n","[Val]   Epoch [337/600] done. Avg Loss: 0.000994\n","[Train] Epoch 338, Avg Loss = 0.000810\n","[Val]   Epoch [338/600] done. Avg Loss: 0.000764\n","[Train] Epoch 339, Avg Loss = 0.000812\n","[Val]   Epoch [339/600] done. Avg Loss: 0.000775\n","[Train] Epoch 340, Avg Loss = 0.000801\n","[Val]   Epoch [340/600] done. Avg Loss: 0.000877\n","[Train] Epoch 341, Avg Loss = 0.000856\n","[Val]   Epoch [341/600] done. Avg Loss: 0.000875\n","[Train] Epoch 342, Avg Loss = 0.000804\n","[Val]   Epoch [342/600] done. Avg Loss: 0.000769\n","[Train] Epoch 343, Avg Loss = 0.000802\n","[Val]   Epoch [343/600] done. Avg Loss: 0.000871\n","[Train] Epoch 344, Avg Loss = 0.000830\n","[Val]   Epoch [344/600] done. Avg Loss: 0.001048\n","[Train] Epoch 345, Avg Loss = 0.000890\n","[Val]   Epoch [345/600] done. Avg Loss: 0.000901\n","[Train] Epoch 346, Avg Loss = 0.000728\n","[Val]   Epoch [346/600] done. Avg Loss: 0.000846\n","[Train] Epoch 347, Avg Loss = 0.000802\n","[Val]   Epoch [347/600] done. Avg Loss: 0.001126\n","[Train] Epoch 348, Avg Loss = 0.000817\n","[Val]   Epoch [348/600] done. Avg Loss: 0.000846\n","[Train] Epoch 349, Avg Loss = 0.000961\n","[Val]   Epoch [349/600] done. Avg Loss: 0.001246\n","[Train] Epoch 350, Avg Loss = 0.000996\n","[Val]   Epoch [350/600] done. Avg Loss: 0.000900\n","[Train] Epoch 351, Avg Loss = 0.000877\n","[Val]   Epoch [351/600] done. Avg Loss: 0.000760\n","[Train] Epoch 352, Avg Loss = 0.000752\n","[Val]   Epoch [352/600] done. Avg Loss: 0.000802\n","[Train] Epoch 353, Avg Loss = 0.000872\n","[Val]   Epoch [353/600] done. Avg Loss: 0.001008\n","[Train] Epoch 354, Avg Loss = 0.000776\n","[Val]   Epoch [354/600] done. Avg Loss: 0.000827\n","[Train] Epoch 355, Avg Loss = 0.000722\n","[Val]   Epoch [355/600] done. Avg Loss: 0.000911\n","[Train] Epoch 356, Avg Loss = 0.000799\n","[Val]   Epoch [356/600] done. Avg Loss: 0.000955\n","[Train] Epoch 357, Avg Loss = 0.000764\n","[Val]   Epoch [357/600] done. Avg Loss: 0.000874\n","[Train] Epoch 358, Avg Loss = 0.000868\n","[Val]   Epoch [358/600] done. Avg Loss: 0.000853\n","[Train] Epoch 359, Avg Loss = 0.000747\n","[Val]   Epoch [359/600] done. Avg Loss: 0.000926\n","[Train] Epoch 360, Avg Loss = 0.000893\n","[Val]   Epoch [360/600] done. Avg Loss: 0.000942\n","[Train] Epoch 361, Avg Loss = 0.000797\n","[Val]   Epoch [361/600] done. Avg Loss: 0.000906\n","[Train] Epoch 362, Avg Loss = 0.000745\n","[Val]   Epoch [362/600] done. Avg Loss: 0.000866\n","[Train] Epoch 363, Avg Loss = 0.000825\n","[Val]   Epoch [363/600] done. Avg Loss: 0.000874\n","[Train] Epoch 364, Avg Loss = 0.000834\n","[Val]   Epoch [364/600] done. Avg Loss: 0.000858\n","[Train] Epoch 365, Avg Loss = 0.000924\n","[Val]   Epoch [365/600] done. Avg Loss: 0.000939\n","[Train] Epoch 366, Avg Loss = 0.000724\n","[Val]   Epoch [366/600] done. Avg Loss: 0.000766\n","[Train] Epoch 367, Avg Loss = 0.000740\n","[Val]   Epoch [367/600] done. Avg Loss: 0.000838\n","[Train] Epoch 368, Avg Loss = 0.000756\n","[Val]   Epoch [368/600] done. Avg Loss: 0.000758\n","[Train] Epoch 369, Avg Loss = 0.000915\n","[Val]   Epoch [369/600] done. Avg Loss: 0.000849\n","[Train] Epoch 370, Avg Loss = 0.000754\n","[Val]   Epoch [370/600] done. Avg Loss: 0.000791\n","[Train] Epoch 371, Avg Loss = 0.000880\n","[Val]   Epoch [371/600] done. Avg Loss: 0.000883\n","[Train] Epoch 372, Avg Loss = 0.000799\n","[Val]   Epoch [372/600] done. Avg Loss: 0.000795\n","[Train] Epoch 373, Avg Loss = 0.000721\n","[Val]   Epoch [373/600] done. Avg Loss: 0.000824\n","[Train] Epoch 374, Avg Loss = 0.000811\n","[Val]   Epoch [374/600] done. Avg Loss: 0.000817\n","[Train] Epoch 375, Avg Loss = 0.000903\n","[Val]   Epoch [375/600] done. Avg Loss: 0.000788\n","[Train] Epoch 376, Avg Loss = 0.000724\n","[Val]   Epoch [376/600] done. Avg Loss: 0.000736\n","[Train] Epoch 377, Avg Loss = 0.000768\n","[Val]   Epoch [377/600] done. Avg Loss: 0.000848\n","[Train] Epoch 378, Avg Loss = 0.000835\n","[Val]   Epoch [378/600] done. Avg Loss: 0.000944\n","[Train] Epoch 379, Avg Loss = 0.000868\n","[Val]   Epoch [379/600] done. Avg Loss: 0.001009\n","[Train] Epoch 380, Avg Loss = 0.000747\n","[Val]   Epoch [380/600] done. Avg Loss: 0.000706\n","[Train] Epoch 381, Avg Loss = 0.000740\n","[Val]   Epoch [381/600] done. Avg Loss: 0.000840\n","[Train] Epoch 382, Avg Loss = 0.000757\n","[Val]   Epoch [382/600] done. Avg Loss: 0.000787\n","[Train] Epoch 383, Avg Loss = 0.000812\n","[Val]   Epoch [383/600] done. Avg Loss: 0.000935\n","[Train] Epoch 384, Avg Loss = 0.000789\n","[Val]   Epoch [384/600] done. Avg Loss: 0.000786\n","[Train] Epoch 385, Avg Loss = 0.000914\n","[Val]   Epoch [385/600] done. Avg Loss: 0.000988\n","[Train] Epoch 386, Avg Loss = 0.000817\n","[Val]   Epoch [386/600] done. Avg Loss: 0.000795\n","[Train] Epoch 387, Avg Loss = 0.000742\n","[Val]   Epoch [387/600] done. Avg Loss: 0.000834\n","[Train] Epoch 388, Avg Loss = 0.000827\n","[Val]   Epoch [388/600] done. Avg Loss: 0.001239\n","[Train] Epoch 389, Avg Loss = 0.000779\n","[Val]   Epoch [389/600] done. Avg Loss: 0.000739\n","[Train] Epoch 390, Avg Loss = 0.000717\n","[Val]   Epoch [390/600] done. Avg Loss: 0.001091\n","[Train] Epoch 391, Avg Loss = 0.000806\n","[Val]   Epoch [391/600] done. Avg Loss: 0.000773\n","[Train] Epoch 392, Avg Loss = 0.000741\n","[Val]   Epoch [392/600] done. Avg Loss: 0.001031\n","[Train] Epoch 393, Avg Loss = 0.000828\n","[Val]   Epoch [393/600] done. Avg Loss: 0.000868\n","[Train] Epoch 394, Avg Loss = 0.000724\n","[Val]   Epoch [394/600] done. Avg Loss: 0.000897\n","[Train] Epoch 395, Avg Loss = 0.000837\n","[Val]   Epoch [395/600] done. Avg Loss: 0.000802\n","[Train] Epoch 396, Avg Loss = 0.000779\n","[Val]   Epoch [396/600] done. Avg Loss: 0.000926\n","[Train] Epoch 397, Avg Loss = 0.000874\n","[Val]   Epoch [397/600] done. Avg Loss: 0.000822\n","[Train] Epoch 398, Avg Loss = 0.000755\n","[Val]   Epoch [398/600] done. Avg Loss: 0.000856\n","[Train] Epoch 399, Avg Loss = 0.000834\n","[Val]   Epoch [399/600] done. Avg Loss: 0.000890\n","[Train] Epoch 400, Avg Loss = 0.000770\n","[Val]   Epoch [400/600] done. Avg Loss: 0.000798\n","[Train] Epoch 401, Avg Loss = 0.000717\n","[Val]   Epoch [401/600] done. Avg Loss: 0.000941\n","[Train] Epoch 402, Avg Loss = 0.000737\n","[Val]   Epoch [402/600] done. Avg Loss: 0.000744\n","[Train] Epoch 403, Avg Loss = 0.000767\n","[Val]   Epoch [403/600] done. Avg Loss: 0.000860\n","[Train] Epoch 404, Avg Loss = 0.000816\n","[Val]   Epoch [404/600] done. Avg Loss: 0.001021\n","[Train] Epoch 405, Avg Loss = 0.000789\n","[Val]   Epoch [405/600] done. Avg Loss: 0.000996\n","[Train] Epoch 406, Avg Loss = 0.000796\n","[Val]   Epoch [406/600] done. Avg Loss: 0.000819\n","[Train] Epoch 407, Avg Loss = 0.000890\n","[Val]   Epoch [407/600] done. Avg Loss: 0.001124\n","[Train] Epoch 408, Avg Loss = 0.000753\n","[Val]   Epoch [408/600] done. Avg Loss: 0.000710\n","[Train] Epoch 409, Avg Loss = 0.000795\n","[Val]   Epoch [409/600] done. Avg Loss: 0.000860\n","[Train] Epoch 410, Avg Loss = 0.000710\n","[Val]   Epoch [410/600] done. Avg Loss: 0.000801\n","[Train] Epoch 411, Avg Loss = 0.000832\n","[Val]   Epoch [411/600] done. Avg Loss: 0.000849\n","[Train] Epoch 412, Avg Loss = 0.000712\n","[Val]   Epoch [412/600] done. Avg Loss: 0.000741\n","[Train] Epoch 413, Avg Loss = 0.000739\n","[Val]   Epoch [413/600] done. Avg Loss: 0.000778\n","[Train] Epoch 414, Avg Loss = 0.000763\n","[Val]   Epoch [414/600] done. Avg Loss: 0.001380\n","[Train] Epoch 415, Avg Loss = 0.000887\n","[Val]   Epoch [415/600] done. Avg Loss: 0.000747\n","[Train] Epoch 416, Avg Loss = 0.000700\n","[Val]   Epoch [416/600] done. Avg Loss: 0.000758\n","[Train] Epoch 417, Avg Loss = 0.000707\n","[Val]   Epoch [417/600] done. Avg Loss: 0.000800\n","[Train] Epoch 418, Avg Loss = 0.000909\n","[Val]   Epoch [418/600] done. Avg Loss: 0.000848\n","[Train] Epoch 419, Avg Loss = 0.000695\n","[Val]   Epoch [419/600] done. Avg Loss: 0.000739\n","[Train] Epoch 420, Avg Loss = 0.000769\n","[Val]   Epoch [420/600] done. Avg Loss: 0.000824\n","[Train] Epoch 421, Avg Loss = 0.000719\n","[Val]   Epoch [421/600] done. Avg Loss: 0.000729\n","[Train] Epoch 422, Avg Loss = 0.000707\n","[Val]   Epoch [422/600] done. Avg Loss: 0.000858\n","[Train] Epoch 423, Avg Loss = 0.000769\n","[Val]   Epoch [423/600] done. Avg Loss: 0.000844\n","[Train] Epoch 424, Avg Loss = 0.000760\n","[Val]   Epoch [424/600] done. Avg Loss: 0.000864\n","[Train] Epoch 425, Avg Loss = 0.000759\n","[Val]   Epoch [425/600] done. Avg Loss: 0.000773\n","[Train] Epoch 426, Avg Loss = 0.000745\n","[Val]   Epoch [426/600] done. Avg Loss: 0.000965\n","[Train] Epoch 427, Avg Loss = 0.000815\n","[Val]   Epoch [427/600] done. Avg Loss: 0.001004\n","[Train] Epoch 428, Avg Loss = 0.000794\n","[Val]   Epoch [428/600] done. Avg Loss: 0.001058\n","[Train] Epoch 429, Avg Loss = 0.000785\n","[Val]   Epoch [429/600] done. Avg Loss: 0.001115\n","[Train] Epoch 430, Avg Loss = 0.000906\n","[Val]   Epoch [430/600] done. Avg Loss: 0.000792\n","[Train] Epoch 431, Avg Loss = 0.000702\n","[Val]   Epoch [431/600] done. Avg Loss: 0.000698\n","[Train] Epoch 432, Avg Loss = 0.000857\n","[Val]   Epoch [432/600] done. Avg Loss: 0.000847\n","[Train] Epoch 433, Avg Loss = 0.000712\n","[Val]   Epoch [433/600] done. Avg Loss: 0.000738\n","[Train] Epoch 434, Avg Loss = 0.000781\n","[Val]   Epoch [434/600] done. Avg Loss: 0.000888\n","[Train] Epoch 435, Avg Loss = 0.000745\n","[Val]   Epoch [435/600] done. Avg Loss: 0.000812\n","[Train] Epoch 436, Avg Loss = 0.000840\n","[Val]   Epoch [436/600] done. Avg Loss: 0.000774\n","[Train] Epoch 437, Avg Loss = 0.000711\n","[Val]   Epoch [437/600] done. Avg Loss: 0.000752\n","[Train] Epoch 438, Avg Loss = 0.000770\n","[Val]   Epoch [438/600] done. Avg Loss: 0.001225\n","[Train] Epoch 439, Avg Loss = 0.000848\n","[Val]   Epoch [439/600] done. Avg Loss: 0.000791\n","[Train] Epoch 440, Avg Loss = 0.000665\n","[Val]   Epoch [440/600] done. Avg Loss: 0.000702\n","[Train] Epoch 441, Avg Loss = 0.000684\n","[Val]   Epoch [441/600] done. Avg Loss: 0.000759\n","[Train] Epoch 442, Avg Loss = 0.000721\n","[Val]   Epoch [442/600] done. Avg Loss: 0.001019\n","[Train] Epoch 443, Avg Loss = 0.000766\n","[Val]   Epoch [443/600] done. Avg Loss: 0.000884\n","[Train] Epoch 444, Avg Loss = 0.000798\n","[Val]   Epoch [444/600] done. Avg Loss: 0.000708\n","[Train] Epoch 445, Avg Loss = 0.000740\n","[Val]   Epoch [445/600] done. Avg Loss: 0.000741\n","[Train] Epoch 446, Avg Loss = 0.000726\n","[Val]   Epoch [446/600] done. Avg Loss: 0.000975\n","[Train] Epoch 447, Avg Loss = 0.000823\n","[Val]   Epoch [447/600] done. Avg Loss: 0.000769\n","[Train] Epoch 448, Avg Loss = 0.000727\n","[Val]   Epoch [448/600] done. Avg Loss: 0.001031\n","[Train] Epoch 449, Avg Loss = 0.000785\n","[Val]   Epoch [449/600] done. Avg Loss: 0.000824\n","[Train] Epoch 450, Avg Loss = 0.000808\n","[Val]   Epoch [450/600] done. Avg Loss: 0.001110\n","[Train] Epoch 451, Avg Loss = 0.000831\n","[Val]   Epoch [451/600] done. Avg Loss: 0.000779\n","[Train] Epoch 452, Avg Loss = 0.000699\n","[Val]   Epoch [452/600] done. Avg Loss: 0.000773\n","[Train] Epoch 453, Avg Loss = 0.000775\n","[Val]   Epoch [453/600] done. Avg Loss: 0.000779\n","[Train] Epoch 454, Avg Loss = 0.000680\n","[Val]   Epoch [454/600] done. Avg Loss: 0.000768\n","[Train] Epoch 455, Avg Loss = 0.000758\n","[Val]   Epoch [455/600] done. Avg Loss: 0.000732\n","[Train] Epoch 456, Avg Loss = 0.000825\n","[Val]   Epoch [456/600] done. Avg Loss: 0.000866\n","[Train] Epoch 457, Avg Loss = 0.000696\n","[Val]   Epoch [457/600] done. Avg Loss: 0.000765\n","[Train] Epoch 458, Avg Loss = 0.000663\n","[Val]   Epoch [458/600] done. Avg Loss: 0.000716\n","[Train] Epoch 459, Avg Loss = 0.000867\n","[Val]   Epoch [459/600] done. Avg Loss: 0.000762\n","[Train] Epoch 460, Avg Loss = 0.000711\n","[Val]   Epoch [460/600] done. Avg Loss: 0.000754\n","[Train] Epoch 461, Avg Loss = 0.000737\n","[Val]   Epoch [461/600] done. Avg Loss: 0.000875\n","[Train] Epoch 462, Avg Loss = 0.000678\n","[Val]   Epoch [462/600] done. Avg Loss: 0.000767\n","[Train] Epoch 463, Avg Loss = 0.000700\n","[Val]   Epoch [463/600] done. Avg Loss: 0.000763\n","[Train] Epoch 464, Avg Loss = 0.000871\n","[Val]   Epoch [464/600] done. Avg Loss: 0.000937\n","[Train] Epoch 465, Avg Loss = 0.000794\n","[Val]   Epoch [465/600] done. Avg Loss: 0.000834\n","[Train] Epoch 466, Avg Loss = 0.000702\n","[Val]   Epoch [466/600] done. Avg Loss: 0.000858\n","[Train] Epoch 467, Avg Loss = 0.000831\n","[Val]   Epoch [467/600] done. Avg Loss: 0.000923\n","[Train] Epoch 468, Avg Loss = 0.000763\n","[Val]   Epoch [468/600] done. Avg Loss: 0.000898\n","[Train] Epoch 469, Avg Loss = 0.000734\n","[Val]   Epoch [469/600] done. Avg Loss: 0.000795\n","[Train] Epoch 470, Avg Loss = 0.000644\n","[Val]   Epoch [470/600] done. Avg Loss: 0.000794\n","[Train] Epoch 471, Avg Loss = 0.000746\n","[Val]   Epoch [471/600] done. Avg Loss: 0.000897\n","[Train] Epoch 472, Avg Loss = 0.000860\n","[Val]   Epoch [472/600] done. Avg Loss: 0.000718\n","[Train] Epoch 473, Avg Loss = 0.000695\n","[Val]   Epoch [473/600] done. Avg Loss: 0.000908\n","[Train] Epoch 474, Avg Loss = 0.000709\n","[Val]   Epoch [474/600] done. Avg Loss: 0.000706\n","[Train] Epoch 475, Avg Loss = 0.000689\n","[Val]   Epoch [475/600] done. Avg Loss: 0.000844\n","[Train] Epoch 476, Avg Loss = 0.000704\n","[Val]   Epoch [476/600] done. Avg Loss: 0.001006\n","[Train] Epoch 477, Avg Loss = 0.000796\n","[Val]   Epoch [477/600] done. Avg Loss: 0.000740\n","[Train] Epoch 478, Avg Loss = 0.000867\n","[Val]   Epoch [478/600] done. Avg Loss: 0.000897\n","[Train] Epoch 479, Avg Loss = 0.000688\n","[Val]   Epoch [479/600] done. Avg Loss: 0.000767\n","[Train] Epoch 480, Avg Loss = 0.000687\n","[Val]   Epoch [480/600] done. Avg Loss: 0.000730\n","[Train] Epoch 481, Avg Loss = 0.000731\n","[Val]   Epoch [481/600] done. Avg Loss: 0.000838\n","[Train] Epoch 482, Avg Loss = 0.000668\n","[Val]   Epoch [482/600] done. Avg Loss: 0.000695\n","[Train] Epoch 483, Avg Loss = 0.000696\n","[Val]   Epoch [483/600] done. Avg Loss: 0.000993\n","[Train] Epoch 484, Avg Loss = 0.000739\n","[Val]   Epoch [484/600] done. Avg Loss: 0.001343\n","[Train] Epoch 485, Avg Loss = 0.000788\n","[Val]   Epoch [485/600] done. Avg Loss: 0.000842\n","[Train] Epoch 486, Avg Loss = 0.000684\n","[Val]   Epoch [486/600] done. Avg Loss: 0.001080\n","[Train] Epoch 487, Avg Loss = 0.000770\n","[Val]   Epoch [487/600] done. Avg Loss: 0.000795\n","[Train] Epoch 488, Avg Loss = 0.000695\n","[Val]   Epoch [488/600] done. Avg Loss: 0.000764\n","[Train] Epoch 489, Avg Loss = 0.000699\n","[Val]   Epoch [489/600] done. Avg Loss: 0.000779\n","[Train] Epoch 490, Avg Loss = 0.000691\n","[Val]   Epoch [490/600] done. Avg Loss: 0.000706\n","[Train] Epoch 491, Avg Loss = 0.000695\n","[Val]   Epoch [491/600] done. Avg Loss: 0.000861\n","[Train] Epoch 492, Avg Loss = 0.000777\n","[Val]   Epoch [492/600] done. Avg Loss: 0.000946\n","[Train] Epoch 493, Avg Loss = 0.000769\n","[Val]   Epoch [493/600] done. Avg Loss: 0.000969\n","[Train] Epoch 494, Avg Loss = 0.000741\n","[Val]   Epoch [494/600] done. Avg Loss: 0.000677\n","[Train] Epoch 495, Avg Loss = 0.000673\n","[Val]   Epoch [495/600] done. Avg Loss: 0.000737\n","[Train] Epoch 496, Avg Loss = 0.000730\n","[Val]   Epoch [496/600] done. Avg Loss: 0.000708\n","[Train] Epoch 497, Avg Loss = 0.000745\n","[Val]   Epoch [497/600] done. Avg Loss: 0.000772\n","[Train] Epoch 498, Avg Loss = 0.000651\n","[Val]   Epoch [498/600] done. Avg Loss: 0.000929\n","[Train] Epoch 499, Avg Loss = 0.000752\n","[Val]   Epoch [499/600] done. Avg Loss: 0.000954\n","[Train] Epoch 500, Avg Loss = 0.000651\n","[Val]   Epoch [500/600] done. Avg Loss: 0.000758\n","[Train] Epoch 501, Avg Loss = 0.000751\n","[Val]   Epoch [501/600] done. Avg Loss: 0.000779\n","[Train] Epoch 502, Avg Loss = 0.000895\n","[Val]   Epoch [502/600] done. Avg Loss: 0.000710\n","[Train] Epoch 503, Avg Loss = 0.000643\n","[Val]   Epoch [503/600] done. Avg Loss: 0.000659\n","[Train] Epoch 504, Avg Loss = 0.000656\n","[Val]   Epoch [504/600] done. Avg Loss: 0.000821\n","[Train] Epoch 505, Avg Loss = 0.000759\n","[Val]   Epoch [505/600] done. Avg Loss: 0.000770\n","[Train] Epoch 506, Avg Loss = 0.000649\n","[Val]   Epoch [506/600] done. Avg Loss: 0.000643\n","[Train] Epoch 507, Avg Loss = 0.000843\n","[Val]   Epoch [507/600] done. Avg Loss: 0.000781\n","[Train] Epoch 508, Avg Loss = 0.000701\n","[Val]   Epoch [508/600] done. Avg Loss: 0.000860\n","[Train] Epoch 509, Avg Loss = 0.000697\n","[Val]   Epoch [509/600] done. Avg Loss: 0.000720\n","[Train] Epoch 510, Avg Loss = 0.000689\n","[Val]   Epoch [510/600] done. Avg Loss: 0.000856\n","[Train] Epoch 511, Avg Loss = 0.000782\n","[Val]   Epoch [511/600] done. Avg Loss: 0.000725\n","[Train] Epoch 512, Avg Loss = 0.000803\n","[Val]   Epoch [512/600] done. Avg Loss: 0.000722\n","[Train] Epoch 513, Avg Loss = 0.000654\n","[Val]   Epoch [513/600] done. Avg Loss: 0.000726\n","[Train] Epoch 514, Avg Loss = 0.000677\n","[Val]   Epoch [514/600] done. Avg Loss: 0.001127\n","[Train] Epoch 515, Avg Loss = 0.000787\n","[Val]   Epoch [515/600] done. Avg Loss: 0.000810\n","[Train] Epoch 516, Avg Loss = 0.000656\n","[Val]   Epoch [516/600] done. Avg Loss: 0.000769\n","[Train] Epoch 517, Avg Loss = 0.000735\n","[Val]   Epoch [517/600] done. Avg Loss: 0.000735\n","[Train] Epoch 518, Avg Loss = 0.000664\n","[Val]   Epoch [518/600] done. Avg Loss: 0.000749\n","[Train] Epoch 519, Avg Loss = 0.000633\n","[Val]   Epoch [519/600] done. Avg Loss: 0.000838\n","[Train] Epoch 520, Avg Loss = 0.000664\n","[Val]   Epoch [520/600] done. Avg Loss: 0.000754\n","[Train] Epoch 521, Avg Loss = 0.000731\n","[Val]   Epoch [521/600] done. Avg Loss: 0.000992\n","[Train] Epoch 522, Avg Loss = 0.000688\n","[Val]   Epoch [522/600] done. Avg Loss: 0.000784\n","[Train] Epoch 523, Avg Loss = 0.000631\n","[Val]   Epoch [523/600] done. Avg Loss: 0.000751\n","[Train] Epoch 524, Avg Loss = 0.000754\n","[Val]   Epoch [524/600] done. Avg Loss: 0.000731\n","[Train] Epoch 525, Avg Loss = 0.000795\n","[Val]   Epoch [525/600] done. Avg Loss: 0.000900\n","[Train] Epoch 526, Avg Loss = 0.000692\n","[Val]   Epoch [526/600] done. Avg Loss: 0.000871\n","[Train] Epoch 527, Avg Loss = 0.000699\n","[Val]   Epoch [527/600] done. Avg Loss: 0.000983\n","[Train] Epoch 528, Avg Loss = 0.000735\n","[Val]   Epoch [528/600] done. Avg Loss: 0.000915\n","[Train] Epoch 529, Avg Loss = 0.000698\n","[Val]   Epoch [529/600] done. Avg Loss: 0.000741\n","[Train] Epoch 530, Avg Loss = 0.000660\n","[Val]   Epoch [530/600] done. Avg Loss: 0.000714\n","[Train] Epoch 531, Avg Loss = 0.000676\n","[Val]   Epoch [531/600] done. Avg Loss: 0.001004\n","[Train] Epoch 532, Avg Loss = 0.000749\n","[Val]   Epoch [532/600] done. Avg Loss: 0.000740\n","[Train] Epoch 533, Avg Loss = 0.000673\n","[Val]   Epoch [533/600] done. Avg Loss: 0.000761\n","[Train] Epoch 534, Avg Loss = 0.000666\n","[Val]   Epoch [534/600] done. Avg Loss: 0.000888\n","[Train] Epoch 535, Avg Loss = 0.000894\n","[Val]   Epoch [535/600] done. Avg Loss: 0.000884\n","[Train] Epoch 536, Avg Loss = 0.000724\n","[Val]   Epoch [536/600] done. Avg Loss: 0.000713\n","[Train] Epoch 537, Avg Loss = 0.000655\n","[Val]   Epoch [537/600] done. Avg Loss: 0.000770\n","[Train] Epoch 538, Avg Loss = 0.000703\n","[Val]   Epoch [538/600] done. Avg Loss: 0.000706\n","[Train] Epoch 539, Avg Loss = 0.000624\n","[Val]   Epoch [539/600] done. Avg Loss: 0.000898\n","[Train] Epoch 540, Avg Loss = 0.000702\n","[Val]   Epoch [540/600] done. Avg Loss: 0.000732\n","[Train] Epoch 541, Avg Loss = 0.000719\n","[Val]   Epoch [541/600] done. Avg Loss: 0.000820\n","[Train] Epoch 542, Avg Loss = 0.000706\n","[Val]   Epoch [542/600] done. Avg Loss: 0.000786\n","[Train] Epoch 543, Avg Loss = 0.000752\n","[Val]   Epoch [543/600] done. Avg Loss: 0.000640\n","[Train] Epoch 544, Avg Loss = 0.000612\n","[Val]   Epoch [544/600] done. Avg Loss: 0.000738\n","[Train] Epoch 545, Avg Loss = 0.000680\n","[Val]   Epoch [545/600] done. Avg Loss: 0.000769\n","[Train] Epoch 546, Avg Loss = 0.000669\n","[Val]   Epoch [546/600] done. Avg Loss: 0.000723\n","[Train] Epoch 547, Avg Loss = 0.000841\n","[Val]   Epoch [547/600] done. Avg Loss: 0.001054\n","[Train] Epoch 548, Avg Loss = 0.000686\n","[Val]   Epoch [548/600] done. Avg Loss: 0.000748\n","[Train] Epoch 549, Avg Loss = 0.000706\n","[Val]   Epoch [549/600] done. Avg Loss: 0.001030\n","[Train] Epoch 550, Avg Loss = 0.000636\n","[Val]   Epoch [550/600] done. Avg Loss: 0.000654\n","[Train] Epoch 551, Avg Loss = 0.000725\n","[Val]   Epoch [551/600] done. Avg Loss: 0.000785\n","[Train] Epoch 552, Avg Loss = 0.000694\n","[Val]   Epoch [552/600] done. Avg Loss: 0.000677\n","[Train] Epoch 553, Avg Loss = 0.000685\n","[Val]   Epoch [553/600] done. Avg Loss: 0.000867\n","[Train] Epoch 554, Avg Loss = 0.000721\n","[Val]   Epoch [554/600] done. Avg Loss: 0.000844\n","[Train] Epoch 555, Avg Loss = 0.000747\n","[Val]   Epoch [555/600] done. Avg Loss: 0.000787\n","[Train] Epoch 556, Avg Loss = 0.000615\n","[Val]   Epoch [556/600] done. Avg Loss: 0.000716\n","[Train] Epoch 557, Avg Loss = 0.000666\n","[Val]   Epoch [557/600] done. Avg Loss: 0.000731\n","[Train] Epoch 558, Avg Loss = 0.000644\n","[Val]   Epoch [558/600] done. Avg Loss: 0.000708\n","[Train] Epoch 559, Avg Loss = 0.000746\n","[Val]   Epoch [559/600] done. Avg Loss: 0.000811\n","[Train] Epoch 560, Avg Loss = 0.000671\n","[Val]   Epoch [560/600] done. Avg Loss: 0.000774\n","[Train] Epoch 561, Avg Loss = 0.000667\n","[Val]   Epoch [561/600] done. Avg Loss: 0.000754\n","[Train] Epoch 562, Avg Loss = 0.000753\n","[Val]   Epoch [562/600] done. Avg Loss: 0.000755\n","[Train] Epoch 563, Avg Loss = 0.000752\n","[Val]   Epoch [563/600] done. Avg Loss: 0.000875\n","[Train] Epoch 564, Avg Loss = 0.000679\n","[Val]   Epoch [564/600] done. Avg Loss: 0.000688\n","[Train] Epoch 565, Avg Loss = 0.000644\n","[Val]   Epoch [565/600] done. Avg Loss: 0.000677\n","[Train] Epoch 566, Avg Loss = 0.000644\n","[Val]   Epoch [566/600] done. Avg Loss: 0.000981\n","[Train] Epoch 567, Avg Loss = 0.000755\n","[Val]   Epoch [567/600] done. Avg Loss: 0.000690\n","[Train] Epoch 568, Avg Loss = 0.000707\n","[Val]   Epoch [568/600] done. Avg Loss: 0.000773\n","[Train] Epoch 569, Avg Loss = 0.000659\n","[Val]   Epoch [569/600] done. Avg Loss: 0.000916\n","[Train] Epoch 570, Avg Loss = 0.000674\n","[Val]   Epoch [570/600] done. Avg Loss: 0.000693\n","[Train] Epoch 571, Avg Loss = 0.000727\n","[Val]   Epoch [571/600] done. Avg Loss: 0.000711\n","[Train] Epoch 572, Avg Loss = 0.000683\n","[Val]   Epoch [572/600] done. Avg Loss: 0.000700\n","[Train] Epoch 573, Avg Loss = 0.000660\n","[Val]   Epoch [573/600] done. Avg Loss: 0.000713\n","[Train] Epoch 574, Avg Loss = 0.000691\n","[Val]   Epoch [574/600] done. Avg Loss: 0.000725\n","[Train] Epoch 575, Avg Loss = 0.000677\n","[Val]   Epoch [575/600] done. Avg Loss: 0.000716\n","[Train] Epoch 576, Avg Loss = 0.000735\n","[Val]   Epoch [576/600] done. Avg Loss: 0.000809\n","[Train] Epoch 577, Avg Loss = 0.000691\n","[Val]   Epoch [577/600] done. Avg Loss: 0.000692\n","[Train] Epoch 578, Avg Loss = 0.000626\n","[Val]   Epoch [578/600] done. Avg Loss: 0.000805\n","[Train] Epoch 579, Avg Loss = 0.000684\n","[Val]   Epoch [579/600] done. Avg Loss: 0.000729\n","[Train] Epoch 580, Avg Loss = 0.000658\n","[Val]   Epoch [580/600] done. Avg Loss: 0.000903\n","[Train] Epoch 581, Avg Loss = 0.000683\n","[Val]   Epoch [581/600] done. Avg Loss: 0.000762\n","[Train] Epoch 582, Avg Loss = 0.000815\n","[Val]   Epoch [582/600] done. Avg Loss: 0.000955\n","[Train] Epoch 583, Avg Loss = 0.000634\n","[Val]   Epoch [583/600] done. Avg Loss: 0.000711\n","[Train] Epoch 584, Avg Loss = 0.000627\n","[Val]   Epoch [584/600] done. Avg Loss: 0.000746\n","[Train] Epoch 585, Avg Loss = 0.000880\n","[Val]   Epoch [585/600] done. Avg Loss: 0.000813\n","[Train] Epoch 586, Avg Loss = 0.000608\n","[Val]   Epoch [586/600] done. Avg Loss: 0.000676\n","[Train] Epoch 587, Avg Loss = 0.000761\n","[Val]   Epoch [587/600] done. Avg Loss: 0.000811\n","[Train] Epoch 588, Avg Loss = 0.000626\n","[Val]   Epoch [588/600] done. Avg Loss: 0.000699\n","[Train] Epoch 589, Avg Loss = 0.000658\n","[Val]   Epoch [589/600] done. Avg Loss: 0.000640\n","[Train] Epoch 590, Avg Loss = 0.000609\n","[Val]   Epoch [590/600] done. Avg Loss: 0.000646\n","[Train] Epoch 591, Avg Loss = 0.000685\n","[Val]   Epoch [591/600] done. Avg Loss: 0.000799\n","[Train] Epoch 592, Avg Loss = 0.000705\n","[Val]   Epoch [592/600] done. Avg Loss: 0.000728\n","[Train] Epoch 593, Avg Loss = 0.000688\n","[Val]   Epoch [593/600] done. Avg Loss: 0.000782\n","[Train] Epoch 594, Avg Loss = 0.000653\n","[Val]   Epoch [594/600] done. Avg Loss: 0.000746\n","[Train] Epoch 595, Avg Loss = 0.000764\n","[Val]   Epoch [595/600] done. Avg Loss: 0.000919\n","[Train] Epoch 596, Avg Loss = 0.000718\n","[Val]   Epoch [596/600] done. Avg Loss: 0.000811\n","[Train] Epoch 597, Avg Loss = 0.000681\n","[Val]   Epoch [597/600] done. Avg Loss: 0.001097\n","[Train] Epoch 598, Avg Loss = 0.000707\n","[Val]   Epoch [598/600] done. Avg Loss: 0.000717\n","[Train] Epoch 599, Avg Loss = 0.000618\n","[Val]   Epoch [599/600] done. Avg Loss: 0.000620\n","[Train] Epoch 600, Avg Loss = 0.000645\n","[Val]   Epoch [600/600] done. Avg Loss: 0.000708\n"]}]},{"cell_type":"code","source":["# ────────────────────────────────────────────────────────────────────────────\n","# (C) InverseNet 정의 (MLP)\n","# ────────────────────────────────────────────────────────────────────────────\n","t_path = '/content/drive/MyDrive/Colab Notebooks/Bi-directional20250402'\n","os.makedirs(t_path, exist_ok=True)\n","num_layers = len(material_sequence)  # 3\n","class InverseNet(nn.Module):\n","    def __init__(self, input_dim, output_dim):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(input_dim, 1198),\n","            nn.ReLU(),\n","            nn.Linear(1198, 1125),\n","            nn.ReLU(),\n","            nn.Linear(1125, 853),\n","            nn.ReLU(),\n","            nn.Linear(853, 1411),\n","            nn.ReLU(),\n","            nn.Linear(1411, 1320),\n","            nn.ReLU(),\n","            nn.Linear(1320, 1958),\n","            nn.ReLU(),\n","            nn.Linear(1958, output_dim)\n","        )\n","\n","    def forward(self, x):\n","        # x must be shape (B, input_dim)\n","        return self.net(x)\n","\n","# Re-initialize the model and optimizer after changing the class definition\n","inverse_net = InverseNet(input_dim=NUM_WAVELENGTHS, output_dim=num_layers).to(device)\n","inverse_net = inverse_net.to(torch.float64)\n","optimizer = optim.Adam(inverse_net.parameters(), lr=0.0016277391324458217)\n","\n","# ────────────────────────────────────────────────────────────────────────────\n","# (D) 학습 루프: bi-TMM 방식\n","# ────────────────────────────────────────────────────────────────────────────\n","num_epochs = 600\n","save_interval = 600\n","output_dir = \"C:/Users/PC/Desktop/Deep/\"\n","os.makedirs(output_dir, exist_ok=True)\n","alpha = 1.0616785666328346\n","forward_net.eval()\n","for epoch in range(num_epochs):\n","    inverse_net.train()\n","    total_loss = 0.0\n","    for batch_idx, batch in enumerate(train_loader):\n","        T_target = batch['T_target'].to(device)      # [B, 401]\n","        d_norm_true = batch['d_norm'].to(device)     # [B, 3]\n","\n","        optimizer.zero_grad()\n","        d_norm_pred = inverse_net(T_target.to(torch.float64)).squeeze(1)\n","\n","        if batch_idx == 0:\n","            print(f\"\\n[DEBUG] Epoch {epoch+1}, Batch {batch_idx+1}\")\n","        d_nm_pred = destandardize_thickness(d_norm_pred)\n","        d_nm_true = destandardize_thickness(d_norm_true)\n","\n","        T_pred_batch = model(d_nm_pred)\n","        squeezed_T_target = T_target.squeeze(1)\n","        T_target_destd = destandardize_spectrum(squeezed_T_target)\n","\n","        loss_spectrum = F.mse_loss(T_pred_batch, T_target_destd)\n","        loss_thickness = F.mse_loss(d_norm_pred, d_norm_true.squeeze(1))\n","\n","        if epoch < 18:\n","            loss = loss_spectrum + alpha * loss_thickness\n","        else:\n","            loss = loss_spectrum\n","\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    avg_train_loss = total_loss / len(train_loader)\n","    print(f\"[Train] Epoch [{epoch+1}/{num_epochs}] done. Avg Loss: {avg_train_loss:.6f}\")\n","\n","    # ───────────────────────────────────────────\n","    # Validation\n","    # ───────────────────────────────────────────\n","    inverse_net.eval()\n","    val_loss_total = 0.0\n","    with torch.no_grad():\n","        for val_batch in val_loader:\n","            T_val = val_batch['T_target'].to(device)\n","            d_norm_val = val_batch['d_norm'].to(device)\n","\n","            d_pred_val = inverse_net(T_val.to(torch.float64)).squeeze(1)\n","            d_nm_val_pred = destandardize_thickness(d_pred_val)\n","            T_val_pred = model(d_nm_val_pred)\n","\n","            T_val_destd = destandardize_spectrum(T_val.squeeze(1))\n","\n","            loss_val_spectrum = F.mse_loss(T_val_pred, T_val_destd)\n","            loss_val_thickness = F.mse_loss(d_pred_val, d_norm_val.squeeze(1))\n","\n","            if epoch < 18:\n","                val_loss = loss_val_spectrum + alpha * loss_val_thickness\n","            else:\n","                val_loss = loss_val_spectrum\n","\n","            val_loss_total += val_loss.item()\n","\n","    avg_val_loss = val_loss_total / len(val_loader)\n","    print(f\"[Val]   Epoch [{epoch+1}/{num_epochs}] done. Avg Loss: {avg_val_loss:.6f}\")\n","# 저장할 경로와 파일명을 설정합니다.\n","#SAVE_PATH = os.path.join(t_path, \"inverse_net_final.pth\")  # ✅ t_path로 수정\n","#torch.save(inverse_net.state_dict(), SAVE_PATH)\n","#print(f\"\\nModel saved successfully to {SAVE_PATH}\")\n","# =========================================================="],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":245},"id":"9oRdmem4aZ1_","executionInfo":{"status":"error","timestamp":1752137503375,"user_tz":-540,"elapsed":7806,"user":{"displayName":"이스타의행운","userId":"08935579543132628543"}},"outputId":"c91d8b13-74bb-4d30-f298-25d4206b4034"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","[DEBUG] Epoch 1, Batch 1\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[12], line 73\u001b[0m\n\u001b[0;32m     71\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     72\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 73\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Train] Epoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] done. Avg Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_train_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["# **OPTUNA**"],"metadata":{"id":"lfh0wgyHpLKu"}},{"cell_type":"code","source":["# ────────────────────────────────────────────────────────────────────────────\n","# (C) InverseNet 정의 (MLP)\n","# ────────────────────────────────────────────────────────────────────────────\n","num_layers = len(material_sequence)  # 3\n","class InverseNet(nn.Module):\n","    def __init__(self, input_dim, output_dim):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(401, 1198),\n","            nn.ReLU(),\n","            nn.Linear(1198, 1125),\n","            nn.ReLU(),\n","            nn.Linear(1125, 853),\n","            nn.ReLU(),\n","            nn.Linear(853, 1411),\n","            nn.ReLU(),\n","            nn.Linear(1411, 1320),\n","            nn.ReLU(),\n","            nn.Linear(1320, 1958),\n","            nn.ReLU(),\n","            nn.Linear(1958, 3)\n","        )\n","\n","    def forward(self, x):\n","        # x must be shape (B, input_dim)\n","        return self.net(x)\n","\n","# Re-initialize the model and optimizer after changing the class definition\n","inverse_net = InverseNet(input_dim=NUM_WAVELENGTHS, output_dim=num_layers).to(device)\n","inverse_net = inverse_net.to(torch.float64)\n","optimizer = optim.Adam(inverse_net.parameters(), lr=0.00018272261776066238)\n","\n","# ────────────────────────────────────────────────────────────────────────────\n","# (D) 학습 루프: bi-TMM 방식\n","# ────────────────────────────────────────────────────────────────────────────\n","num_epochs = 600\n","save_interval = 600\n","output_dir = \"C:/Users/PC/Desktop/Deep/\"\n","os.makedirs(output_dir, exist_ok=True)\n","ALPHA = 1.0616785666328346\n","BETA = 18\n","for epoch in range(1, num_epochs + 1):\n","    # ---- train ----\n","    inverse_net.train()\n","    epoch_loss = 0.0\n","    for batch in train_loader:\n","        T_target = batch['T_target'].to(device)\n","        d_norm_true = batch['d_norm'].to(device)\n","\n","        optimizer.zero_grad()\n","        x = T_target.squeeze(1)\n","        d_norm_pred = inverse_net(x)\n","\n","        d_nm_pred = destandardize_thickness(d_norm_pred)\n","        T_pred_batch = model(d_nm_pred)  # ⬅️ 수정됨: no torch.no_grad()\n","        T_target_destd = destandardize_spectrum(x)\n","\n","        loss_spectrum = F.mse_loss(T_pred_batch, T_target_destd)\n","        loss_thickness = F.mse_loss(d_norm_pred, d_norm_true.squeeze(1))\n","        loss = loss_spectrum + ALPHA * loss_thickness if epoch <= BETA else loss_spectrum\n","\n","        loss.backward()\n","        optimizer.step()\n","        epoch_loss += loss.item()\n","\n","    # ---- validation ----\n","    inverse_net.eval()\n","    val_loss_total = 0.0\n","    with torch.no_grad():\n","        for val_batch in val_loader:\n","            T_val = val_batch['T_target'].to(device)\n","            d_norm_val = val_batch['d_norm'].to(device)\n","\n","            x_val = T_val.squeeze(1).to(torch.float64)\n","            d_pred_val = inverse_net(x_val)\n","            d_nm_val_pred = destandardize_thickness(d_pred_val)\n","            T_val_pred = model(d_nm_val_pred)\n","            T_val_destd = destandardize_spectrum(x_val)\n","\n","            loss_spectrum_v = F.mse_loss(T_val_pred, T_val_destd)\n","            loss_thickness_v = F.mse_loss(d_pred_val, d_norm_val.squeeze(1))\n","            val_loss = (loss_spectrum_v + ALPHA * loss_thickness_v) if epoch <= BETA else loss_spectrum_v\n","            val_loss_total += val_loss.item()\n","\n","    avg_train_loss = epoch_loss / len(train_loader)\n","    avg_val_loss = val_loss_total / len(val_loader)\n","    if epoch % 1 == 0 or epoch == 1:\n","        print(f\"Epoch[{epoch}/{num_epochs}]  train: {avg_train_loss:.6f}  val: {avg_val_loss:.6f}\")"],"metadata":{"id":"tFfplCnTlUJi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1752146778426,"user_tz":-540,"elapsed":7542770,"user":{"displayName":"이스타의행운","userId":"08935579543132628543"}},"outputId":"1bd95b50-6ebe-4df5-b95f-ae75bf54ac2a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch[1/600]  train: 0.135368  val: 0.118797\n","Epoch[2/600]  train: 0.100794  val: 0.097688\n","Epoch[3/600]  train: 0.091826  val: 0.085317\n","Epoch[4/600]  train: 0.081808  val: 0.077263\n","Epoch[5/600]  train: 0.073841  val: 0.072828\n","Epoch[6/600]  train: 0.066115  val: 0.061843\n","Epoch[7/600]  train: 0.059717  val: 0.054563\n","Epoch[8/600]  train: 0.057066  val: 0.058477\n","Epoch[9/600]  train: 0.054412  val: 0.053253\n","Epoch[10/600]  train: 0.050232  val: 0.047905\n","Epoch[11/600]  train: 0.049475  val: 0.046976\n","Epoch[12/600]  train: 0.047790  val: 0.040881\n","Epoch[13/600]  train: 0.046604  val: 0.042200\n","Epoch[14/600]  train: 0.045473  val: 0.043145\n","Epoch[15/600]  train: 0.041094  val: 0.033753\n","Epoch[16/600]  train: 0.040722  val: 0.035785\n","Epoch[17/600]  train: 0.037986  val: 0.035878\n","Epoch[18/600]  train: 0.034197  val: 0.044687\n","Epoch[19/600]  train: 0.008417  val: 0.004775\n","Epoch[20/600]  train: 0.005179  val: 0.004686\n","Epoch[21/600]  train: 0.004736  val: 0.003428\n","Epoch[22/600]  train: 0.004353  val: 0.003186\n","Epoch[23/600]  train: 0.004221  val: 0.002629\n","Epoch[24/600]  train: 0.003778  val: 0.003129\n","Epoch[25/600]  train: 0.003737  val: 0.002534\n","Epoch[26/600]  train: 0.003306  val: 0.002325\n","Epoch[27/600]  train: 0.003080  val: 0.003090\n","Epoch[28/600]  train: 0.004142  val: 0.003425\n","Epoch[29/600]  train: 0.004192  val: 0.006437\n","Epoch[30/600]  train: 0.004013  val: 0.006037\n","Epoch[31/600]  train: 0.003037  val: 0.002369\n","Epoch[32/600]  train: 0.003284  val: 0.002583\n","Epoch[33/600]  train: 0.002571  val: 0.003760\n","Epoch[34/600]  train: 0.001969  val: 0.001228\n","Epoch[35/600]  train: 0.001442  val: 0.000982\n","Epoch[36/600]  train: 0.001620  val: 0.001315\n","Epoch[37/600]  train: 0.001581  val: 0.001638\n","Epoch[38/600]  train: 0.001746  val: 0.001642\n","Epoch[39/600]  train: 0.001170  val: 0.001810\n","Epoch[40/600]  train: 0.001239  val: 0.001196\n","Epoch[41/600]  train: 0.002065  val: 0.002596\n","Epoch[42/600]  train: 0.002057  val: 0.001396\n","Epoch[43/600]  train: 0.001319  val: 0.000625\n","Epoch[44/600]  train: 0.001225  val: 0.001884\n","Epoch[45/600]  train: 0.001048  val: 0.001216\n","Epoch[46/600]  train: 0.001174  val: 0.001848\n","Epoch[47/600]  train: 0.001132  val: 0.001309\n","Epoch[48/600]  train: 0.001384  val: 0.001333\n","Epoch[49/600]  train: 0.001219  val: 0.000967\n","Epoch[50/600]  train: 0.001920  val: 0.006009\n","Epoch[51/600]  train: 0.006035  val: 0.002301\n","Epoch[52/600]  train: 0.002960  val: 0.004376\n","Epoch[53/600]  train: 0.004907  val: 0.006058\n","Epoch[54/600]  train: 0.004325  val: 0.002563\n","Epoch[55/600]  train: 0.003244  val: 0.003078\n","Epoch[56/600]  train: 0.002530  val: 0.001941\n","Epoch[57/600]  train: 0.001686  val: 0.001176\n","Epoch[58/600]  train: 0.000870  val: 0.000383\n","Epoch[59/600]  train: 0.000696  val: 0.000425\n","Epoch[60/600]  train: 0.000745  val: 0.000401\n","Epoch[61/600]  train: 0.000697  val: 0.000693\n","Epoch[62/600]  train: 0.000849  val: 0.000329\n","Epoch[63/600]  train: 0.001076  val: 0.000659\n","Epoch[64/600]  train: 0.001006  val: 0.001690\n","Epoch[65/600]  train: 0.000888  val: 0.000438\n","Epoch[66/600]  train: 0.000544  val: 0.000304\n","Epoch[67/600]  train: 0.000748  val: 0.001274\n","Epoch[68/600]  train: 0.001274  val: 0.000584\n","Epoch[69/600]  train: 0.001367  val: 0.001963\n","Epoch[70/600]  train: 0.001349  val: 0.000578\n","Epoch[71/600]  train: 0.000791  val: 0.000729\n","Epoch[72/600]  train: 0.000958  val: 0.001012\n","Epoch[73/600]  train: 0.001026  val: 0.001254\n","Epoch[74/600]  train: 0.000801  val: 0.000540\n","Epoch[75/600]  train: 0.000903  val: 0.001302\n","Epoch[76/600]  train: 0.000646  val: 0.000457\n","Epoch[77/600]  train: 0.000526  val: 0.001213\n","Epoch[78/600]  train: 0.000836  val: 0.001459\n","Epoch[79/600]  train: 0.000928  val: 0.000991\n","Epoch[80/600]  train: 0.000835  val: 0.000332\n","Epoch[81/600]  train: 0.000816  val: 0.000550\n","Epoch[82/600]  train: 0.000632  val: 0.000408\n","Epoch[83/600]  train: 0.000632  val: 0.000342\n","Epoch[84/600]  train: 0.000756  val: 0.000418\n","Epoch[85/600]  train: 0.003072  val: 0.004837\n","Epoch[86/600]  train: 0.003156  val: 0.001137\n","Epoch[87/600]  train: 0.001850  val: 0.001612\n","Epoch[88/600]  train: 0.001641  val: 0.002145\n","Epoch[89/600]  train: 0.003638  val: 0.002698\n","Epoch[90/600]  train: 0.001387  val: 0.000728\n","Epoch[91/600]  train: 0.001420  val: 0.001022\n","Epoch[92/600]  train: 0.001424  val: 0.003475\n","Epoch[93/600]  train: 0.001450  val: 0.000905\n","Epoch[94/600]  train: 0.001212  val: 0.001101\n","Epoch[95/600]  train: 0.001409  val: 0.000998\n","Epoch[96/600]  train: 0.001592  val: 0.001637\n","Epoch[97/600]  train: 0.001202  val: 0.001468\n","Epoch[98/600]  train: 0.001254  val: 0.001350\n","Epoch[99/600]  train: 0.001288  val: 0.000984\n","Epoch[100/600]  train: 0.000618  val: 0.001018\n","Epoch[101/600]  train: 0.000844  val: 0.000904\n","Epoch[102/600]  train: 0.000941  val: 0.000640\n","Epoch[103/600]  train: 0.001360  val: 0.001218\n","Epoch[104/600]  train: 0.001304  val: 0.000631\n","Epoch[105/600]  train: 0.002081  val: 0.000598\n","Epoch[106/600]  train: 0.001288  val: 0.001915\n","Epoch[107/600]  train: 0.001264  val: 0.000657\n","Epoch[108/600]  train: 0.000419  val: 0.000336\n","Epoch[109/600]  train: 0.000725  val: 0.000437\n","Epoch[110/600]  train: 0.000759  val: 0.000534\n","Epoch[111/600]  train: 0.000568  val: 0.000235\n","Epoch[112/600]  train: 0.000372  val: 0.000735\n","Epoch[113/600]  train: 0.000663  val: 0.000498\n","Epoch[114/600]  train: 0.000556  val: 0.000471\n","Epoch[115/600]  train: 0.000755  val: 0.000847\n","Epoch[116/600]  train: 0.000698  val: 0.000475\n","Epoch[117/600]  train: 0.000865  val: 0.001116\n","Epoch[118/600]  train: 0.000700  val: 0.000619\n","Epoch[119/600]  train: 0.000382  val: 0.000345\n","Epoch[120/600]  train: 0.000402  val: 0.000274\n","Epoch[121/600]  train: 0.000683  val: 0.000406\n","Epoch[122/600]  train: 0.001584  val: 0.003376\n","Epoch[123/600]  train: 0.001679  val: 0.000667\n","Epoch[124/600]  train: 0.001126  val: 0.002053\n","Epoch[125/600]  train: 0.001721  val: 0.002150\n","Epoch[126/600]  train: 0.001408  val: 0.000959\n","Epoch[127/600]  train: 0.001814  val: 0.002989\n","Epoch[128/600]  train: 0.001174  val: 0.001462\n","Epoch[129/600]  train: 0.000651  val: 0.000328\n","Epoch[130/600]  train: 0.000865  val: 0.001669\n","Epoch[131/600]  train: 0.000820  val: 0.000545\n","Epoch[132/600]  train: 0.002488  val: 0.001016\n","Epoch[133/600]  train: 0.000813  val: 0.000258\n","Epoch[134/600]  train: 0.000784  val: 0.000613\n","Epoch[135/600]  train: 0.001068  val: 0.000400\n","Epoch[136/600]  train: 0.000418  val: 0.000606\n","Epoch[137/600]  train: 0.000418  val: 0.000875\n","Epoch[138/600]  train: 0.000708  val: 0.001033\n","Epoch[139/600]  train: 0.000551  val: 0.000229\n","Epoch[140/600]  train: 0.000583  val: 0.001707\n","Epoch[141/600]  train: 0.001127  val: 0.000457\n","Epoch[142/600]  train: 0.000340  val: 0.000709\n","Epoch[143/600]  train: 0.001176  val: 0.002802\n","Epoch[144/600]  train: 0.000718  val: 0.000227\n","Epoch[145/600]  train: 0.000585  val: 0.000317\n","Epoch[146/600]  train: 0.000433  val: 0.000303\n","Epoch[147/600]  train: 0.000329  val: 0.000787\n","Epoch[148/600]  train: 0.000506  val: 0.000368\n","Epoch[149/600]  train: 0.000408  val: 0.000246\n","Epoch[150/600]  train: 0.000428  val: 0.000745\n","Epoch[151/600]  train: 0.000863  val: 0.000515\n","Epoch[152/600]  train: 0.000495  val: 0.000275\n","Epoch[153/600]  train: 0.000560  val: 0.000277\n","Epoch[154/600]  train: 0.000770  val: 0.001096\n","Epoch[155/600]  train: 0.000730  val: 0.000264\n","Epoch[156/600]  train: 0.000467  val: 0.000392\n","Epoch[157/600]  train: 0.000327  val: 0.000250\n","Epoch[158/600]  train: 0.000726  val: 0.000654\n","Epoch[159/600]  train: 0.000435  val: 0.000290\n","Epoch[160/600]  train: 0.000464  val: 0.000243\n","Epoch[161/600]  train: 0.000503  val: 0.000367\n","Epoch[162/600]  train: 0.000513  val: 0.000593\n","Epoch[163/600]  train: 0.000491  val: 0.000891\n","Epoch[164/600]  train: 0.001752  val: 0.001963\n","Epoch[165/600]  train: 0.001727  val: 0.002859\n","Epoch[166/600]  train: 0.002371  val: 0.001649\n","Epoch[167/600]  train: 0.001544  val: 0.000573\n","Epoch[168/600]  train: 0.000880  val: 0.001275\n","Epoch[169/600]  train: 0.000817  val: 0.001114\n","Epoch[170/600]  train: 0.000851  val: 0.001731\n","Epoch[171/600]  train: 0.001205  val: 0.000589\n","Epoch[172/600]  train: 0.000594  val: 0.000423\n","Epoch[173/600]  train: 0.000803  val: 0.000749\n","Epoch[174/600]  train: 0.000905  val: 0.000550\n","Epoch[175/600]  train: 0.000591  val: 0.000922\n","Epoch[176/600]  train: 0.000855  val: 0.000239\n","Epoch[177/600]  train: 0.000436  val: 0.000273\n","Epoch[178/600]  train: 0.000885  val: 0.000839\n","Epoch[179/600]  train: 0.000565  val: 0.000433\n","Epoch[180/600]  train: 0.000486  val: 0.000849\n","Epoch[181/600]  train: 0.001003  val: 0.001163\n","Epoch[182/600]  train: 0.001633  val: 0.003547\n","Epoch[183/600]  train: 0.001348  val: 0.002167\n","Epoch[184/600]  train: 0.000950  val: 0.000362\n","Epoch[185/600]  train: 0.000471  val: 0.000353\n","Epoch[186/600]  train: 0.000515  val: 0.000293\n","Epoch[187/600]  train: 0.000624  val: 0.000571\n","Epoch[188/600]  train: 0.000401  val: 0.000450\n","Epoch[189/600]  train: 0.000551  val: 0.000296\n","Epoch[190/600]  train: 0.000490  val: 0.000748\n","Epoch[191/600]  train: 0.001112  val: 0.000626\n","Epoch[192/600]  train: 0.001358  val: 0.000971\n","Epoch[193/600]  train: 0.001145  val: 0.000792\n","Epoch[194/600]  train: 0.000819  val: 0.000457\n","Epoch[195/600]  train: 0.000710  val: 0.000784\n","Epoch[196/600]  train: 0.001285  val: 0.000785\n","Epoch[197/600]  train: 0.000957  val: 0.002129\n","Epoch[198/600]  train: 0.001129  val: 0.002409\n","Epoch[199/600]  train: 0.001205  val: 0.001119\n","Epoch[200/600]  train: 0.000716  val: 0.000893\n","Epoch[201/600]  train: 0.001278  val: 0.001457\n","Epoch[202/600]  train: 0.001008  val: 0.001415\n","Epoch[203/600]  train: 0.001283  val: 0.000965\n","Epoch[204/600]  train: 0.000864  val: 0.001312\n","Epoch[205/600]  train: 0.002009  val: 0.001330\n","Epoch[206/600]  train: 0.000859  val: 0.000484\n","Epoch[207/600]  train: 0.001190  val: 0.001206\n","Epoch[208/600]  train: 0.000733  val: 0.000936\n","Epoch[209/600]  train: 0.000876  val: 0.000494\n","Epoch[210/600]  train: 0.001167  val: 0.001196\n","Epoch[211/600]  train: 0.000569  val: 0.000419\n","Epoch[212/600]  train: 0.000531  val: 0.000777\n","Epoch[213/600]  train: 0.000646  val: 0.000512\n","Epoch[214/600]  train: 0.000599  val: 0.000472\n","Epoch[215/600]  train: 0.000932  val: 0.000903\n","Epoch[216/600]  train: 0.001422  val: 0.001145\n","Epoch[217/600]  train: 0.000627  val: 0.000510\n","Epoch[218/600]  train: 0.000563  val: 0.000329\n","Epoch[219/600]  train: 0.000620  val: 0.000646\n","Epoch[220/600]  train: 0.000845  val: 0.000669\n","Epoch[221/600]  train: 0.000595  val: 0.001580\n","Epoch[222/600]  train: 0.001267  val: 0.000920\n","Epoch[223/600]  train: 0.000811  val: 0.000690\n","Epoch[224/600]  train: 0.000939  val: 0.002543\n","Epoch[225/600]  train: 0.001362  val: 0.000992\n","Epoch[226/600]  train: 0.000758  val: 0.000639\n","Epoch[227/600]  train: 0.000727  val: 0.000574\n","Epoch[228/600]  train: 0.001033  val: 0.001026\n","Epoch[229/600]  train: 0.001000  val: 0.001053\n","Epoch[230/600]  train: 0.001176  val: 0.000847\n","Epoch[231/600]  train: 0.000675  val: 0.000683\n","Epoch[232/600]  train: 0.000590  val: 0.000391\n","Epoch[233/600]  train: 0.000524  val: 0.000709\n","Epoch[234/600]  train: 0.000566  val: 0.001265\n","Epoch[235/600]  train: 0.001083  val: 0.001480\n","Epoch[236/600]  train: 0.000925  val: 0.000804\n","Epoch[237/600]  train: 0.000772  val: 0.001567\n","Epoch[238/600]  train: 0.000784  val: 0.000420\n","Epoch[239/600]  train: 0.000588  val: 0.000797\n","Epoch[240/600]  train: 0.000470  val: 0.000598\n","Epoch[241/600]  train: 0.000521  val: 0.000355\n","Epoch[242/600]  train: 0.000470  val: 0.000281\n","Epoch[243/600]  train: 0.000638  val: 0.002253\n","Epoch[244/600]  train: 0.000993  val: 0.001649\n","Epoch[245/600]  train: 0.000637  val: 0.000651\n","Epoch[246/600]  train: 0.000410  val: 0.001140\n","Epoch[247/600]  train: 0.000454  val: 0.000326\n","Epoch[248/600]  train: 0.000640  val: 0.001497\n","Epoch[249/600]  train: 0.002326  val: 0.000869\n","Epoch[250/600]  train: 0.000494  val: 0.001005\n","Epoch[251/600]  train: 0.001074  val: 0.000867\n","Epoch[252/600]  train: 0.000862  val: 0.002839\n","Epoch[253/600]  train: 0.000695  val: 0.001508\n","Epoch[254/600]  train: 0.000854  val: 0.001219\n","Epoch[255/600]  train: 0.003072  val: 0.004020\n","Epoch[256/600]  train: 0.002383  val: 0.002345\n","Epoch[257/600]  train: 0.001158  val: 0.001639\n","Epoch[258/600]  train: 0.000741  val: 0.000576\n","Epoch[259/600]  train: 0.000729  val: 0.000726\n","Epoch[260/600]  train: 0.000456  val: 0.000327\n","Epoch[261/600]  train: 0.000454  val: 0.001011\n","Epoch[262/600]  train: 0.000803  val: 0.000462\n","Epoch[263/600]  train: 0.000556  val: 0.000491\n","Epoch[264/600]  train: 0.000499  val: 0.000370\n","Epoch[265/600]  train: 0.000447  val: 0.000635\n","Epoch[266/600]  train: 0.000952  val: 0.000683\n","Epoch[267/600]  train: 0.000861  val: 0.000421\n","Epoch[268/600]  train: 0.001666  val: 0.000831\n","Epoch[269/600]  train: 0.000590  val: 0.000600\n","Epoch[270/600]  train: 0.000620  val: 0.000382\n","Epoch[271/600]  train: 0.000734  val: 0.000396\n","Epoch[272/600]  train: 0.000445  val: 0.000407\n","Epoch[273/600]  train: 0.000495  val: 0.001350\n","Epoch[274/600]  train: 0.000561  val: 0.000556\n","Epoch[275/600]  train: 0.000555  val: 0.000605\n","Epoch[276/600]  train: 0.000573  val: 0.000377\n","Epoch[277/600]  train: 0.000568  val: 0.000381\n","Epoch[278/600]  train: 0.000722  val: 0.000727\n","Epoch[279/600]  train: 0.000870  val: 0.000723\n","Epoch[280/600]  train: 0.000440  val: 0.000293\n","Epoch[281/600]  train: 0.000497  val: 0.000401\n","Epoch[282/600]  train: 0.000809  val: 0.000453\n","Epoch[283/600]  train: 0.000728  val: 0.000980\n","Epoch[284/600]  train: 0.000619  val: 0.000641\n","Epoch[285/600]  train: 0.000995  val: 0.002813\n","Epoch[286/600]  train: 0.000782  val: 0.000552\n","Epoch[287/600]  train: 0.000640  val: 0.001978\n","Epoch[288/600]  train: 0.000751  val: 0.000659\n","Epoch[289/600]  train: 0.000515  val: 0.000308\n","Epoch[290/600]  train: 0.000998  val: 0.001072\n","Epoch[291/600]  train: 0.000556  val: 0.000397\n","Epoch[292/600]  train: 0.000749  val: 0.000978\n","Epoch[293/600]  train: 0.001268  val: 0.000619\n","Epoch[294/600]  train: 0.000662  val: 0.000923\n","Epoch[295/600]  train: 0.001153  val: 0.002270\n","Epoch[296/600]  train: 0.001628  val: 0.002725\n","Epoch[297/600]  train: 0.001390  val: 0.001199\n","Epoch[298/600]  train: 0.001003  val: 0.001241\n","Epoch[299/600]  train: 0.000911  val: 0.000592\n","Epoch[300/600]  train: 0.000781  val: 0.001753\n","Epoch[301/600]  train: 0.000825  val: 0.000521\n","Epoch[302/600]  train: 0.000701  val: 0.000513\n","Epoch[303/600]  train: 0.000903  val: 0.000537\n","Epoch[304/600]  train: 0.000950  val: 0.000834\n","Epoch[305/600]  train: 0.000917  val: 0.000779\n","Epoch[306/600]  train: 0.000880  val: 0.000797\n","Epoch[307/600]  train: 0.000809  val: 0.000881\n","Epoch[308/600]  train: 0.001022  val: 0.001936\n","Epoch[309/600]  train: 0.000762  val: 0.000776\n","Epoch[310/600]  train: 0.000928  val: 0.001327\n","Epoch[311/600]  train: 0.001175  val: 0.000694\n","Epoch[312/600]  train: 0.001446  val: 0.001129\n","Epoch[313/600]  train: 0.001018  val: 0.001250\n","Epoch[314/600]  train: 0.000963  val: 0.000965\n","Epoch[315/600]  train: 0.000810  val: 0.000728\n","Epoch[316/600]  train: 0.001207  val: 0.001430\n","Epoch[317/600]  train: 0.001033  val: 0.000678\n","Epoch[318/600]  train: 0.001123  val: 0.001694\n","Epoch[319/600]  train: 0.001156  val: 0.000946\n","Epoch[320/600]  train: 0.001322  val: 0.001496\n","Epoch[321/600]  train: 0.001863  val: 0.002024\n","Epoch[322/600]  train: 0.001419  val: 0.000963\n","Epoch[323/600]  train: 0.001041  val: 0.000631\n","Epoch[324/600]  train: 0.000746  val: 0.000616\n","Epoch[325/600]  train: 0.000827  val: 0.000766\n","Epoch[326/600]  train: 0.001007  val: 0.001930\n","Epoch[327/600]  train: 0.000814  val: 0.001041\n","Epoch[328/600]  train: 0.000712  val: 0.000660\n","Epoch[329/600]  train: 0.000704  val: 0.000642\n","Epoch[330/600]  train: 0.000741  val: 0.000982\n","Epoch[331/600]  train: 0.000820  val: 0.000789\n","Epoch[332/600]  train: 0.000823  val: 0.000572\n","Epoch[333/600]  train: 0.001084  val: 0.001638\n","Epoch[334/600]  train: 0.000913  val: 0.001018\n","Epoch[335/600]  train: 0.000772  val: 0.001217\n","Epoch[336/600]  train: 0.000859  val: 0.001219\n","Epoch[337/600]  train: 0.000771  val: 0.000695\n","Epoch[338/600]  train: 0.000781  val: 0.000798\n","Epoch[339/600]  train: 0.000826  val: 0.000606\n","Epoch[340/600]  train: 0.000872  val: 0.001142\n","Epoch[341/600]  train: 0.000928  val: 0.001161\n","Epoch[342/600]  train: 0.000688  val: 0.000646\n","Epoch[343/600]  train: 0.000659  val: 0.001327\n","Epoch[344/600]  train: 0.001070  val: 0.000768\n","Epoch[345/600]  train: 0.001002  val: 0.000761\n","Epoch[346/600]  train: 0.000987  val: 0.000702\n","Epoch[347/600]  train: 0.000763  val: 0.001091\n","Epoch[348/600]  train: 0.000843  val: 0.000564\n","Epoch[349/600]  train: 0.000630  val: 0.000627\n","Epoch[350/600]  train: 0.000692  val: 0.000550\n","Epoch[351/600]  train: 0.000633  val: 0.000639\n","Epoch[352/600]  train: 0.000807  val: 0.000533\n","Epoch[353/600]  train: 0.000789  val: 0.000899\n","Epoch[354/600]  train: 0.000675  val: 0.000785\n","Epoch[355/600]  train: 0.000765  val: 0.000512\n","Epoch[356/600]  train: 0.000683  val: 0.000641\n","Epoch[357/600]  train: 0.000748  val: 0.000595\n","Epoch[358/600]  train: 0.000737  val: 0.000496\n","Epoch[359/600]  train: 0.000945  val: 0.000553\n","Epoch[360/600]  train: 0.000669  val: 0.000669\n","Epoch[361/600]  train: 0.000952  val: 0.000677\n","Epoch[362/600]  train: 0.000662  val: 0.000487\n","Epoch[363/600]  train: 0.000753  val: 0.000529\n","Epoch[364/600]  train: 0.000555  val: 0.000611\n","Epoch[365/600]  train: 0.000699  val: 0.000670\n","Epoch[366/600]  train: 0.000801  val: 0.000458\n","Epoch[367/600]  train: 0.000727  val: 0.000493\n","Epoch[368/600]  train: 0.000509  val: 0.000433\n","Epoch[369/600]  train: 0.000710  val: 0.001217\n","Epoch[370/600]  train: 0.000796  val: 0.001234\n","Epoch[371/600]  train: 0.000757  val: 0.000478\n","Epoch[372/600]  train: 0.000721  val: 0.000530\n","Epoch[373/600]  train: 0.000631  val: 0.000768\n","Epoch[374/600]  train: 0.000731  val: 0.000575\n","Epoch[375/600]  train: 0.000710  val: 0.000634\n","Epoch[376/600]  train: 0.001158  val: 0.001936\n","Epoch[377/600]  train: 0.001047  val: 0.000924\n","Epoch[378/600]  train: 0.000718  val: 0.000607\n","Epoch[379/600]  train: 0.000834  val: 0.000600\n","Epoch[380/600]  train: 0.000616  val: 0.000457\n","Epoch[381/600]  train: 0.000660  val: 0.000514\n","Epoch[382/600]  train: 0.000670  val: 0.000697\n","Epoch[383/600]  train: 0.000771  val: 0.000610\n","Epoch[384/600]  train: 0.000630  val: 0.000553\n","Epoch[385/600]  train: 0.000814  val: 0.000795\n","Epoch[386/600]  train: 0.000630  val: 0.000541\n","Epoch[387/600]  train: 0.000751  val: 0.000709\n","Epoch[388/600]  train: 0.000933  val: 0.000570\n","Epoch[389/600]  train: 0.001105  val: 0.002110\n","Epoch[390/600]  train: 0.000960  val: 0.000612\n","Epoch[391/600]  train: 0.000781  val: 0.000878\n","Epoch[392/600]  train: 0.001021  val: 0.001526\n","Epoch[393/600]  train: 0.000878  val: 0.000558\n","Epoch[394/600]  train: 0.001353  val: 0.001414\n","Epoch[395/600]  train: 0.000753  val: 0.000593\n","Epoch[396/600]  train: 0.000651  val: 0.000488\n","Epoch[397/600]  train: 0.001154  val: 0.000841\n","Epoch[398/600]  train: 0.000723  val: 0.000595\n","Epoch[399/600]  train: 0.000665  val: 0.000784\n","Epoch[400/600]  train: 0.001226  val: 0.000573\n","Epoch[401/600]  train: 0.000613  val: 0.000550\n","Epoch[402/600]  train: 0.000817  val: 0.001535\n","Epoch[403/600]  train: 0.000932  val: 0.000658\n","Epoch[404/600]  train: 0.000611  val: 0.000698\n","Epoch[405/600]  train: 0.000815  val: 0.001157\n","Epoch[406/600]  train: 0.000911  val: 0.001380\n","Epoch[407/600]  train: 0.001360  val: 0.001349\n","Epoch[408/600]  train: 0.000983  val: 0.000449\n","Epoch[409/600]  train: 0.000597  val: 0.000463\n","Epoch[410/600]  train: 0.000529  val: 0.000345\n","Epoch[411/600]  train: 0.000503  val: 0.000340\n","Epoch[412/600]  train: 0.000752  val: 0.000363\n","Epoch[413/600]  train: 0.000497  val: 0.000589\n","Epoch[414/600]  train: 0.000693  val: 0.000541\n","Epoch[415/600]  train: 0.000450  val: 0.000772\n","Epoch[416/600]  train: 0.000999  val: 0.001130\n","Epoch[417/600]  train: 0.001283  val: 0.000645\n","Epoch[418/600]  train: 0.000710  val: 0.000995\n","Epoch[419/600]  train: 0.000938  val: 0.000592\n","Epoch[420/600]  train: 0.000522  val: 0.000482\n","Epoch[421/600]  train: 0.000609  val: 0.000793\n","Epoch[422/600]  train: 0.000591  val: 0.000562\n","Epoch[423/600]  train: 0.001065  val: 0.000461\n","Epoch[424/600]  train: 0.001134  val: 0.001856\n","Epoch[425/600]  train: 0.000700  val: 0.001673\n","Epoch[426/600]  train: 0.000885  val: 0.000555\n","Epoch[427/600]  train: 0.000863  val: 0.000510\n","Epoch[428/600]  train: 0.000626  val: 0.000536\n","Epoch[429/600]  train: 0.000700  val: 0.000465\n","Epoch[430/600]  train: 0.000720  val: 0.000553\n","Epoch[431/600]  train: 0.000724  val: 0.001396\n","Epoch[432/600]  train: 0.000773  val: 0.000915\n","Epoch[433/600]  train: 0.000735  val: 0.000437\n","Epoch[434/600]  train: 0.000525  val: 0.000640\n","Epoch[435/600]  train: 0.000702  val: 0.000977\n","Epoch[436/600]  train: 0.001025  val: 0.001122\n","Epoch[437/600]  train: 0.000698  val: 0.001351\n","Epoch[438/600]  train: 0.000745  val: 0.000509\n","Epoch[439/600]  train: 0.001027  val: 0.001894\n","Epoch[440/600]  train: 0.000791  val: 0.000701\n","Epoch[441/600]  train: 0.000556  val: 0.000436\n","Epoch[442/600]  train: 0.000579  val: 0.000517\n","Epoch[443/600]  train: 0.000600  val: 0.001116\n","Epoch[444/600]  train: 0.000760  val: 0.000973\n","Epoch[445/600]  train: 0.000660  val: 0.000539\n","Epoch[446/600]  train: 0.000514  val: 0.001036\n","Epoch[447/600]  train: 0.000929  val: 0.000749\n","Epoch[448/600]  train: 0.000861  val: 0.000424\n","Epoch[449/600]  train: 0.000473  val: 0.000437\n","Epoch[450/600]  train: 0.000447  val: 0.000329\n","Epoch[451/600]  train: 0.000496  val: 0.001680\n","Epoch[452/600]  train: 0.000767  val: 0.000641\n","Epoch[453/600]  train: 0.000523  val: 0.000374\n","Epoch[454/600]  train: 0.000874  val: 0.000684\n","Epoch[455/600]  train: 0.000948  val: 0.000932\n","Epoch[456/600]  train: 0.000720  val: 0.001091\n","Epoch[457/600]  train: 0.001065  val: 0.000938\n","Epoch[458/600]  train: 0.000616  val: 0.000506\n","Epoch[459/600]  train: 0.000737  val: 0.001168\n","Epoch[460/600]  train: 0.000731  val: 0.000466\n","Epoch[461/600]  train: 0.000544  val: 0.000914\n","Epoch[462/600]  train: 0.001697  val: 0.000992\n","Epoch[463/600]  train: 0.001315  val: 0.002326\n","Epoch[464/600]  train: 0.000931  val: 0.000550\n","Epoch[465/600]  train: 0.000715  val: 0.000610\n","Epoch[466/600]  train: 0.000544  val: 0.000592\n","Epoch[467/600]  train: 0.000770  val: 0.000785\n","Epoch[468/600]  train: 0.000787  val: 0.000485\n","Epoch[469/600]  train: 0.000582  val: 0.000554\n","Epoch[470/600]  train: 0.000643  val: 0.000765\n","Epoch[471/600]  train: 0.000613  val: 0.000652\n","Epoch[472/600]  train: 0.000744  val: 0.000953\n","Epoch[473/600]  train: 0.000611  val: 0.000826\n","Epoch[474/600]  train: 0.000687  val: 0.000439\n","Epoch[475/600]  train: 0.000766  val: 0.000687\n","Epoch[476/600]  train: 0.000803  val: 0.000880\n","Epoch[477/600]  train: 0.000604  val: 0.000478\n","Epoch[478/600]  train: 0.000728  val: 0.000680\n","Epoch[479/600]  train: 0.000913  val: 0.002030\n","Epoch[480/600]  train: 0.000948  val: 0.000595\n","Epoch[481/600]  train: 0.000887  val: 0.000779\n","Epoch[482/600]  train: 0.000608  val: 0.000503\n","Epoch[483/600]  train: 0.000865  val: 0.000620\n","Epoch[484/600]  train: 0.000556  val: 0.000518\n","Epoch[485/600]  train: 0.001138  val: 0.000761\n","Epoch[486/600]  train: 0.001059  val: 0.003475\n","Epoch[487/600]  train: 0.001742  val: 0.000476\n","Epoch[488/600]  train: 0.000728  val: 0.000477\n","Epoch[489/600]  train: 0.000559  val: 0.000573\n","Epoch[490/600]  train: 0.000621  val: 0.000436\n","Epoch[491/600]  train: 0.000717  val: 0.001037\n","Epoch[492/600]  train: 0.000804  val: 0.000536\n","Epoch[493/600]  train: 0.000714  val: 0.000666\n","Epoch[494/600]  train: 0.001121  val: 0.000507\n","Epoch[495/600]  train: 0.000594  val: 0.000460\n","Epoch[496/600]  train: 0.000548  val: 0.000464\n","Epoch[497/600]  train: 0.000792  val: 0.000753\n","Epoch[498/600]  train: 0.000691  val: 0.000778\n","Epoch[499/600]  train: 0.000560  val: 0.000857\n","Epoch[500/600]  train: 0.000668  val: 0.001905\n","Epoch[501/600]  train: 0.000782  val: 0.000480\n","Epoch[502/600]  train: 0.000523  val: 0.000454\n","Epoch[503/600]  train: 0.000502  val: 0.000533\n","Epoch[504/600]  train: 0.000488  val: 0.000818\n","Epoch[505/600]  train: 0.000871  val: 0.000502\n","Epoch[506/600]  train: 0.000733  val: 0.000648\n","Epoch[507/600]  train: 0.000900  val: 0.000685\n","Epoch[508/600]  train: 0.000783  val: 0.001210\n","Epoch[509/600]  train: 0.000716  val: 0.000546\n","Epoch[510/600]  train: 0.000929  val: 0.000685\n","Epoch[511/600]  train: 0.000650  val: 0.000530\n","Epoch[512/600]  train: 0.000482  val: 0.000500\n","Epoch[513/600]  train: 0.000901  val: 0.000548\n","Epoch[514/600]  train: 0.000688  val: 0.000686\n","Epoch[515/600]  train: 0.000778  val: 0.000604\n","Epoch[516/600]  train: 0.000733  val: 0.000856\n","Epoch[517/600]  train: 0.000703  val: 0.000718\n","Epoch[518/600]  train: 0.000603  val: 0.000469\n","Epoch[519/600]  train: 0.000625  val: 0.000559\n","Epoch[520/600]  train: 0.000698  val: 0.000861\n","Epoch[521/600]  train: 0.000935  val: 0.002922\n","Epoch[522/600]  train: 0.002384  val: 0.000712\n","Epoch[523/600]  train: 0.001042  val: 0.000791\n","Epoch[524/600]  train: 0.000736  val: 0.000580\n","Epoch[525/600]  train: 0.000616  val: 0.000489\n","Epoch[526/600]  train: 0.000553  val: 0.000548\n","Epoch[527/600]  train: 0.000831  val: 0.000541\n","Epoch[528/600]  train: 0.000612  val: 0.000512\n","Epoch[529/600]  train: 0.000703  val: 0.000479\n","Epoch[530/600]  train: 0.000762  val: 0.000977\n","Epoch[531/600]  train: 0.002074  val: 0.001634\n","Epoch[532/600]  train: 0.001509  val: 0.001024\n","Epoch[533/600]  train: 0.000865  val: 0.000806\n","Epoch[534/600]  train: 0.000729  val: 0.000924\n","Epoch[535/600]  train: 0.000808  val: 0.000785\n","Epoch[536/600]  train: 0.000753  val: 0.000499\n","Epoch[537/600]  train: 0.000614  val: 0.000581\n","Epoch[538/600]  train: 0.000803  val: 0.000579\n","Epoch[539/600]  train: 0.000582  val: 0.000349\n","Epoch[540/600]  train: 0.000609  val: 0.000465\n","Epoch[541/600]  train: 0.000623  val: 0.000505\n","Epoch[542/600]  train: 0.000664  val: 0.000624\n","Epoch[543/600]  train: 0.001177  val: 0.000557\n","Epoch[544/600]  train: 0.000813  val: 0.000732\n","Epoch[545/600]  train: 0.001871  val: 0.001997\n","Epoch[546/600]  train: 0.001278  val: 0.000804\n","Epoch[547/600]  train: 0.000744  val: 0.000641\n","Epoch[548/600]  train: 0.000603  val: 0.000485\n","Epoch[549/600]  train: 0.000879  val: 0.001155\n","Epoch[550/600]  train: 0.000789  val: 0.001468\n","Epoch[551/600]  train: 0.001205  val: 0.000528\n","Epoch[552/600]  train: 0.000772  val: 0.000774\n","Epoch[553/600]  train: 0.000829  val: 0.001605\n","Epoch[554/600]  train: 0.000935  val: 0.000823\n","Epoch[555/600]  train: 0.000970  val: 0.000573\n","Epoch[556/600]  train: 0.000620  val: 0.000639\n","Epoch[557/600]  train: 0.000689  val: 0.000605\n","Epoch[558/600]  train: 0.000566  val: 0.000518\n","Epoch[559/600]  train: 0.000820  val: 0.000952\n","Epoch[560/600]  train: 0.000721  val: 0.000977\n","Epoch[561/600]  train: 0.001613  val: 0.001225\n","Epoch[562/600]  train: 0.000640  val: 0.000831\n","Epoch[563/600]  train: 0.000635  val: 0.000621\n","Epoch[564/600]  train: 0.000720  val: 0.000696\n","Epoch[565/600]  train: 0.000664  val: 0.000451\n","Epoch[566/600]  train: 0.000596  val: 0.000546\n","Epoch[567/600]  train: 0.000707  val: 0.000575\n","Epoch[568/600]  train: 0.000533  val: 0.000376\n","Epoch[569/600]  train: 0.000631  val: 0.000756\n","Epoch[570/600]  train: 0.000960  val: 0.000489\n","Epoch[571/600]  train: 0.000649  val: 0.000721\n","Epoch[572/600]  train: 0.000636  val: 0.000679\n","Epoch[573/600]  train: 0.000747  val: 0.000747\n","Epoch[574/600]  train: 0.000610  val: 0.000622\n","Epoch[575/600]  train: 0.000755  val: 0.001553\n","Epoch[576/600]  train: 0.000767  val: 0.000913\n","Epoch[577/600]  train: 0.000629  val: 0.001142\n","Epoch[578/600]  train: 0.000568  val: 0.000651\n","Epoch[579/600]  train: 0.000718  val: 0.001591\n","Epoch[580/600]  train: 0.000772  val: 0.000601\n","Epoch[581/600]  train: 0.000679  val: 0.000767\n","Epoch[582/600]  train: 0.000551  val: 0.001131\n","Epoch[583/600]  train: 0.000708  val: 0.000553\n","Epoch[584/600]  train: 0.000653  val: 0.000796\n","Epoch[585/600]  train: 0.001056  val: 0.000666\n","Epoch[586/600]  train: 0.000605  val: 0.000453\n","Epoch[587/600]  train: 0.000656  val: 0.000703\n","Epoch[588/600]  train: 0.000637  val: 0.000507\n","Epoch[589/600]  train: 0.000840  val: 0.000955\n","Epoch[590/600]  train: 0.000834  val: 0.000834\n","Epoch[591/600]  train: 0.000713  val: 0.000678\n","Epoch[592/600]  train: 0.000665  val: 0.000432\n","Epoch[593/600]  train: 0.000957  val: 0.001486\n","Epoch[594/600]  train: 0.000947  val: 0.000817\n","Epoch[595/600]  train: 0.000789  val: 0.000802\n","Epoch[596/600]  train: 0.000636  val: 0.000487\n","Epoch[597/600]  train: 0.000688  val: 0.000741\n","Epoch[598/600]  train: 0.000761  val: 0.000568\n","Epoch[599/600]  train: 0.000502  val: 0.000576\n","Epoch[600/600]  train: 0.000659  val: 0.001231\n"]}]},{"cell_type":"code","source":["DRIVE_PATH = \"C:/Users/PC/Desktop/Deep/Materials_380\"\n","DATA_DIR = DRIVE_PATH\n","# ==========================================================\n","#      [2. (참고) 나중에 저장된 모델 불러오기]\n","# ==========================================================\n","# 이 부분은 나중에 모델을 사용할 때 참고하시면 됩니다.\n","\n","# 1. 먼저, 저장했을 때와 '똑같은 구조'의 모델 인스턴스를 생성합니다.\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model_to_load = InverseNet(input_dim=NUM_WAVELENGTHS, output_dim=num_layers).to(device)\n","model_to_load = model_to_load.to(torch.float64) # 저장 시 사용한 dtype과 동일하게 설정\n","\n","# 2. 저장된 파라미터 파일(.pth)을 불러와 모델에 덮어씌웁니다.\n","LOAD_PATH = SAVE_PATH # 위에서 저장한 경로\n","model_to_load.load_state_dict(torch.load(LOAD_PATH))\n","\n","# 3. 모델을 '평가 모드'로 전환합니다. (매우 중요!)\n","#    BatchNorm, Dropout 등의 레이어가 있을 경우, 추론 시에는 다르게 동작해야 하기 때문입니다.\n","model_to_load.eval()\n","\n","print(f\"\\nModel loaded successfully from {LOAD_PATH}\")\n","\n","# 이제 model_to_load를 새로운 데이터 예측에 사용할 수 있습니다.\n","# 예시:\n","# with torch.no_grad(): # 예측 시에는 그래디언트 계산이 필요 없습니다.\n","#     some_new_spectrum = torch.rand(1, NUM_WAVELENGTHS, dtype=torch.float64).to(device)\n","#     predicted_thickness = model_to_load(some_new_spectrum)\n","#     print(\"\\nPrediction with loaded model:\", predicted_thickness)\n","# =========================================================="],"metadata":{"id":"yXv81Rt2op2J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import optuna\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import os\n","\n","# =================================================================================\n","# 이 상단에 (A)와 (B) 섹션의 코드를 그대로 유지합니다.\n","# (단, DataLoader 정의는 아래 objective 함수 안으로 이동했으므로 여기서는 제외)\n","# (drive.mount, material_files, material_data, WAVELENGTHS, TMMNetwork,\n","#  Dataset 클래스 정의, 표준화/역표준화 함수 등 모든 사전 준비 코드가 여기에 있어야 합니다.)\n","# =================================================================================\n","\n","\n","def objective(trial):\n","    \"\"\"Optuna가 하이퍼파라미터 한 세트를 시험(trial)하는 함수\"\"\"\n","    seed_everything(42)\n","\n","    # --- 1. 탐색할 하이퍼파라미터 정의 ---\n","    # 각 trial 마다 Optuna가 이 범위 안에서 새로운 값을 제안합니다.\n","    lr = trial.suggest_float(\"lr\", 1e-4, 5e-4, log=True)\n","    alpha = trial.suggest_float(\"alpha\", 0.1, 1.2, log=True)\n","    n_layers = trial.suggest_int(\"n_layers\", 4, 4)  # 고정값 4로 제한\n","    n_units = trial.suggest_categorical(\"n_units\", [768])  # 고정값 768\n","    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64])\n","    beta = trial.suggest_int(\"beta\", 18, 23)\n","    num_epochs = trial.suggest_int(\"num_epochs\", 180, 200)\n","\n","    # --- 2. 제안된 하이퍼파라미터로 동적 컴포넌트 생성 ---\n","\n","    # DataLoader: trial에서 제안된 batch_size를 사용해야 하므로 함수 내에서 생성\n","    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n","\n","    # InverseNet 정의: trial에서 제안된 n_layers, n_units를 사용\n","    class InverseNet(nn.Module):\n","        def __init__(self, input_dim, output_dim, num_hidden_layers, units):\n","            super().__init__()\n","            layers = []\n","\n","            # Input Layer\n","            layers.append(nn.Linear(input_dim, units))\n","\n","            layers.append(nn.ReLU())\n","\n","            # Hidden Layers\n","            for _ in range(num_hidden_layers):\n","                layers.append(nn.Linear(units, units))\n","\n","                layers.append(nn.ReLU())\n","\n","            # Output Layer\n","            layers.append(nn.Linear(units, output_dim))\n","\n","            self.net = nn.Sequential(*layers)\n","\n","        def forward(self, x):\n","            return self.net(x)\n","\n","    # 모델 및 옵티마이저 생성: trial에서 제안된 값들로 인스턴스화\n","    num_layers_output = len(material_sequence)\n","    inverse_net = InverseNet(NUM_WAVELENGTHS, num_layers_output, n_layers, n_units).to(device)\n","    inverse_net = inverse_net.to(torch.float64)\n","    optimizer = optim.Adam(inverse_net.parameters(), lr=lr)\n","\n","    # --- 3. 기억하고 있는 학습 루프 실행 ---\n","\n","\n","    for epoch in range(num_epochs):\n","        inverse_net.train()\n","        total_loss = 0.0\n","        for batch_idx, batch in enumerate(dataloader):\n","            T_target = batch['T_target'].to(device)\n","            d_norm_true = batch['d_norm'].to(device)\n","\n","            optimizer.zero_grad()\n","\n","            squeezed_T_target = T_target.squeeze(1)\n","            d_norm_pred = inverse_net(squeezed_T_target)\n","\n","            d_nm_pred = destandardize_thickness(d_norm_pred)\n","            T_pred_batch = model(d_nm_pred)\n","\n","            T_target_destd = destandardize_spectrum(squeezed_T_target)\n","\n","            loss_spectrum = F.mse_loss(T_pred_batch, T_target_destd)\n","            loss_thickness = F.mse_loss(d_norm_pred, d_norm_true.squeeze(1))\n","\n","            # 2단계 학습 전략 (기억하고 있는 코드 로직)\n","            if epoch < beta:\n","                loss = loss_spectrum + alpha * loss_thickness # trial에서 제안된 alpha 사용\n","            else:\n","                loss = loss_spectrum\n","\n","            loss.backward()\n","            optimizer.step()\n","            total_loss += loss.item()\n","\n","    # 최종 epoch의 평균 Loss를 이 trial의 성능 지표로 반환\n","    final_avg_loss = total_loss / len(dataloader)\n","\n","    # Pruning을 위해 중간 결과 보고\n","    trial.report(final_avg_loss, epoch)\n","    if trial.should_prune():\n","        raise optuna.exceptions.TrialPruned()\n","\n","    return final_avg_loss\n","\n","# --------------------------------------------------------------------------\n","# Optuna Study 실행\n","# --------------------------------------------------------------------------\n","if __name__ == \"__main__\":\n","    # 'minimize' 방향으로 objective 함수(loss)를 최적화하는 연구(study) 생성\n","    study = optuna.create_study(\n","        direction=\"minimize\",\n","        pruner=optuna.pruners.MedianPruner(n_warmup_steps=3),\n","        sampler=optuna.samplers.TPESampler(seed=42)\n","    )\n","\n","    study.optimize(objective, n_trials=100, n_jobs=1)  # CPU-only 병렬이면 n_jobs=4 가능\n","\n","    # --- 최적 결과 확인 ---\n","    print(\"\\n\\n========================================================\")\n","    print(\"                      탐색 완료!                      \")\n","    print(\"========================================================\")\n","    print(\"최적의 Trial 번호:\", study.best_trial.number)\n","    print(\"최적의 Loss (Value):\", study.best_trial.value)\n","    print(\"\\n최적의 하이퍼파라미터 (Best Params):\")\n","    for key, value in study.best_trial.params.items():\n","        print(f\"    {key}: {value}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"00mLRmWibUeW","executionInfo":{"status":"error","timestamp":1749980685942,"user_tz":-540,"elapsed":3681695,"user":{"displayName":"이스타의행운","userId":"08935579543132628543"}},"outputId":"93c2d56c-b082-470c-b603-74493540d36a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[I 2025-06-15 17:43:24,246] A new study created in memory with name: no-name-178cfa68-0202-4323-ab29-12fd772c5cf5\n","[I 2025-06-15 18:10:12,062] Trial 0 finished with value: 0.0007120522152449268 and parameters: {'lr': 0.00018272261776066238, 'alpha': 1.0616785666328346, 'n_layers': 4, 'n_units': 768, 'batch_size': 32, 'beta': 18, 'num_epochs': 183}. Best is trial 0 with value: 0.0007120522152449268.\n","[I 2025-06-15 18:36:48,236] Trial 1 finished with value: 0.0031391777486834316 and parameters: {'lr': 0.00010979908036596662, 'alpha': 0.8605201659336437, 'n_layers': 4, 'n_units': 768, 'batch_size': 64, 'beta': 18, 'num_epochs': 200}. Best is trial 0 with value: 0.0007120522152449268.\n","[W 2025-06-15 18:44:45,774] Trial 2 failed with parameters: {'lr': 0.0003818145165896873, 'alpha': 0.16949324171244332, 'n_layers': 4, 'n_units': 768, 'batch_size': 64, 'beta': 19, 'num_epochs': 191} because of the following error: KeyboardInterrupt().\n","Traceback (most recent call last):\n","  File \"C:\\Users\\PC\\Desktop\\Deep\\dl_env\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n","    value_or_values = func(trial)\n","  File \"C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_9752\\1624498814.py\", line 96, in objective\n","    total_loss += loss.item()\n","KeyboardInterrupt\n","[W 2025-06-15 18:44:45,775] Trial 2 failed with value None.\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[29], line 119\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# 'minimize' 방향으로 objective 함수(loss)를 최적화하는 연구(study) 생성\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(\n\u001b[0;32m    114\u001b[0m         direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    115\u001b[0m         pruner\u001b[38;5;241m=\u001b[39moptuna\u001b[38;5;241m.\u001b[39mpruners\u001b[38;5;241m.\u001b[39mMedianPruner(n_warmup_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m),\n\u001b[0;32m    116\u001b[0m         sampler\u001b[38;5;241m=\u001b[39moptuna\u001b[38;5;241m.\u001b[39msamplers\u001b[38;5;241m.\u001b[39mTPESampler(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m    117\u001b[0m     )\n\u001b[1;32m--> 119\u001b[0m     \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# CPU-only 병렬이면 n_jobs=4 가능\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# --- 최적 결과 확인 ---\u001b[39;00m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m========================================================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[1;32m~\\Desktop\\Deep\\dl_env\\lib\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32m~\\Desktop\\Deep\\dl_env\\lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n","File \u001b[1;32m~\\Desktop\\Deep\\dl_env\\lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n","File \u001b[1;32m~\\Desktop\\Deep\\dl_env\\lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n","File \u001b[1;32m~\\Desktop\\Deep\\dl_env\\lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n","Cell \u001b[1;32mIn[29], line 96\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     94\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     95\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 96\u001b[0m         total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# 최종 epoch의 평균 Loss를 이 trial의 성능 지표로 반환\u001b[39;00m\n\u001b[0;32m     99\u001b[0m final_avg_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader)\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","import os\n","num_layers = len(material_sequence)  # 3\n","class InverseNet(nn.Module):\n","    def __init__(self, input_dim, output_dim):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(input_dim, 768),\n","            nn.ReLU(),\n","            nn.Linear(768, 768),\n","            nn.ReLU(),\n","            nn.Linear(768, 768),\n","            nn.ReLU(),\n","            nn.Linear(768, 768),\n","            nn.ReLU(),\n","            nn.Linear(768, output_dim),\n","            # no nonlinearity here\n","        )\n","\n","    def forward(self, x):\n","        # x must be shape (B, input_dim)\n","        return self.net(x)\n","\n","# ---------------------------\n","# 1. 저장된 모델 로드\n","# ---------------------------\n","inverse_net = InverseNet(input_dim=NUM_WAVELENGTHS, output_dim=len(material_sequence)).to(device)\n","inverse_net = inverse_net.to(torch.float64)\n","\n","# 모델 파라미터 로드\n","ckpt_path = os.path.join(\"C:/Users/PC/Desktop/Deep/\", \"inverse_net_final.pth\")\n","inverse_net.load_state_dict(torch.load(ckpt_path, map_location=device))\n","inverse_net.eval()  # 평가모드\n","print(f\"모델 로드 완료: {ckpt_path}\")\n","\n","# ---------------------------\n","# 2. 첫 배치만 inference\n","# ---------------------------\n","for batch_idx, batch in enumerate(dataloader):\n","    d_norm_true = batch['d_norm'].to(device)     # (B, 3)\n","    T_target = batch['T_target'].to(device)      # (B, 401)\n","\n","    # (1) Target 출력\n","    d_true_nm = destandardize_thickness(d_norm_true)\n","    print(\"=== d_true (nm) ===\")\n","    print(d_true_nm[0].cpu().numpy())\n","    print(\"=== T_target 스펙트럼 ===\")\n","    print(\"min, max, mean = \",\n","          T_target[0].min().item(), T_target[0].max().item(), T_target[0].mean().item())\n","\n","    # (2) 두께 예측\n","    with torch.no_grad():\n","        d_pred_norm = inverse_net(T_target)  # (B, 3)\n","        d_pred_nm = destandardize_thickness(d_pred_norm)\n","\n","    print(\"\\n=== d_pred_norm (표준화된 두께) ===\")\n","    print(d_pred_norm[0].cpu().numpy())\n","    print(\"→ d_pred_nm =\", d_pred_nm[0].cpu().numpy())\n","\n","    # (3) TMM forward (1개씩)\n","    T_pred_list = []\n","    for i in range(T_target.shape[0]):\n","        thickness_nm = d_pred_nm[i]  # (3,)\n","        T_pred = model(thickness_nm.to(torch.float64))  # (401,)\n","        T_pred_list.append(T_pred)\n","\n","        if batch_idx == 0 and i == 0:\n","            print(\"→ TMM 입력 두께 =\", thickness_nm.cpu().numpy())\n","            print(\"TMM 결과 시각화:\")\n","\n","            plt.plot(WAVELENGTHS, T_pred.detach().cpu().numpy(), label='T_pred')\n","            plt.plot(WAVELENGTHS, T_target[i].detach().cpu().numpy(), label='T_target', linestyle='dashed')\n","            plt.xlim(380, 780)\n","            plt.ylim(0, 1)\n","            plt.xlabel(\"Wavelength (nm)\")\n","            plt.ylabel(\"Transmittance\")\n","            plt.grid(True)\n","            plt.legend()\n","            plt.show()\n","\n","    # (4) MSE 계산\n","    T_pred_batch = torch.stack(T_pred_list, dim=0)  # (B, 401)\n","    mse_loss = F.mse_loss(T_pred_batch, T_target)\n","    print(f\"\\n첫 배치 전체 MSE loss = {mse_loss.item():.6f}\")\n","    break  # 첫 배치만 체크하고 종료\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"XAs57G6yFt90","outputId":"2512d9d5-c15b-4bff-e1e1-f8eb755d958c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_29672\\3715825692.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  inverse_net.load_state_dict(torch.load(ckpt_path, map_location=device))\n"]},{"output_type":"stream","name":"stdout","text":["모델 로드 완료: C:/Users/PC/Desktop/Deep/inverse_net_final.pth\n","=== d_true (nm) ===\n","[[  10. 1300.   15.]]\n","=== T_target 스펙트럼 ===\n","min, max, mean =  -0.18730372650155494 3.612421506591234 0.8806223545470996\n","\n","=== d_pred_norm (표준화된 두께) ===\n","[[-1.32444453  1.22742783 -0.80030322]]\n","→ d_pred_nm = [[  10.63476292 1305.11102745   14.34100166]]\n","→ TMM 입력 두께 = [[  10.63476292 1305.11102745   14.34100166]]\n","TMM 결과 시각화:\n"]},{"output_type":"error","ename":"ValueError","evalue":"x and y must have same first dimension, but have shapes (401,) and (1, 401)","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[10], line 73\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m→ TMM 입력 두께 =\u001b[39m\u001b[38;5;124m\"\u001b[39m, thickness_nm\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTMM 결과 시각화:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 73\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mWAVELENGTHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT_pred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mT_pred\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(WAVELENGTHS, T_target[i]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mT_target\u001b[39m\u001b[38;5;124m'\u001b[39m, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdashed\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     75\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlim(\u001b[38;5;241m380\u001b[39m, \u001b[38;5;241m780\u001b[39m)\n","File \u001b[1;32m~\\Desktop\\Deep\\dl_env\\lib\\site-packages\\matplotlib\\pyplot.py:3838\u001b[0m, in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3830\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[0;32m   3831\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mplot\u001b[39m(\n\u001b[0;32m   3832\u001b[0m     \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3836\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3837\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[1;32m-> 3838\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gca()\u001b[38;5;241m.\u001b[39mplot(\n\u001b[0;32m   3839\u001b[0m         \u001b[38;5;241m*\u001b[39margs,\n\u001b[0;32m   3840\u001b[0m         scalex\u001b[38;5;241m=\u001b[39mscalex,\n\u001b[0;32m   3841\u001b[0m         scaley\u001b[38;5;241m=\u001b[39mscaley,\n\u001b[0;32m   3842\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: data} \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[0;32m   3843\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3844\u001b[0m     )\n","File \u001b[1;32m~\\Desktop\\Deep\\dl_env\\lib\\site-packages\\matplotlib\\axes\\_axes.py:1777\u001b[0m, in \u001b[0;36mAxes.plot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1534\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1535\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[0;32m   1536\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1776\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[1;32m-> 1777\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[0;32m   1778\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[0;32m   1779\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n","File \u001b[1;32m~\\Desktop\\Deep\\dl_env\\lib\\site-packages\\matplotlib\\axes\\_base.py:297\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[1;34m(self, axes, data, return_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    295\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    296\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m--> 297\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_kwargs\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32m~\\Desktop\\Deep\\dl_env\\lib\\site-packages\\matplotlib\\axes\\_base.py:494\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[1;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[0;32m    491\u001b[0m     axes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m--> 494\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    495\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    498\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (401,) and (1, 401)"]}]}]}
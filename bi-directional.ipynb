{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyMa58FWjh7N51J9/5y9+Bks"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "OKdr0AFPWE7x"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# 파장 정의\n",
        "WAVELENGTHS = np.arange(380, 781, 1)\n",
        "NUM_WAVELENGTHS = len(WAVELENGTHS) #401\n",
        "\n",
        "# GitHub raw 주소 (파일들이 위치한 경로)\n",
        "base_url = \"https://raw.githubusercontent.com/KIMEUIJOON-KNU/Bi-directional/main/\"\n",
        "\n",
        "# 사용할 재료 목록\n",
        "material_files = [\n",
        "    ('SiO2', 'SiO2_380.csv'),\n",
        "    ('WO3',  'WO3_380.csv'),\n",
        "    ('Ag',   'Ag_380.csv')\n",
        "]\n",
        "\n",
        "material_data = {}\n",
        "material_names = [info[0] for info in material_files]\n",
        "print(\"재료 리스트:\", material_names)\n",
        "\n",
        "# 재료 인덱싱 매핑\n",
        "material_names = sorted(set(material_names))\n",
        "material_to_index = {name: i for i, name in enumerate(material_names)}\n",
        "index_to_material = {i: name for name, i in material_to_index.items()}\n",
        "print(\"재료 인덱스 매핑:\", index_to_material)\n",
        "\n",
        "# --- 재료 파일 불러오기 ---\n",
        "for material_name, filename in material_files:\n",
        "    file_url = base_url + filename\n",
        "    print(f\"처리 중: {filename} (materials: {material_name}) → {file_url}\")\n",
        "    try:\n",
        "        data = pd.read_csv(file_url, header=None)\n",
        "    except Exception as e:\n",
        "        print(f\"  🚫 오류 발생: {e}\")\n",
        "        continue\n",
        "\n",
        "    if data.empty:\n",
        "        print(\"  ⚠️ 경고: 파일이 비어있습니다.\")\n",
        "        continue\n",
        "\n",
        "    wavelengths = data.iloc[:, 0].values  # 파장\n",
        "    n = data.iloc[:, 1].values           # 실수부\n",
        "    k = data.iloc[:, 2].values           # 허수부\n",
        "\n",
        "    n_complex = n + 1j * k               # 복소 굴절률\n",
        "\n",
        "    material_data[material_name] = (wavelengths, n_complex)\n",
        "    print(f\"로드 완료: {len(wavelengths)} points\")\n",
        "\n",
        "# --- 공기 굴절률 추가 ---\n",
        "material_data['Air'] = np.ones(NUM_WAVELENGTHS, dtype=np.complex128)\n",
        "print(\"Air 굴절률 추가 완료\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6wcpY_qWA9B",
        "outputId": "8eef385a-23fa-4c16-9f8a-7fe6e09ad575"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "재료 리스트: ['SiO2', 'WO3', 'Ag']\n",
            "재료 인덱스 매핑: {0: 'Ag', 1: 'SiO2', 2: 'WO3'}\n",
            "처리 중: SiO2_380.csv (materials: SiO2) → https://raw.githubusercontent.com/KIMEUIJOON-KNU/Bi-directional/main/SiO2_380.csv\n",
            "로드 완료: 401 points\n",
            "처리 중: WO3_380.csv (materials: WO3) → https://raw.githubusercontent.com/KIMEUIJOON-KNU/Bi-directional/main/WO3_380.csv\n",
            "로드 완료: 401 points\n",
            "처리 중: Ag_380.csv (materials: Ag) → https://raw.githubusercontent.com/KIMEUIJOON-KNU/Bi-directional/main/Ag_380.csv\n",
            "로드 완료: 401 points\n",
            "Air 굴절률 추가 완료\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "# (1) 파장 정의 (380~780 nm, 1 nm 간격 → 총 401개 파장 지점)\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "λ_tensor_global = torch.tensor(WAVELENGTHS, dtype=torch.float64).to(device) * 1e-9  # [m 단위로 변환된 파장 텐서]\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "# (2) 층별 재료 정의 (MIM 구조 예시: 금속-절연체-금속)\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "material_sequence = ['Ag', 'WO3', 'Ag']  # 구조에 사용될 재료 순서 (입사면에서부터 아래 방향)\n",
        "\n",
        "# material_data 에서 각 재료의 복소 굴절률 (n + ik) 값 추출 (shape: [401])\n",
        "n_list_np_arrays = [material_data[mat][1] for mat in material_sequence]  # 길이 3 리스트 (각각 [401] 크기)\n",
        "\n",
        "# numpy → torch 변환 후, complex64 형식으로 묶어서 GPU로 이동 (shape: [3, 401])\n",
        "n_list_torch = torch.stack([\n",
        "    torch.from_numpy(arr).to(torch.complex64) for arr in n_list_np_arrays\n",
        "], dim=0).to(device)\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "# (3) 입사 매질 (n_i)와 출사 매질 (n_s) 설정\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "n_i_np = material_data['SiO2'][1]   # 입사 매질: SiO2의 복소 굴절률 (n + ik)\n",
        "n_s_np = material_data['Air']       # 출사 매질: Air (n=1, k=0)\n",
        "\n",
        "n_i_torch = torch.from_numpy(n_i_np).to(torch.complex64).to(device)  # torch 변환 후 GPU 이동\n",
        "n_s_torch = torch.from_numpy(n_s_np).to(torch.complex64).to(device)\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "# (4) 두께 그리드 정의 (nm 단위, 총 조합 수 = 8 * 187 * 8 = 11968개)\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "d1_list_nm = np.arange(5, 41, 5)      # 금속층 (d1): 5 ~ 40 nm, 간격 5 nm → 총 8개\n",
        "d2_list_nm = np.arange(150, 801, 5)    # 절연층 (d2): 20 ~ 950 nm, 간격 5 nm → 총 187개\n",
        "d3_list_nm = np.arange(5, 41, 5)      # 금속층 (d3): 5 ~ 40 nm, 간격 5 nm → 총 8개\n",
        "\n",
        "print(len(d1_list_nm)*len(d2_list_nm)*len(d3_list_nm))  # 총 조합 개수 출력 (8 × 187 × 8 = 11,968)\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "# (5) 샘플 저장용 리스트 초기화\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "all_d_tilde = []    # ▶ 각 샘플의 (d1, d2, d3) 두께 조합을 저장할 리스트\n",
        "                    #   일반적으로 나중에 정규화된 형태로 저장되며 학습 입력으로 사용\n",
        "\n",
        "all_T_target = []   # ▶ 각 조합에 대응하는 스펙트럼 T(λ) [shape: (401,)] 를 저장할 리스트\n",
        "                    #   Forward model을 통해 계산된 정답 스펙트럼 (target)\n"
      ],
      "metadata": {
        "id": "qKoHAdIqxvHg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bd87892-03d5-466f-8d66-9a299cc8431f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "# (A-1) TMMNetwork 정의 (클래스 이름, dtype 통일, calculate() 추가)\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "class TMMNetwork(nn.Module):\n",
        "    def __init__(self, n_list_input, n_i_input, n_s_input, wavelengths_m_tensor):\n",
        "        super().__init__()\n",
        "        dtype_complex = torch.complex128\n",
        "        dtype_float = torch.float64\n",
        "\n",
        "        n_list_real = n_list_input.real\n",
        "        n_list_imag = -torch.abs(n_list_input.imag)\n",
        "        self.register_buffer('n_list', torch.complex(n_list_real, n_list_imag))\n",
        "        self.register_buffer('n_i', n_i_input.to(dtype_complex))\n",
        "        self.register_buffer('n_s', n_s_input.to(dtype_complex))\n",
        "\n",
        "        # 파수 k0\n",
        "        k0 = 2 * torch.pi / torch.clamp(wavelengths_m_tensor, min=1e-20)\n",
        "        self.register_buffer('k0', k0)  # 이제 k0_tilde 아님\n",
        "\n",
        "        self.num_layers = self.n_list.shape[0]\n",
        "        self.num_wavelengths = wavelengths_m_tensor.shape[0]\n",
        "\n",
        "    def forward(self, thicknesses_nm):\n",
        "        device = thicknesses_nm.device\n",
        "        dtype_complex = torch.complex128\n",
        "        imag_unit = torch.tensor(1j, dtype=dtype_complex, device=device)\n",
        "\n",
        "        if thicknesses_nm.ndim == 1:\n",
        "            thicknesses_nm = thicknesses_nm.unsqueeze(0)  # (1, 3)\n",
        "        B = thicknesses_nm.shape[0]  # batch size\n",
        "\n",
        "        # Convert to meters\n",
        "        thicknesses_m = thicknesses_nm * 1e-9  # (B, 3)\n",
        "\n",
        "        # (B, 3, 401): broadcasting layer × wavelength\n",
        "        delta = thicknesses_m[:, :, None] * self.k0[None, None, :] * self.n_list[None, :, :]\n",
        "\n",
        "        cosδ = torch.cos(delta)\n",
        "        sinδ = torch.sin(delta)\n",
        "\n",
        "        # 초기 M_total: (B, 401, 2, 2)\n",
        "        M_total = torch.eye(2, dtype=dtype_complex, device=device).repeat(B, self.num_wavelengths, 1, 1)\n",
        "\n",
        "        for j in range(self.num_layers):\n",
        "            Yj = self.n_list[j] * 2.654e-3  # (401,)\n",
        "\n",
        "            sin_j = sinδ[:, j, :]\n",
        "            cos_j = cosδ[:, j, :]\n",
        "\n",
        "            m11 = cos_j\n",
        "            m12 = imag_unit * sin_j / Yj[None, :]\n",
        "            m21 = imag_unit * Yj[None, :] * sin_j\n",
        "            m22 = cos_j\n",
        "\n",
        "            Mj = torch.stack([\n",
        "                torch.stack([m11, m12], dim=-1),\n",
        "                torch.stack([m21, m22], dim=-1)\n",
        "            ], dim=-2)  # shape: (B, 401, 2, 2)\n",
        "\n",
        "            M_total = torch.matmul(M_total, Mj)\n",
        "\n",
        "        Y_in = self.n_i * 2.654e-3\n",
        "        Y_out = self.n_s * 2.654e-3\n",
        "\n",
        "        B_ = M_total[:, :, 0, 0] + M_total[:, :, 0, 1] * Y_out\n",
        "        C_ = M_total[:, :, 1, 0] + M_total[:, :, 1, 1] * Y_out\n",
        "        denom = Y_in * B_ + C_\n",
        "        t1 = (2 * Y_in) / denom  # (B, 401)\n",
        "\n",
        "        T = (torch.real(self.n_s) / torch.real(self.n_i)) * torch.abs(t1) ** 2\n",
        "        return T.to(torch.float64)  # (B, 401)\n",
        "\n",
        "model = TMMNetwork(n_list_torch, n_i_torch, n_s_torch, λ_tensor_global).to(device)"
      ],
      "metadata": {
        "id": "V33NauzAxlVI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# ------------------------------\n",
        "# 시드 고정 (재현성 확보)\n",
        "# ------------------------------\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)  # if using multi-GPU\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(42)  # 시드 고정 실행"
      ],
      "metadata": {
        "id": "5p_PSFaJx_Y3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "from torch.utils.data import random_split\n",
        "all_d = []\n",
        "all_T_target = []\n",
        "\n",
        "print(\"Generating (normalized_d → T_target) pairs ...\")\n",
        "for d1 in d1_list_nm:\n",
        "    for d2 in d2_list_nm:\n",
        "        for d3 in d3_list_nm:\n",
        "            # (a) [d1, d2, d3]\n",
        "            d_nm_vec = torch.tensor([[d1, d2, d3]], dtype=torch.float64)  # shape: (1, 3)\n",
        "            all_d.append(d_nm_vec.cpu().numpy())\n",
        "            # (b) TMM forward\n",
        "            T_spec = model(d_nm_vec.to(device))  # forward(nm 단위 두께)\n",
        "            all_T_target.append(T_spec.detach().cpu().numpy())\n",
        "# NumPy 배열로 변환\n",
        "all_d = np.array(all_d, dtype=np.float64)   # shape: (3096, 3)\n",
        "#ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ두께표준화ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
        "THICKNESS_MIN = torch.tensor(np.min(all_d, axis=0), dtype=torch.float64, device=device)\n",
        "THICKNESS_MAX = torch.tensor(np.max(all_d, axis=0), dtype=torch.float64, device=device)\n",
        "def standardize_thickness(d_nm_array):\n",
        "    d_nm_array = torch.tensor(d_nm_array, dtype=torch.float64, device=THICKNESS_MIN.device) if not isinstance(d_nm_array, torch.Tensor) else d_nm_array.to(dtype=torch.float64, device=THICKNESS_MIN.device)\n",
        "    return (d_nm_array - THICKNESS_MIN) / (THICKNESS_MAX - THICKNESS_MIN)\n",
        "\n",
        "def destandardize_thickness(d_norm_array):\n",
        "    d_norm_array = d_norm_array.clone().to(dtype=torch.float64, device=THICKNESS_MIN.device)\n",
        "    return d_norm_array * (THICKNESS_MAX - THICKNESS_MIN) + THICKNESS_MIN\n",
        "\n",
        "all_d_norm = standardize_thickness(all_d)  # shape: (3096, 3)\n",
        "d_nm_check = destandardize_thickness(all_d_norm)\n",
        "\n",
        "\n",
        "#ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ스펙트럼표준화ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
        "all_T_target = np.array(all_T_target, dtype=np.float64) # shape: (3096, 401)\n",
        "print(\"Total samples:\", all_d.shape[0])  # 3096\n",
        "\n",
        "SPECTRUM_MIN = torch.tensor(np.min(all_T_target, axis=0), dtype=torch.float64).to(device)\n",
        "SPECTRUM_MAX = torch.tensor(np.max(all_T_target, axis=0), dtype=torch.float64).to(device)\n",
        "def standardize_spectrum(T_array):\n",
        "    T_tensor = torch.tensor(T_array, dtype=torch.float64).to(device)\n",
        "    return (T_tensor - SPECTRUM_MIN) / (SPECTRUM_MAX - SPECTRUM_MIN)\n",
        "\n",
        "def destandardize_spectrum(T_std_array):\n",
        "    return T_std_array * (SPECTRUM_MAX - SPECTRUM_MIN) + SPECTRUM_MIN\n",
        "\n",
        "# destandardize_spectrum의 결과(GPU 텐서)를 .cpu().numpy()로 변환하여 비교합니다.\n",
        "all_T_target_std = standardize_spectrum(all_T_target)\n",
        "restored_T = destandardize_spectrum(all_T_target_std)\n",
        "assert np.allclose(restored_T.cpu().numpy(), all_T_target, atol=1e-5)\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "# (B) Dataset / DataLoader 구축\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "class BiTMMNormalizedDataset(Dataset):\n",
        "    def __init__(self, d_norm_array, T_array):\n",
        "        self.d_norm = d_norm_array.clone().to(dtype=torch.float64)\n",
        "        self.T_spec = T_array.clone().detach().to(dtype=torch.float64) if isinstance(T_array, torch.Tensor) else torch.tensor(T_array, dtype=torch.float64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.d_norm.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'd_norm': self.d_norm[idx],       # shape: (3,)1\n",
        "            'T_target': self.T_spec[idx]      # shape: (401,)\n",
        "        }\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "\n",
        "# 시드 고정\n",
        "seed = 42\n",
        "g = torch.Generator().manual_seed(seed)\n",
        "\n",
        "# Dataset 생성\n",
        "dataset = BiTMMNormalizedDataset(all_d_norm, all_T_target_std)\n",
        "\n",
        "# 전체 길이 및 split 비율\n",
        "total_size = len(dataset)\n",
        "train_size = int(0.9 * total_size)\n",
        "val_size   = int(0.05 * total_size)\n",
        "test_size  = total_size - train_size - val_size  # 나머지\n",
        "\n",
        "print(f\"Total: {total_size}, Train: {train_size}, Val: {val_size}, Test: {test_size}\")\n",
        "\n",
        "# Dataset 분할\n",
        "train_dataset, val_dataset, test_dataset = random_split(\n",
        "    dataset, [train_size, val_size, test_size], generator=g  # g로 통일\n",
        ")\n",
        "\n",
        "# DataLoader 정의\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  num_workers=0, generator=g)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, num_workers=0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suR-NIYM9jjV",
        "outputId": "7799abb3-fd68-40ed-c692-6e97d5a3c026"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating (normalized_d → T_target) pairs ...\n",
            "Total samples: 8384\n",
            "Total: 8384, Train: 7545, Val: 419, Test: 420\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Forward Training **"
      ],
      "metadata": {
        "id": "GF1IxuMUxqpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "\n",
        "# ──────────────────────────────────────────────────────\n",
        "# [1] 모델 정의: ForwardNet\n",
        "# - 입력: 정규화된 두께 벡터 (3차원)\n",
        "# - 출력: 정규화된 스펙트럼 벡터 (401차원)\n",
        "# ──────────────────────────────────────────────────────\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "# MLP 각 층의 뉴런 수 (실험적으로 설정된 값)\n",
        "N1 = 1190\n",
        "N2 = 824\n",
        "N3 = 920\n",
        "\n",
        "class ForwardNet(nn.Module):\n",
        "    def __init__(self, input_dim=3, output_dim=401):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(3, N1),   # 입력층 → 첫 번째 은닉층\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(N1, N2),  # 첫 번째 → 두 번째 은닉층\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(N2, N3),  # 두 번째 → 세 번째 은닉층\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(N3, 401)  # 세 번째 → 출력층 (스펙트럼 401포인트)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# ──────────────────────────────────────────────────────\n",
        "# [2] 모델 및 옵티마이저 초기화\n",
        "# ──────────────────────────────────────────────────────\n",
        "\n",
        "# 모델을 GPU + float64로 초기화\n",
        "forward_net = ForwardNet(input_dim=3, output_dim=401).to(device).to(torch.float64)\n",
        "\n",
        "# 옵티마이저: Adam, 초기 학습률 1e-3\n",
        "optimizer = torch.optim.Adam(forward_net.parameters(), lr=1e-3)\n",
        "\n",
        "# ──────────────────────────────────────────────────────\n",
        "# [3] 학습 관련 파라미터 설정\n",
        "# ──────────────────────────────────────────────────────\n",
        "num_epochs = 600  # 전체 학습 epoch 수\n",
        "\n",
        "# 학습 결과를 저장할 디렉토리 생성\n",
        "output_dir = \"C:/Users/PC/Desktop/Deep/\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# 학습 및 검증 손실 기록용 리스트\n",
        "train_loss_history = []\n",
        "val_loss_history = []\n",
        "\n",
        "# ──────────────────────────────────────────────────────\n",
        "# [4] 학습 루프 시작\n",
        "# ──────────────────────────────────────────────────────\n",
        "for epoch in range(num_epochs):\n",
        "    forward_net.train()  # 학습 모드 전환\n",
        "    total_loss = 0.0     # 누적 손실 초기화\n",
        "\n",
        "    # ▶ 한 epoch 동안 전체 train_loader 순회\n",
        "    for batch in train_loader:\n",
        "        d_norm = batch['d_norm'].to(device)        # 입력: 정규화된 두께 [B, 3]\n",
        "        T_target = batch['T_target'].to(device)    # 정답: 정규화된 스펙트럼 [B, 401]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        T_pred = forward_net(d_norm)               # 모델 예측\n",
        "\n",
        "        loss = F.mse_loss(T_pred, T_target)        # 평균 제곱 오차\n",
        "        loss.backward()                            # 역전파\n",
        "        optimizer.step()                           # 가중치 업데이트\n",
        "        total_loss += loss.item()                  # 손실 누적\n",
        "\n",
        "    # ▶ 평균 학습 손실 계산 및 기록\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    train_loss_history.append(avg_train_loss)\n",
        "    print(f\"[Train] Epoch {epoch+1}, Avg Loss = {avg_train_loss:.6f}\")\n",
        "\n",
        "    # ───────────────────────────────────────────\n",
        "    # [5] 검증 루프 (Validation)\n",
        "    # ───────────────────────────────────────────\n",
        "    forward_net.eval()        # 평가 모드 전환\n",
        "    val_loss_total = 0.0\n",
        "\n",
        "    with torch.no_grad():    # 그래디언트 비활성화 (메모리 절약)\n",
        "        for batch in val_loader:\n",
        "            d_norm = batch['d_norm'].to(device)\n",
        "            T_target = batch['T_target'].to(device)\n",
        "\n",
        "            T_pred = forward_net(d_norm)           # 예측\n",
        "            loss = F.mse_loss(T_pred, T_target)    # 손실 계산\n",
        "            val_loss_total += loss.item()\n",
        "\n",
        "    # ▶ 평균 검증 손실 계산 및 기록\n",
        "    avg_val_loss = val_loss_total / len(val_loader)\n",
        "    val_loss_history.append(avg_val_loss)\n",
        "    print(f\"[Val]   Epoch [{epoch+1}/{num_epochs}] done. Avg Loss: {avg_val_loss:.6f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lsU_1U9mxoxo",
        "outputId": "37a4f662-2832-4875-d926-cd88c4b02d4f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Train] Epoch 1, Avg Loss = 0.024856\n",
            "[Val]   Epoch [1/600] done. Avg Loss: 0.022393\n",
            "[Train] Epoch 2, Avg Loss = 0.022374\n",
            "[Val]   Epoch [2/600] done. Avg Loss: 0.021987\n",
            "[Train] Epoch 3, Avg Loss = 0.021553\n",
            "[Val]   Epoch [3/600] done. Avg Loss: 0.020566\n",
            "[Train] Epoch 4, Avg Loss = 0.020332\n",
            "[Val]   Epoch [4/600] done. Avg Loss: 0.018631\n",
            "[Train] Epoch 5, Avg Loss = 0.018621\n",
            "[Val]   Epoch [5/600] done. Avg Loss: 0.018245\n",
            "[Train] Epoch 6, Avg Loss = 0.016813\n",
            "[Val]   Epoch [6/600] done. Avg Loss: 0.014914\n",
            "[Train] Epoch 7, Avg Loss = 0.014887\n",
            "[Val]   Epoch [7/600] done. Avg Loss: 0.013470\n",
            "[Train] Epoch 8, Avg Loss = 0.013610\n",
            "[Val]   Epoch [8/600] done. Avg Loss: 0.011817\n",
            "[Train] Epoch 9, Avg Loss = 0.012065\n",
            "[Val]   Epoch [9/600] done. Avg Loss: 0.010882\n",
            "[Train] Epoch 10, Avg Loss = 0.010592\n",
            "[Val]   Epoch [10/600] done. Avg Loss: 0.009454\n",
            "[Train] Epoch 11, Avg Loss = 0.008899\n",
            "[Val]   Epoch [11/600] done. Avg Loss: 0.008016\n",
            "[Train] Epoch 12, Avg Loss = 0.007298\n",
            "[Val]   Epoch [12/600] done. Avg Loss: 0.006803\n",
            "[Train] Epoch 13, Avg Loss = 0.006188\n",
            "[Val]   Epoch [13/600] done. Avg Loss: 0.006109\n",
            "[Train] Epoch 14, Avg Loss = 0.005444\n",
            "[Val]   Epoch [14/600] done. Avg Loss: 0.005259\n",
            "[Train] Epoch 15, Avg Loss = 0.005134\n",
            "[Val]   Epoch [15/600] done. Avg Loss: 0.005510\n",
            "[Train] Epoch 16, Avg Loss = 0.004906\n",
            "[Val]   Epoch [16/600] done. Avg Loss: 0.004476\n",
            "[Train] Epoch 17, Avg Loss = 0.004375\n",
            "[Val]   Epoch [17/600] done. Avg Loss: 0.004404\n",
            "[Train] Epoch 18, Avg Loss = 0.004259\n",
            "[Val]   Epoch [18/600] done. Avg Loss: 0.005044\n",
            "[Train] Epoch 19, Avg Loss = 0.003994\n",
            "[Val]   Epoch [19/600] done. Avg Loss: 0.003750\n",
            "[Train] Epoch 20, Avg Loss = 0.003850\n",
            "[Val]   Epoch [20/600] done. Avg Loss: 0.004028\n",
            "[Train] Epoch 21, Avg Loss = 0.003699\n",
            "[Val]   Epoch [21/600] done. Avg Loss: 0.003873\n",
            "[Train] Epoch 22, Avg Loss = 0.003524\n",
            "[Val]   Epoch [22/600] done. Avg Loss: 0.003452\n",
            "[Train] Epoch 23, Avg Loss = 0.003336\n",
            "[Val]   Epoch [23/600] done. Avg Loss: 0.003245\n",
            "[Train] Epoch 24, Avg Loss = 0.003273\n",
            "[Val]   Epoch [24/600] done. Avg Loss: 0.003301\n",
            "[Train] Epoch 25, Avg Loss = 0.003220\n",
            "[Val]   Epoch [25/600] done. Avg Loss: 0.003259\n",
            "[Train] Epoch 26, Avg Loss = 0.003088\n",
            "[Val]   Epoch [26/600] done. Avg Loss: 0.003138\n",
            "[Train] Epoch 27, Avg Loss = 0.003140\n",
            "[Val]   Epoch [27/600] done. Avg Loss: 0.003072\n",
            "[Train] Epoch 28, Avg Loss = 0.002849\n",
            "[Val]   Epoch [28/600] done. Avg Loss: 0.003627\n",
            "[Train] Epoch 29, Avg Loss = 0.003099\n",
            "[Val]   Epoch [29/600] done. Avg Loss: 0.002832\n",
            "[Train] Epoch 30, Avg Loss = 0.002841\n",
            "[Val]   Epoch [30/600] done. Avg Loss: 0.003006\n",
            "[Train] Epoch 31, Avg Loss = 0.002669\n",
            "[Val]   Epoch [31/600] done. Avg Loss: 0.002655\n",
            "[Train] Epoch 32, Avg Loss = 0.002755\n",
            "[Val]   Epoch [32/600] done. Avg Loss: 0.002817\n",
            "[Train] Epoch 33, Avg Loss = 0.002498\n",
            "[Val]   Epoch [33/600] done. Avg Loss: 0.002542\n",
            "[Train] Epoch 34, Avg Loss = 0.002665\n",
            "[Val]   Epoch [34/600] done. Avg Loss: 0.002475\n",
            "[Train] Epoch 35, Avg Loss = 0.002476\n",
            "[Val]   Epoch [35/600] done. Avg Loss: 0.002882\n",
            "[Train] Epoch 36, Avg Loss = 0.002606\n",
            "[Val]   Epoch [36/600] done. Avg Loss: 0.002347\n",
            "[Train] Epoch 37, Avg Loss = 0.002513\n",
            "[Val]   Epoch [37/600] done. Avg Loss: 0.002214\n",
            "[Train] Epoch 38, Avg Loss = 0.002279\n",
            "[Val]   Epoch [38/600] done. Avg Loss: 0.002319\n",
            "[Train] Epoch 39, Avg Loss = 0.002229\n",
            "[Val]   Epoch [39/600] done. Avg Loss: 0.002574\n",
            "[Train] Epoch 40, Avg Loss = 0.002162\n",
            "[Val]   Epoch [40/600] done. Avg Loss: 0.002138\n",
            "[Train] Epoch 41, Avg Loss = 0.002250\n",
            "[Val]   Epoch [41/600] done. Avg Loss: 0.002252\n",
            "[Train] Epoch 42, Avg Loss = 0.002579\n",
            "[Val]   Epoch [42/600] done. Avg Loss: 0.003562\n",
            "[Train] Epoch 43, Avg Loss = 0.002329\n",
            "[Val]   Epoch [43/600] done. Avg Loss: 0.002238\n",
            "[Train] Epoch 44, Avg Loss = 0.002010\n",
            "[Val]   Epoch [44/600] done. Avg Loss: 0.002145\n",
            "[Train] Epoch 45, Avg Loss = 0.002150\n",
            "[Val]   Epoch [45/600] done. Avg Loss: 0.002143\n",
            "[Train] Epoch 46, Avg Loss = 0.002121\n",
            "[Val]   Epoch [46/600] done. Avg Loss: 0.002469\n",
            "[Train] Epoch 47, Avg Loss = 0.002216\n",
            "[Val]   Epoch [47/600] done. Avg Loss: 0.002030\n",
            "[Train] Epoch 48, Avg Loss = 0.001905\n",
            "[Val]   Epoch [48/600] done. Avg Loss: 0.002415\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-7-2573985307.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                            \u001b[0;31m# 역전파\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                           \u001b[0;31m# 가중치 업데이트\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                  \u001b[0;31m# 손실 누적\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;31m# ▶ 평균 학습 손실 계산 및 기록\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "# (C) InverseNet 정의 (MLP)\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "t_path = '/content/drive/MyDrive/Colab Notebooks/Bi-directional20250402'\n",
        "os.makedirs(t_path, exist_ok=True)\n",
        "num_layers = len(material_sequence)  # 3\n",
        "class InverseNet(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 1198),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1198, 1125),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1125, 853),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(853, 1411),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1411, 1320),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1320, 1958),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1958, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x must be shape (B, input_dim)\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "inverse_net = InverseNet(input_dim=NUM_WAVELENGTHS, output_dim=num_layers).to(device)\n",
        "inverse_net = inverse_net.to(torch.float64)\n",
        "optimizer = optim.Adam(inverse_net.parameters(), lr=0.0016277391324458217)\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "# (D) 학습 루프\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "num_epochs = 600\n",
        "alpha = 1.0616785666328346\n",
        "forward_net.eval()\n",
        "for epoch in range(num_epochs):\n",
        "    inverse_net.train()\n",
        "    total_loss = 0.0\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        T_target = batch['T_target'].to(device)      # [B, 401]\n",
        "        d_norm_true = batch['d_norm'].to(device)     # [B, 3]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        d_norm_pred = inverse_net(T_target.to(torch.float64)).squeeze(1)\n",
        "\n",
        "        if batch_idx == 0:\n",
        "            print(f\"\\n[DEBUG] Epoch {epoch+1}, Batch {batch_idx+1}\")\n",
        "        d_nm_pred = destandardize_thickness(d_norm_pred)\n",
        "        d_nm_true = destandardize_thickness(d_norm_true)\n",
        "\n",
        "        T_pred_batch = model(d_nm_pred)\n",
        "        squeezed_T_target = T_target.squeeze(1)\n",
        "        T_target_destd = destandardize_spectrum(squeezed_T_target)\n",
        "\n",
        "        loss_spectrum = F.mse_loss(T_pred_batch, T_target_destd)\n",
        "        loss_thickness = F.mse_loss(d_norm_pred, d_norm_true.squeeze(1))\n",
        "\n",
        "        if epoch < 18:\n",
        "            loss = loss_spectrum + alpha * loss_thickness\n",
        "        else:\n",
        "            loss = loss_spectrum\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    print(f\"[Train] Epoch [{epoch+1}/{num_epochs}] done. Avg Loss: {avg_train_loss:.6f}\")\n",
        "\n",
        "    # ───────────────────────────────────────────\n",
        "    # Validation\n",
        "    # ───────────────────────────────────────────\n",
        "    inverse_net.eval()\n",
        "    val_loss_total = 0.0\n",
        "    with torch.no_grad():\n",
        "        for val_batch in val_loader:\n",
        "            T_val = val_batch['T_target'].to(device)\n",
        "            d_norm_val = val_batch['d_norm'].to(device)\n",
        "\n",
        "            d_pred_val = inverse_net(T_val.to(torch.float64)).squeeze(1)\n",
        "            d_nm_val_pred = destandardize_thickness(d_pred_val)\n",
        "            T_val_pred = model(d_nm_val_pred)\n",
        "\n",
        "            T_val_destd = destandardize_spectrum(T_val.squeeze(1))\n",
        "\n",
        "            loss_val_spectrum = F.mse_loss(T_val_pred, T_val_destd)\n",
        "            loss_val_thickness = F.mse_loss(d_pred_val, d_norm_val.squeeze(1))\n",
        "\n",
        "            if epoch < 18:\n",
        "                val_loss = loss_val_spectrum + alpha * loss_val_thickness\n",
        "            else:\n",
        "                val_loss = loss_val_spectrum\n",
        "\n",
        "            val_loss_total += val_loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss_total / len(val_loader)\n",
        "    print(f\"[Val]   Epoch [{epoch+1}/{num_epochs}] done. Avg Loss: {avg_val_loss:.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9oRdmem4aZ1_",
        "outputId": "2c605ab9-ca00-46c8-84ee-b02caf6c7ac2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[DEBUG] Epoch 1, Batch 1\n",
            "[Train] Epoch [1/600] done. Avg Loss: 0.138271\n",
            "[Val]   Epoch [1/600] done. Avg Loss: 0.126636\n",
            "\n",
            "[DEBUG] Epoch 2, Batch 1\n",
            "[Train] Epoch [2/600] done. Avg Loss: 0.123173\n",
            "[Val]   Epoch [2/600] done. Avg Loss: 0.125824\n",
            "\n",
            "[DEBUG] Epoch 3, Batch 1\n",
            "[Train] Epoch [3/600] done. Avg Loss: 0.118725\n",
            "[Val]   Epoch [3/600] done. Avg Loss: 0.107659\n",
            "\n",
            "[DEBUG] Epoch 4, Batch 1\n",
            "[Train] Epoch [4/600] done. Avg Loss: 0.106658\n",
            "[Val]   Epoch [4/600] done. Avg Loss: 0.098445\n",
            "\n",
            "[DEBUG] Epoch 5, Batch 1\n",
            "[Train] Epoch [5/600] done. Avg Loss: 0.100070\n",
            "[Val]   Epoch [5/600] done. Avg Loss: 0.096010\n",
            "\n",
            "[DEBUG] Epoch 6, Batch 1\n",
            "[Train] Epoch [6/600] done. Avg Loss: 0.097049\n",
            "[Val]   Epoch [6/600] done. Avg Loss: 0.089660\n",
            "\n",
            "[DEBUG] Epoch 7, Batch 1\n",
            "[Train] Epoch [7/600] done. Avg Loss: 0.095435\n",
            "[Val]   Epoch [7/600] done. Avg Loss: 0.092927\n",
            "\n",
            "[DEBUG] Epoch 8, Batch 1\n",
            "[Train] Epoch [8/600] done. Avg Loss: 0.089577\n",
            "[Val]   Epoch [8/600] done. Avg Loss: 0.083823\n",
            "\n",
            "[DEBUG] Epoch 9, Batch 1\n",
            "[Train] Epoch [9/600] done. Avg Loss: 0.078315\n",
            "[Val]   Epoch [9/600] done. Avg Loss: 0.075152\n",
            "\n",
            "[DEBUG] Epoch 10, Batch 1\n",
            "[Train] Epoch [10/600] done. Avg Loss: 0.072024\n",
            "[Val]   Epoch [10/600] done. Avg Loss: 0.068288\n",
            "\n",
            "[DEBUG] Epoch 11, Batch 1\n",
            "[Train] Epoch [11/600] done. Avg Loss: 0.068013\n",
            "[Val]   Epoch [11/600] done. Avg Loss: 0.075916\n",
            "\n",
            "[DEBUG] Epoch 12, Batch 1\n",
            "[Train] Epoch [12/600] done. Avg Loss: 0.066073\n",
            "[Val]   Epoch [12/600] done. Avg Loss: 0.068439\n",
            "\n",
            "[DEBUG] Epoch 13, Batch 1\n",
            "[Train] Epoch [13/600] done. Avg Loss: 0.066152\n",
            "[Val]   Epoch [13/600] done. Avg Loss: 0.059132\n",
            "\n",
            "[DEBUG] Epoch 14, Batch 1\n",
            "[Train] Epoch [14/600] done. Avg Loss: 0.062155\n",
            "[Val]   Epoch [14/600] done. Avg Loss: 0.063628\n",
            "\n",
            "[DEBUG] Epoch 15, Batch 1\n",
            "[Train] Epoch [15/600] done. Avg Loss: 0.062215\n",
            "[Val]   Epoch [15/600] done. Avg Loss: 0.061032\n",
            "\n",
            "[DEBUG] Epoch 16, Batch 1\n",
            "[Train] Epoch [16/600] done. Avg Loss: 0.060361\n",
            "[Val]   Epoch [16/600] done. Avg Loss: 0.056488\n",
            "\n",
            "[DEBUG] Epoch 17, Batch 1\n",
            "[Train] Epoch [17/600] done. Avg Loss: 0.058385\n",
            "[Val]   Epoch [17/600] done. Avg Loss: 0.059092\n",
            "\n",
            "[DEBUG] Epoch 18, Batch 1\n",
            "[Train] Epoch [18/600] done. Avg Loss: 0.057843\n",
            "[Val]   Epoch [18/600] done. Avg Loss: 0.055955\n",
            "\n",
            "[DEBUG] Epoch 19, Batch 1\n",
            "[Train] Epoch [19/600] done. Avg Loss: 0.013480\n",
            "[Val]   Epoch [19/600] done. Avg Loss: 0.010588\n",
            "\n",
            "[DEBUG] Epoch 20, Batch 1\n",
            "[Train] Epoch [20/600] done. Avg Loss: 0.011821\n",
            "[Val]   Epoch [20/600] done. Avg Loss: 0.010942\n",
            "\n",
            "[DEBUG] Epoch 21, Batch 1\n",
            "[Train] Epoch [21/600] done. Avg Loss: 0.011594\n",
            "[Val]   Epoch [21/600] done. Avg Loss: 0.009289\n",
            "\n",
            "[DEBUG] Epoch 22, Batch 1\n",
            "[Train] Epoch [22/600] done. Avg Loss: 0.009811\n",
            "[Val]   Epoch [22/600] done. Avg Loss: 0.008799\n",
            "\n",
            "[DEBUG] Epoch 23, Batch 1\n",
            "[Train] Epoch [23/600] done. Avg Loss: 0.009447\n",
            "[Val]   Epoch [23/600] done. Avg Loss: 0.007516\n",
            "\n",
            "[DEBUG] Epoch 24, Batch 1\n",
            "[Train] Epoch [24/600] done. Avg Loss: 0.008896\n",
            "[Val]   Epoch [24/600] done. Avg Loss: 0.008307\n",
            "\n",
            "[DEBUG] Epoch 25, Batch 1\n",
            "[Train] Epoch [25/600] done. Avg Loss: 0.009067\n",
            "[Val]   Epoch [25/600] done. Avg Loss: 0.007000\n",
            "\n",
            "[DEBUG] Epoch 26, Batch 1\n",
            "[Train] Epoch [26/600] done. Avg Loss: 0.009028\n",
            "[Val]   Epoch [26/600] done. Avg Loss: 0.006943\n",
            "\n",
            "[DEBUG] Epoch 27, Batch 1\n",
            "[Train] Epoch [27/600] done. Avg Loss: 0.009179\n",
            "[Val]   Epoch [27/600] done. Avg Loss: 0.006646\n",
            "\n",
            "[DEBUG] Epoch 28, Batch 1\n",
            "[Train] Epoch [28/600] done. Avg Loss: 0.008724\n",
            "[Val]   Epoch [28/600] done. Avg Loss: 0.009723\n",
            "\n",
            "[DEBUG] Epoch 29, Batch 1\n",
            "[Train] Epoch [29/600] done. Avg Loss: 0.007408\n",
            "[Val]   Epoch [29/600] done. Avg Loss: 0.010216\n",
            "\n",
            "[DEBUG] Epoch 30, Batch 1\n",
            "[Train] Epoch [30/600] done. Avg Loss: 0.013154\n",
            "[Val]   Epoch [30/600] done. Avg Loss: 0.012691\n",
            "\n",
            "[DEBUG] Epoch 31, Batch 1\n",
            "[Train] Epoch [31/600] done. Avg Loss: 0.010693\n",
            "[Val]   Epoch [31/600] done. Avg Loss: 0.008800\n",
            "\n",
            "[DEBUG] Epoch 32, Batch 1\n",
            "[Train] Epoch [32/600] done. Avg Loss: 0.010738\n",
            "[Val]   Epoch [32/600] done. Avg Loss: 0.007780\n",
            "\n",
            "[DEBUG] Epoch 33, Batch 1\n",
            "[Train] Epoch [33/600] done. Avg Loss: 0.008436\n",
            "[Val]   Epoch [33/600] done. Avg Loss: 0.011527\n",
            "\n",
            "[DEBUG] Epoch 34, Batch 1\n",
            "[Train] Epoch [34/600] done. Avg Loss: 0.009565\n",
            "[Val]   Epoch [34/600] done. Avg Loss: 0.009213\n",
            "\n",
            "[DEBUG] Epoch 35, Batch 1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-8-3231285490.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mavg_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import urllib.request\n",
        "\n",
        "class InverseNet(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 1198),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1198, 1125),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1125, 853),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(853, 1411),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1411, 1320),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1320, 1958),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1958, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x must be shape (B, input_dim)\n",
        "        return self.net(x)\n",
        "\n",
        "# ──────────────────────────────────────────────────────\n",
        "# 2. 모델 불러오기 (GitHub → 로컬 저장 → 모델에 로드)\n",
        "# ──────────────────────────────────────────────────────\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "inverse_net = InverseNet(input_dim=401, output_dim=3).to(device).to(torch.float64)\n",
        "\n",
        "# GitHub에서 파라미터 다운로드\n",
        "url = \"https://raw.githubusercontent.com/KIMEUIJOON-KNU/Bi-directional/main/inverse_net_final.pth\"\n",
        "save_path = \"inverse_net_final.pth\"\n",
        "urllib.request.urlretrieve(url, save_path)\n",
        "\n",
        "# 로드\n",
        "inverse_net.load_state_dict(torch.load(save_path, map_location=device))\n",
        "inverse_net.eval()\n",
        "print(\"✅ 모델 로드 및 eval() 전환 완료\")\n",
        "\n",
        "# ──────────────────────────────────────────────────────\n",
        "# 3. test dataset 추출 (random split된 dataset 기반)\n",
        "# ──────────────────────────────────────────────────────\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "\n",
        "# 이미 정의되어 있는 dataset / train_size / val_size / test_size 사용\n",
        "_, _, test_dataset = random_split(dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(42))\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# ──────────────────────────────────────────────────────\n",
        "# 4. test set 예측 및 결과 저장\n",
        "# ──────────────────────────────────────────────────────\n",
        "all_true_d, all_pred_d = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        T_test = batch['T_target'].to(device).squeeze(1)  # shape: [B, 401]\n",
        "        d_true_norm = batch['d_norm'].to(device)\n",
        "\n",
        "        # 예측\n",
        "        d_pred_norm = inverse_net(T_test)\n",
        "\n",
        "        # 정규화된 두께 → 실제 두께로 복원\n",
        "        d_pred_nm = destandardize_thickness(d_pred_norm).cpu().numpy()\n",
        "        d_true_nm = destandardize_thickness(d_true_norm).cpu().numpy()\n",
        "\n",
        "        all_true_d.append(d_true_nm)\n",
        "        all_pred_d.append(d_pred_nm)\n",
        "\n",
        "# ──────────────────────────────────────────────────────\n",
        "# 5. DataFrame으로 결과 정리 (d1, d2, d3 각 비교)\n",
        "# ──────────────────────────────────────────────────────\n",
        "true_all = torch.tensor(np.concatenate(all_true_d, axis=0))\n",
        "pred_all = torch.tensor(np.concatenate(all_pred_d, axis=0))\n",
        "\n",
        "df_result = pd.DataFrame({\n",
        "    \"True_d1\": true_all[:, 0],\n",
        "    \"True_d2\": true_all[:, 1],\n",
        "    \"True_d3\": true_all[:, 2],\n",
        "    \"Pred_d1\": pred_all[:, 0],\n",
        "    \"Pred_d2\": pred_all[:, 1],\n",
        "    \"Pred_d3\": pred_all[:, 2],\n",
        "})\n",
        "\n",
        "# 상위 10개 출력\n",
        "print(df_result.head(10))"
      ],
      "metadata": {
        "id": "0F9_GmzuQoV4",
        "outputId": "2288af52-c3b3-4b0f-a251-8b1b90a77dc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for InverseNet:\n\tMissing key(s) in state_dict: \"net.10.weight\", \"net.10.bias\", \"net.12.weight\", \"net.12.bias\". \n\tsize mismatch for net.0.weight: copying a param with shape torch.Size([768, 401]) from checkpoint, the shape in current model is torch.Size([1198, 401]).\n\tsize mismatch for net.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1198]).\n\tsize mismatch for net.2.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1125, 1198]).\n\tsize mismatch for net.2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1125]).\n\tsize mismatch for net.4.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([853, 1125]).\n\tsize mismatch for net.4.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([853]).\n\tsize mismatch for net.6.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1411, 853]).\n\tsize mismatch for net.6.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1411]).\n\tsize mismatch for net.8.weight: copying a param with shape torch.Size([3, 768]) from checkpoint, the shape in current model is torch.Size([1320, 1411]).\n\tsize mismatch for net.8.bias: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([1320]).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-9-1800647458.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# 로드\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0minverse_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0minverse_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"✅ 모델 로드 및 eval() 전환 완료\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2581\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2582\u001b[0m                 \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[1;32m   2583\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for InverseNet:\n\tMissing key(s) in state_dict: \"net.10.weight\", \"net.10.bias\", \"net.12.weight\", \"net.12.bias\". \n\tsize mismatch for net.0.weight: copying a param with shape torch.Size([768, 401]) from checkpoint, the shape in current model is torch.Size([1198, 401]).\n\tsize mismatch for net.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1198]).\n\tsize mismatch for net.2.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1125, 1198]).\n\tsize mismatch for net.2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1125]).\n\tsize mismatch for net.4.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([853, 1125]).\n\tsize mismatch for net.4.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([853]).\n\tsize mismatch for net.6.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1411, 853]).\n\tsize mismatch for net.6.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1411]).\n\tsize mismatch for net.8.weight: copying a param with shape torch.Size([3, 768]) from checkpoint, the shape in current model is torch.Size([1320, 1411]).\n\tsize mismatch for net.8.bias: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([1320])."
          ]
        }
      ]
    }
  ]
}